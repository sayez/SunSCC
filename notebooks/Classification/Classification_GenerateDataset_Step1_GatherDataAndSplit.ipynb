{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import json\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from astropy.io import fits\n",
    "from astropy.wcs import WCS\n",
    "from astropy.wcs.utils import skycoord_to_pixel\n",
    "from astropy.coordinates import SkyCoord\n",
    "import astropy.units as u\n",
    "\n",
    "from sunpy.coordinates import frames\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "import concurrent.futures\n",
    "from itertools import repeat\n",
    "import multiprocessing\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "import cv2\n",
    "import skimage.io as io\n",
    "\n",
    "import ipywidgets as widgets\n",
    "%matplotlib widget\n",
    "\n",
    "import importlib\n",
    "from sunscc.utils.clustering import tracking_utilities as utils #import the module here, so that it can be reloaded.\n",
    "from sunscc.utils.clustering import Class2Bbox as c2bb\n",
    "importlib.reload(utils)\n",
    "importlib.reload(c2bb)\n",
    "\n",
    "from sunscc.utils.clustering import MeanShift as MS\n",
    "from sunscc.utils.clustering import clustering_utilities as c_utils\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class NpEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        if isinstance(obj, np.int64):\n",
    "            return int(obj)\n",
    "        if isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        if isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        if isinstance(obj, np.bool_):\n",
    "            return bool(obj)\n",
    "        return super(NpEncoder, self).default(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3150\n"
     ]
    }
   ],
   "source": [
    "wl_dir = \"../../datasets/classification/2002-2019_2/all\"  \n",
    "wl_list = sorted(glob.glob(os.path.join(wl_dir, '**/*.FTS'),recursive=True))\n",
    "wl_basenames = [ os.path.basename(wl).split('.')[0] for wl in wl_list ]\n",
    "\n",
    "masks_dir = '../../datasets/classification/2002-2019_2/T425-T375-T325_fgbg'\n",
    "\n",
    "sqlite_db_path = Path(\"../../datasets/classification/2002-2019_2/drawings_sqlite.sqlite\")\n",
    "database = str(sqlite_db_path.resolve())\n",
    "print(len(wl_list), )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_drawings_datetimes = [c_utils.db_string_to_datetime(item[0]) for item in c_utils.get_unique_drawing_datetimes(database,'drawings')]\n",
    "dr_basenames = [c_utils.datetime_to_drawing_name(item) for item in db_drawings_datetimes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f7a913102af49bfb91e216163645d3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "huge_db_dict = c_utils.wl_list2dbGroups(wl_list[:], dr_basenames, database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = '../../datasets/classification/2002-2019_2/'\n",
    "tmp = root_dir+'/sunscc_wl_list2dbGroups_Classification.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../datasets/classification/2002-2019_2//sunscc_wl_list2dbGroups_Classification.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "print(tmp)\n",
    "with open(tmp, 'w') as f:\n",
    "    json.dump(huge_db_dict, f, cls=NpEncoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(tmp, 'r') as f:\n",
    "    huge_db_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "183\n"
     ]
    }
   ],
   "source": [
    "# indexes of images that should not be taken into account as the segmentation is really bad\n",
    "rotten_list = [\n",
    "    \n",
    "    37,38,39,40,52, 64,65,69,70,\n",
    "    \n",
    "    72,97,99,100,101,102,103,104,142,159,160,161,169,187,190,211,212,218,264,300,312,314,316,319,322,327,339,\n",
    "    343,353,356,387,408,413,414,418,424,425,448,473,474,493,512,508,611,614,666,675,696,726,330,747,750,758,\n",
    "    761,784,804,823,832,840,855,914,935,940,948,990,1013,\n",
    "    \n",
    "    1025,1039,1040,1089,1172,1303,1332,1345,1397,1409,1413,1414,1421,1440,1444,1468,1469,1488,1576,1646,1692,\n",
    "    1735,1815,1840,1867,1893,1900,1905,1919,1924,1925,1930,1953,1969,1992,\n",
    "    \n",
    "    2007,2039,2043,2045,2049,2050,2078,2121,2133,2143,2185,2208,2220,2254,2266,2272,2298,2344,3262,3274,2375,\n",
    "    2445,2454,2468,2492,2494,2495,2500,2501,2503,2516,2518,2536,2568,2574,2598,2604,2633,2635,2749,2763,2815,\n",
    "    2818,2820,2821,2834,2835,2851,2857,2867,2896,2899,2848,2951,2952,2956,2964,2980,2981,2994,\n",
    "    \n",
    "    3018,3092,3093,3097,3099,3101,3106,3118,3122,3123,3124,3140,3148 \n",
    "]\n",
    "print(len(rotten_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'huge_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(root_dir\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/sunscc_meanshift_Classification.json\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m----> 3\u001b[0m     json\u001b[38;5;241m.\u001b[39mdump(\u001b[43mhuge_dict\u001b[49m, f, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m=\u001b[39mNpEncoder)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'huge_dict' is not defined"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open(root_dir+'/sunscc_meanshift_Classification.json', 'w') as f:\n",
    "    json.dump(huge_dict, f, cls=NpEncoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(root_dir+'/meanshift_Classification.json', 'r') as f:\n",
    "    huge_dict = json.load(f)\n",
    "print(len(huge_dict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "huge_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(lst, batch_size):\n",
    "    \"\"\"  Yields bacth of specified size \"\"\"\n",
    "    if batch_size<=0:\n",
    "       return\n",
    "    for i in range(0, len(lst), batch_size):\n",
    "        yield lst[i : i + batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "\n",
    "look_distance = .1  # How far to look for neighbours.\n",
    "kernel_bandwidthLon = .35  # Longitude Kernel parameter.\n",
    "kernel_bandwidthLat = .08  # Latitude Kernel parameter.\n",
    "n_iterations = 20 # Number of iterations\n",
    "\n",
    "# num_cpu = 15\n",
    "# num_cpu = 7\n",
    "num_cpu = multiprocessing.cpu_count() // 2\n",
    "\n",
    "input_type = 'confidence_map'\n",
    "# input_type = 'mask'\n",
    "\n",
    "print(num_cpu)\n",
    "\n",
    "for batch in generate_batch(range(len(wl_list)), 400):\n",
    "    idx_start, idx_end = batch[0], batch[-1]\n",
    "    \n",
    "\n",
    "    print(idx_start, '-->', idx_end)\n",
    "    \n",
    "    \n",
    "    with concurrent.futures.ProcessPoolExecutor(max_workers=int(num_cpu)) as executor:\n",
    "        for result_key, result_dict in tqdm(executor.map(c_utils.process_one_image, \n",
    "                                                                    wl_list[idx_start:idx_end],\n",
    "                                                                    repeat(huge_db_dict),\n",
    "                                                                    repeat(deepcopy(huge_dict)),\n",
    "                                                                    repeat(wl_list),\n",
    "                                                                    repeat(rotten_list),\n",
    "                                                                    repeat(masks_dir),\n",
    "                                                                    repeat(look_distance),\n",
    "                                                                    repeat(kernel_bandwidthLon),\n",
    "                                                                    repeat(kernel_bandwidthLat),\n",
    "                                                                    repeat(n_iterations),\n",
    "                                                                    repeat(input_type),)\n",
    "#                                                                     chunksize=max(round(len(wl_list[idx_start:idx_end])//num_cpu),1\n",
    "                                                                , \n",
    "                                                    ):\n",
    "                    if not len(list(result_dict.keys())) == 0:\n",
    "#                         print(result_key)\n",
    "                        huge_dict[result_key] = deepcopy(result_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for im in tqdm(wl_list[:]):\n",
    "    result_key, result_dict = c_utils.process_one_image(\n",
    "                                                                im,\n",
    "                                                                huge_db_dict,\n",
    "                                                                huge_dict,\n",
    "                                                                wl_list,\n",
    "                                                                rotten_list,\n",
    "                                                                masks_dir,\n",
    "                                                                look_distance,\n",
    "                                                                kernel_bandwidthLon,\n",
    "                                                                kernel_bandwidthLat,\n",
    "                                                                n_iterations,\n",
    "                                                                input_type\n",
    "                                                        )\n",
    "#     print(result_key)\n",
    "    if not len(list(result_dict.keys())) == 0:\n",
    "        huge_dict[result_key] = result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(root_dir+'/test/meanshift_Classification.json', 'w') as f:\n",
    "    json.dump(huge_dict, f, cls=NpEncoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(root_dir+'/test/meanshift_Classification.json', 'r') as f:\n",
    "    huge_dict = json.load(f)\n",
    "print(len(list(huge_dict.keys())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# View Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "colors = ['tab:blue','tab:orange','tab:green','tab:red',\n",
    "          'tab:purple','tab:brown','tab:pink','tab:gray',\n",
    "          'tab:olive','tab:cyan']\n",
    "\n",
    "from matplotlib.colors import ListedColormap, LinearSegmentedColormap   \n",
    "cmap_gt = cm.autumn\n",
    "cmap_gt = cmap_gt(range(255))\n",
    "cmap_gt = ListedColormap([(0, 0, 0, 0), *cmap_gt])\n",
    "\n",
    "cmap = plt.get_cmap('autumn')\n",
    "cmap.set_under((0,0,0,0))\n",
    "\n",
    "def my_refresh(value):\n",
    "    ccc = fig.gca()\n",
    "    xlims0 = ccc.get_xlim()\n",
    "    ylims0 = ccc.get_ylim()\n",
    "    \n",
    "    ccc.set_visible(False) \n",
    "    \n",
    "    basename = list(huge_dict.keys())[img_selector.value]\n",
    "    tmp_idx = wl_basenames.index(basename)\n",
    "    wl = wl_list[tmp_idx]\n",
    "     \n",
    "    #####################\n",
    "    \n",
    "    cur_db_dict = huge_db_dict[basename]\n",
    "    \n",
    "    group_list = cur_db_dict[\"group_list\"]\n",
    "    drawing_radius_mm = cur_db_dict[\"dr_radius_mm\"]\n",
    "    drawing_radius_px = cur_db_dict[\"dr_radius_px\"]\n",
    "    date2 = cur_db_dict[\"dr_date\"]\n",
    "    date = cur_db_dict[\"wl_date\"] \n",
    "    \n",
    "    #####################\n",
    "    m, h = utils.open_and_add_celestial(wl_list[tmp_idx])\n",
    "\n",
    "    corrected = False\n",
    "    \n",
    "    if not 'DATE-OBS' in h:\n",
    "        m, h = utils.open_and_add_celestial2(wl_list[tmp_idx], date_obs=date)\n",
    "        corrected = True\n",
    "        \n",
    "    wcs = WCS(h)\n",
    "    wcs.heliographic_observer = m.observer_coordinate\n",
    "    origin = m.data.shape[0]//2, m.data.shape[1]//2\n",
    "    \n",
    "    if input_type == \"mask\":\n",
    "        mask = io.imread(os.path.join(masks_dir,basename+\".png\"))\n",
    "    elif input_type == \"confidence_map\":\n",
    "        mask = np.load(os.path.join(masks_dir,basename+\"_proba_map.npy\"))\n",
    "        \n",
    "    print(m.data.shape , mask.shape)\n",
    "    \n",
    "    m_rot = m.rotate(angle=-h[\"SOLAR_P0\"] * u.deg)\n",
    "    top_right = SkyCoord( 1000 * u.arcsec, 1000 * u.arcsec, frame=m_rot.coordinate_frame)\n",
    "    bottom_left = SkyCoord(-1000 * u.arcsec, -1000 * u.arcsec, frame=m_rot.coordinate_frame)\n",
    "    m_rot_submap = m_rot.submap(bottom_left, top_right=top_right)\n",
    "    m_rot_submap_shape = m_rot_submap.data.shape\n",
    "    m_rot_shape = m_rot.data.shape\n",
    "    deltashapeX = np.abs(m_rot_shape[0]-m_rot_submap_shape[0])\n",
    "    deltashapeY = np.abs(m_rot_shape[1]-m_rot_submap_shape[1]) \n",
    "\n",
    "    h2 = m_rot_submap.fits_header \n",
    "\n",
    "    h2.append(('CTYPE1', 'HPLN-TAN'))\n",
    "    h2.append(('CTYPE2', 'HPLT-TAN'))\n",
    "    wcs2 = WCS(h2)\n",
    "    wcs2.heliographic_observer = m_rot_submap.observer_coordinate\n",
    "    origin = m_rot_submap.data.shape[0]//2, m_rot_submap.data.shape[1]//2\n",
    "    \n",
    "    axClustering = fig.add_subplot(projection=m_rot_submap)\n",
    "    m_rot_submap.plot(axes=axClustering, interpolation='None')\n",
    "    m_rot_submap.draw_grid()   \n",
    "    \n",
    "    disp_mask = c_utils.rotate_CV_bound(mask, angle=h[\"SOLAR_P0\"], interpolation=cv2.INTER_NEAREST) #rotate(mask, angle=h[\"SOLAR_P0\"], reshape=True)\n",
    "    disp_mask = disp_mask[deltashapeX//2:disp_mask.shape[0]-deltashapeX//2,\n",
    "                          deltashapeY//2:disp_mask.shape[1]-deltashapeY//2] \n",
    "    axClustering.imshow(disp_mask, cmap=cmap, interpolation=\"None\", alpha=.5 )\n",
    "    \n",
    "    axClustering.set_title(axClustering.get_title()+ f' [corrected: {corrected}]'  + f'-> drawing: {date2}' )\n",
    "    \n",
    "    ####################\n",
    "    dr_obstime = date+'.000'  \n",
    "    all_sks = []\n",
    "    all_pixels = []\n",
    "    for item in group_list:\n",
    "        cur_sk = SkyCoord(item[\"Longitude\"]*u.rad, item[\"Latitude\"]*u.rad , frame=frames.HeliographicCarrington,\n",
    "                      obstime=dr_obstime, observer=\"earth\") \n",
    "        coords_wl = skycoord_to_pixel(cur_sk, wcs2, origin=0)\n",
    "        all_sks.append(cur_sk)\n",
    "        all_pixels.append(coords_wl)\n",
    "        \n",
    "    bboxes, bboxes_wl, rectangles, rectangles_wl = c_utils.grouplist2bboxes_and_rectangles(group_list, \n",
    "                                                                                   drawing_radius_px,\n",
    "                                                                                   h[\"SOLAR_R\"],\n",
    "                                                                                   all_pixels)\n",
    "    for cur_sk in all_sks:\n",
    "        axClustering.plot_coord(cur_sk, \"o\", color='b', markersize=2)\n",
    "\n",
    "    for i, r in enumerate(rectangles_wl):\n",
    "        axClustering.add_patch(r)\n",
    "        if areas_cb.value:\n",
    "            axClustering.text(bboxes_wl[i][0]+3, bboxes_wl[i][1]+3, \n",
    "                          f' {group_list[i][\"McIntosh\"]} : {group_list[i][\"area_muHem\"]}',color='b') \n",
    "    \n",
    "    ####################\n",
    "    \n",
    "    sunspots_sk, sunspots_areas = c_utils.get_sunspots3(h,m, mask>0, sky_coords=True)\n",
    "    sk_Lon = sunspots_sk.lon.rad\n",
    "    sk_Lat = sunspots_sk.lat.rad\n",
    "    sk_LatLon = np.stack((sk_Lat,sk_Lon),axis=1)\n",
    "    \n",
    "    nan_indexes = np.unique(np.argwhere(np.isnan(sk_LatLon))[:,0])\n",
    "    clean = (~np.isnan(sk_Lon) & ~np.isnan(sk_Lat))\n",
    "    if len(nan_indexes) > 0:\n",
    "        sunspots_sk = sunspots_sk[clean]\n",
    "        sunspots_areas = (np.array(sunspots_areas)[clean]).tolist()\n",
    "        sk_LatLon = sk_LatLon [clean]\n",
    "    \n",
    "    cur_dict = huge_dict[basename]\n",
    "    \n",
    "    ######################\n",
    "    if ms_cb.value:\n",
    "        cur_ms = cur_dict[\"meanshift\"]\n",
    "\n",
    "        ms_centroids = np.array(cur_ms['centroids'])\n",
    "        ms_areas = np.array(cur_ms['areas'])\n",
    "        ms_groups = np.array(cur_ms['groups'])\n",
    "        \n",
    "        my_ms = MS.Mean_Shift(look_distance, kernel_bandwidthLon, kernel_bandwidthLat, sunspots_sk.radius.km[0] ,  n_iterations)\n",
    "        ##########\n",
    "        my_ms.set_centroids(ms_centroids)\n",
    "        ###########\n",
    "        ms_classifications = my_ms.predict(sk_LatLon)\n",
    "\n",
    "        # print(m.date, '  ', date )\n",
    "        sk_sequ_meanshift = SkyCoord(ms_centroids[:,1]*u.rad, ms_centroids[:,0]*u.rad , frame=frames.HeliographicCarrington,\n",
    "                              obstime=m.date, observer=\"earth\")\n",
    "        \n",
    "        pix_centers_meanshift = skycoord_to_pixel(sk_sequ_meanshift, wcs2, origin=0)\n",
    "\n",
    "        for i, sk in enumerate(sk_sequ_meanshift):\n",
    "                axClustering.plot_coord(sk, \"X\", color=colors[i%len(colors)], markersize=8)\n",
    "                if areas_cb.value:\n",
    "                    axClustering.text(pix_centers_meanshift[0][i], pix_centers_meanshift[1][i],  '%.2f' % ms_areas[i] ,va='top',c=colors[i%len(colors)])\n",
    "        for i in range(len(sk_LatLon)):\n",
    "            c = colors[ms_classifications[i]%len(colors)]\n",
    "            axClustering.plot_coord(sunspots_sk[i], \"o\", color=c, markersize=2)\n",
    "\n",
    "look_distance = .1 # How far to look for neighbours.\n",
    "kernel_bandwidthLon = .3  # Longitude Kernel parameter.\n",
    "kernel_bandwidthLat = .08  # Latitude Kernel parameter.\n",
    "n_iterations = 20 # Number of iterations\n",
    "\n",
    "\n",
    "input_type = \"confidence_map\"\n",
    "# input_type = \"mask\"\n",
    "\n",
    "huge_dict = {k: huge_dict[k] for k in sorted(list(huge_dict.keys()))}\n",
    "\n",
    "db_drawings_datetimes = [c_utils.db_string_to_datetime(item[0]) for item in c_utils.get_unique_drawing_datetimes(database,'drawings')]\n",
    "dr_basenames = [c_utils.datetime_to_drawing_name(item) for item in db_drawings_datetimes]\n",
    "    \n",
    "img_selector = widgets.IntSlider(value=158, min=0, max=len(list(huge_dict.keys()))-1)\n",
    "areas_cb = widgets.Checkbox(value=False, description=\"Show Areas\")\n",
    "km_cb = widgets.Checkbox(value=False, description=\"Show KMeans\")\n",
    "ms_cb = widgets.Checkbox(value=True, description=\"Show MeanShift\")\n",
    "\n",
    "img_selector.observe(my_refresh)\n",
    "areas_cb.observe(my_refresh)\n",
    "ms_cb.observe(my_refresh)\n",
    "km_cb.observe(my_refresh)\n",
    "\n",
    "plt.ioff()\n",
    "fig, ax_widget = plt.subplots(nrows=1, ncols=1, figsize=(8,8))\n",
    "my_refresh(0)\n",
    "plt.ion()\n",
    "\n",
    "widgets.VBox([widgets.HBox([img_selector, areas_cb, ms_cb]),fig.canvas])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import rotate as rotate_image\n",
    "\n",
    "def rotate_img_opencv(image, angle, interpolation):\n",
    "    (h, w) = image.shape[:2]\n",
    "    (cX, cY) = (w // 2, h // 2)\n",
    "\n",
    "    M = cv2.getRotationMatrix2D((cX, cY), angle, 1.0)\n",
    "    rotated = cv2.warpAffine(image, M, (w, h))\n",
    "    return rotated\n",
    "\n",
    "def my_refresh(value):\n",
    "    \n",
    "    basename = list(huge_dict.keys())[img_selector.value]\n",
    "    tmp_idx = wl_basenames.index(basename)\n",
    "    wl = wl_list[tmp_idx]\n",
    "    \n",
    "    m, h = utils.open_and_add_celestial(wl_list[tmp_idx])\n",
    "     \n",
    "    if input_type == \"mask\":\n",
    "        mask = io.imread(os.path.join(masks_dir,basename+\".png\"))\n",
    "    elif input_type == \"confidence_map\":\n",
    "#         print('here')\n",
    "        mask = np.load(os.path.join(masks_dir,basename+\"_proba_map.npy\"))\n",
    "    \n",
    "    ax2_widget[0].imshow(m.data, cmap='gray')\n",
    "    ax2_widget[0].imshow(mask, cmap=cmap, alpha = .5, vmin=.1)\n",
    "    ax2_widget[0].set_title(basename)\n",
    "    ax2_widget[0].invert_yaxis()\n",
    "    \n",
    "    m_rot = m.rotate(angle=-h[\"SOLAR_P0\"] * u.deg)\n",
    "    top_right = SkyCoord( 1000 * u.arcsec, 1000 * u.arcsec, frame=m_rot.coordinate_frame)\n",
    "    bottom_left = SkyCoord(-1000 * u.arcsec, -1000 * u.arcsec, frame=m_rot.coordinate_frame)\n",
    "    m_rot_submap = m_rot.submap(bottom_left, top_right=top_right)\n",
    "    m_rot_submap_shape = m_rot_submap.data.shape\n",
    "    m_rot_shape = m_rot.data.shape\n",
    "    deltashapeX = np.abs(m_rot_shape[0]-m_rot_submap_shape[0])\n",
    "    deltashapeY = np.abs(m_rot_shape[1]-m_rot_submap_shape[1]) \n",
    "    \n",
    "    \n",
    "    rot_img = c_utils.rotate_CV_bound(m_rot_submap.data, angle=h[\"SOLAR_P0\"], interpolation=cv2.INTER_NEAREST) #rotate(mask, angle=h[\"SOLAR_P0\"], reshape=True)\n",
    "    disp_mask = c_utils.rotate_CV_bound(mask, angle=h[\"SOLAR_P0\"], interpolation=cv2.INTER_NEAREST) #rotate(mask, angle=h[\"SOLAR_P0\"], reshape=True)\n",
    "    disp_mask = disp_mask[deltashapeX//2:disp_mask.shape[0]-deltashapeX//2,\n",
    "                          deltashapeY//2:disp_mask.shape[1]-deltashapeY//2] \n",
    "    \n",
    "    ax2_widget[1].imshow(m_rot_submap.data, cmap='gray')\n",
    "    ax2_widget[1].imshow(disp_mask, cmap=cmap, alpha = .5, vmin=.1)\n",
    "    ax2_widget[1].set_title(h[\"SOLAR_P0\"])\n",
    "    ax2_widget[1].invert_yaxis()\n",
    "    \n",
    "\n",
    "input_type = \"confidence_map\"\n",
    "\n",
    "look_distance = .1 # How far to look for neighbours.\n",
    "kernel_bandwidthLon = .3  # Longitude Kernel parameter.\n",
    "kernel_bandwidthLat = .08  # Latitude Kernel parameter.\n",
    "n_iterations = 20 # Number of iterations\n",
    "\n",
    "\n",
    "db_drawings_datetimes = [c_utils.db_string_to_datetime(item[0]) for item in c_utils.get_unique_drawing_datetimes(database,'drawings')]\n",
    "dr_basenames = [c_utils.datetime_to_drawing_name(item) for item in db_drawings_datetimes]\n",
    "\n",
    "huge_dict = {k: huge_dict[k] for k in sorted(list(huge_dict.keys()))}\n",
    "\n",
    "    \n",
    "img_selector = widgets.IntSlider(value=3000, min=0, max=len(list(huge_dict.keys()))-1)\n",
    "mask_cb = widgets.Checkbox(value=True, description=\"Show Masks\")\n",
    "\n",
    "img_selector.observe(my_refresh)\n",
    "\n",
    "plt.ioff()\n",
    "fig2, ax2_widget = plt.subplots(nrows=1, ncols=2, figsize=(10,4))\n",
    "my_refresh(0)\n",
    "plt.ion()\n",
    "\n",
    "widgets.VBox([widgets.HBox([img_selector]),fig2.canvas])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Associate groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "image_out_dict = {}\n",
    "image_out_dict_stats = {}\n",
    "\n",
    "# for basename in list(huge_dict.keys())[3400:3401]:\n",
    "# for basename in list(huge_dict.keys())[:1]:\n",
    "#     cur_image_dict = huge_dict[basename]\n",
    "# for basename in [\"UPH20120912082643\",]:\n",
    "for basename in tqdm(list(huge_dict.keys())[:]):\n",
    "# for basename in [\"UPH20170623120332\"]:\n",
    "    cur_image_dict = huge_dict[basename]\n",
    "    \n",
    "    angle = cur_image_dict[\"SOLAR_P0\"]\n",
    "    deltashapeX = cur_image_dict[\"deltashapeX\"]\n",
    "    deltashapeY = cur_image_dict[\"deltashapeY\"]\n",
    "    \n",
    "    drawing_radius_px = huge_db_dict[basename][\"dr_radius_px\"]\n",
    "    \n",
    "    group_list = cur_image_dict['db']\n",
    "    \n",
    "    \n",
    "    ms_dict = cur_image_dict['meanshift']\n",
    "    \n",
    "    centroids = np.array(ms_dict[\"centroids\"])\n",
    "    centroids_px = np.array(ms_dict[\"centroids_px\"])\n",
    "#     print('centroids_px', centroids_px)\n",
    "    \n",
    "#     db_bboxes = grouplist2bboxes(group_list, drawing_radius_px)\n",
    "    db_classes = [{\"Zurich\":item['Zurich'], \"McIntosh\":item['McIntosh'] } for item in group_list]\n",
    "    db_bboxes = [np.array(item['bbox_wl']) for item in group_list]\n",
    "    db_centers_px = np.array([[(b[2]+b[0])/2,(b[3]+b[1])/2] for b in db_bboxes])\n",
    "    \n",
    "#     print(\"db_centers_px\", db_centers_px)\n",
    "#     print(\"db_bboxes\", db_bboxes)\n",
    "    \n",
    "        \n",
    "    # check that current bbox is does not overlap any\n",
    "    isolated_bboxes_bool = np.array(c_utils.get_intersecting_db_bboxes(db_bboxes)) == 0\n",
    "    isolated_bboxes_indices = np.where(isolated_bboxes_bool == True)[0]\n",
    "#     print(\"isolated_bboxes_bool\",isolated_bboxes_bool)\n",
    "#     print(isolated_bboxes_indices)\n",
    "    \n",
    "    cur_out_stats = {\n",
    "        # General info\n",
    "        'num_DB_groups':len(db_bboxes),\n",
    "        'num_MS_groups':len(centroids_px),\n",
    "        'num_DB_isolated_groups':len(isolated_bboxes_indices),\n",
    "        'num_DB_overlaping_bboxes':len(db_bboxes) - len(isolated_bboxes_indices),\n",
    "        # MS with DB matching info\n",
    "        \"num_MSmatchesDB\":0,\n",
    "        # MS with DB rejection info\n",
    "        \"num_noMS_but_DB_reject\":0,\n",
    "        \"num_singleMS_multipleDB_reject\":0,\n",
    "        \"num_oneDBbbox_multipleMSoverlap_ambiguity_reject\":0,\n",
    "        # MS with DB no match info\n",
    "        \"num_noDB_but_MS\":0,\n",
    "        }\n",
    "    cur_out_groups = []\n",
    "    for i, (db_bbox, db_center, db_class) in enumerate(\n",
    "                                            zip([db_bboxes[a] for a in isolated_bboxes_indices.tolist() ],\n",
    "                                                [db_centers_px[a] for a in isolated_bboxes_indices.tolist()],\n",
    "                                                [db_classes[a] for a in isolated_bboxes_indices.tolist()],\n",
    "                                               )):\n",
    "        \n",
    "        \n",
    "        ms_centroids, ms_members = centroids_px, ms_dict['groups_px']\n",
    "        \n",
    "        intersect = c_utils.contains_ms_groups(db_bbox, db_center, ms_centroids, ms_members)\n",
    "        \n",
    "        if sum(intersect) == 0: # Il n'y a eu aucune détection dans cette zone\n",
    "            cur_out_stats['num_noMS_but_DB_reject'] += 1\n",
    "            pass\n",
    "        elif sum(intersect) == 1: # il n'y a de l'overlap qu'avec un seul groupe meanshift            \n",
    "#             print('hit')\n",
    "            idx = intersect.index(True)\n",
    "#             print(idx)\n",
    "            # vérifier que le groupe meanshift n'intersecte aucune autre bbox\n",
    "            num_intersections = np.sum(c_utils.count_group_intersections(ms_members[idx], db_bboxes))\n",
    "            if num_intersections > 1:\n",
    "                cur_out_stats['num_singleMS_multipleDB_reject'] += 1\n",
    "                continue\n",
    "            \n",
    "            Rmm = huge_db_dict[basename]['dr_radius_mm']\n",
    "            R_pixel = huge_db_dict[basename]['dr_radius_px']\n",
    "            sun_center = huge_db_dict[basename]['dr_center_px']\n",
    "            dr_pixpos = np.array([group_list[i]['posx'], group_list[i]['posy']])\n",
    "            \n",
    "            angular_excentricity =  c_utils.get_angle2(dr_pixpos, R_pixel, sun_center)\n",
    "            \n",
    "            cur_group_dict={\n",
    "                            \"centroid_px\": centroids_px[idx],\n",
    "                            \"centroid_Lat\": centroids[idx][0],\n",
    "                            \"centroid_Lon\": centroids[idx][1],\n",
    "                            \"angular_excentricity_rad\": angular_excentricity,\n",
    "                            \"angular_excentricity_deg\": np.rad2deg(angular_excentricity),\n",
    "                            \"Zurich\":   db_class[\"Zurich\"],\n",
    "                            \"McIntosh\": db_class[\"McIntosh\"],\n",
    "                            \"members\": ms_members[idx],\n",
    "                            \"members_mean_px\": np.mean(ms_members[idx], axis=0),\n",
    "                           }\n",
    "            \n",
    "            \n",
    "            cur_out_groups.append(cur_group_dict)\n",
    "            cur_out_stats['num_MSmatchesDB'] += 1\n",
    "\n",
    "        else: # db_bbox intersecte plusieurs groupes meanshift\n",
    "            cur_out_stats['num_oneDBbbox_multipleMSoverlap_ambiguity_reject'] += 1\n",
    "            pass\n",
    "            \n",
    "    if len(cur_out_groups) > 0:\n",
    "        image_out_dict[basename] = { \"angle\": angle,\n",
    "                                     \"deltashapeX\":deltashapeX,\n",
    "                                     \"deltashapeY\":deltashapeY,\n",
    "                                     \"groups\": cur_out_groups,\n",
    "                                   }\n",
    "\n",
    "    # count the number of MS groups that do not have any overlap with the DB\n",
    "    num_intersections_per_group = [np.sum(c_utils.count_group_intersections(ms_members[idx], db_bboxes)) for idx in range(len(ms_members))]\n",
    "    num_MS_without_DB_overlap = len(np.where(np.array(num_intersections_per_group) == 0)[0])\n",
    "    cur_out_stats['num_noDB_but_MS'] = num_MS_without_DB_overlap\n",
    "    \n",
    "    image_out_dict_stats[basename] = cur_out_stats\n",
    "\n",
    "    \n",
    "\n",
    "print('num_images: ', len(list(image_out_dict.keys())))\n",
    "num_groups = 0\n",
    "for k,v in image_out_dict.items():\n",
    "    num_groups += len(v['groups'])\n",
    "print(\"num_groups: \",num_groups)\n",
    "# print(image_out_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot some stats\n",
    "# General info\n",
    "num_DB_groups_per_image = np.array([item['num_DB_groups'] for k,item in image_out_dict_stats.items()])\n",
    "num_MS_groups_per_image = np.array([item['num_MS_groups'] for k,item in image_out_dict_stats.items()])\n",
    "\n",
    "diff_num_groups = num_DB_groups_per_image - num_MS_groups_per_image # should be shown on histogram , closer to 0 is better\n",
    "\n",
    "# number of isolated DB groups per image and mean number in dataset\n",
    "num_isolated_DB_groups = np.array([v['num_DB_isolated_groups'] for k,v in image_out_dict_stats.items()])\n",
    "\n",
    "# numbers of rejects due to single MS group overlapping multiple DB groups (# minimize this)\n",
    "num_singleMS_multipleDB_reject = np.array([v['num_singleMS_multipleDB_reject'] for k,v in image_out_dict_stats.items()])\n",
    "# numbers of rejects due to single DB group overlapping multiple MS groups (# minimize this)\n",
    "num_oneDBbbox_multipleMSoverlap_ambiguity_reject = np.array([v['num_oneDBbbox_multipleMSoverlap_ambiguity_reject'] for k,v in image_out_dict_stats.items()])\n",
    "# numbers of rejects due to no MS group overlapping DB group (# cannot do anything to this, examples are discarded)\n",
    "num_noMS_but_DB_reject = np.array([v['num_noMS_but_DB_reject'] for k,v in image_out_dict_stats.items()])\n",
    "\n",
    "num_optimizable_rejects = num_singleMS_multipleDB_reject + num_oneDBbbox_multipleMSoverlap_ambiguity_reject\n",
    "num_rejects_all = num_optimizable_rejects + num_noMS_but_DB_reject\n",
    "\n",
    "# rate of optimizable rejects\n",
    "rate_optimizable_rejects = num_optimizable_rejects / num_rejects_all\n",
    "\n",
    "# Numbers over whole dataset\n",
    "num_DB_groups = np.sum(num_DB_groups_per_image)\n",
    "num_MS_groups = np.sum(num_MS_groups_per_image)\n",
    "\n",
    "num_isolated_DB_groups_total = np.sum(num_isolated_DB_groups) # cannot do anything to this, maximum value possible\n",
    "rate_isolated_DB_groups_total = num_isolated_DB_groups_total / num_DB_groups\n",
    "num_overlapping_DB_groups_total = num_DB_groups - num_isolated_DB_groups_total # cannot do anything to this, examples are discarded\n",
    "rate_overlapping_DB_groups_total = num_overlapping_DB_groups_total / num_DB_groups\n",
    "\n",
    "num_singleMS_multipleDB_reject_total = np.sum(num_singleMS_multipleDB_reject) # minimize this\n",
    "num_oneDBbbox_multipleMSoverlap_ambiguity_reject_total = np.sum(num_oneDBbbox_multipleMSoverlap_ambiguity_reject) # minimize this\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot some stats\n",
    "# General info\n",
    "num_DB_groups_per_image = np.array([item['num_DB_groups'] for k,item in image_out_dict_stats.items()])\n",
    "num_MS_groups_per_image = np.array([item['num_MS_groups'] for k,item in image_out_dict_stats.items()])\n",
    "\n",
    "diff_num_groups = num_DB_groups_per_image - num_MS_groups_per_image # should be shown on histogram , closer to 0 is better\n",
    "\n",
    "# number of isolated DB groups per image and mean number in dataset\n",
    "num_isolated_DB_groups = np.array([v['num_DB_isolated_groups'] for k,v in image_out_dict_stats.items()])\n",
    "mean_num_isolated_DB_groups = np.mean(num_isolated_DB_groups) # not very useful\n",
    "rate_isolated_DB_groups = num_isolated_DB_groups / num_DB_groups_per_image # not very useful\n",
    "mean_rate_isolated_DB_groups = np.mean(rate_isolated_DB_groups) # not very useful\n",
    "std_rate_isolated_DB_groups = np.std(rate_isolated_DB_groups) # not very useful\n",
    "\n",
    "# Numbers over whole dataset\n",
    "num_DB_groups = np.sum(num_DB_groups_per_image)\n",
    "num_MS_groups = np.sum(num_MS_groups_per_image)\n",
    "num_isolated_DB_groups_total = np.sum(num_isolated_DB_groups)\n",
    "rate_isolated_DB_groups_total = num_isolated_DB_groups_total / num_DB_groups\n",
    "\n",
    "# number of MS groups without DB overlap per image\n",
    "num_MS_groups_without_DB_overlap = np.array([v['num_noDB_but_MS'] for k,v in image_out_dict_stats.items()])\n",
    "mean_num_MS_groups_without_DB_overlap = np.mean(num_MS_groups_without_DB_overlap) # not very useful\n",
    "rate_MS_groups_without_DB_overlap = num_MS_groups_without_DB_overlap / num_MS_groups_per_image\n",
    "mean_rate_MS_groups_without_DB_overlap = np.mean(rate_MS_groups_without_DB_overlap)\n",
    "std_rate_MS_groups_without_DB_overlap = np.std(rate_MS_groups_without_DB_overlap)\n",
    "\n",
    "#number of DB groups without MS overlap per image\n",
    "num_DB_groups_without_MS_overlap = np.array([v['num_noMS_but_DB_reject'] for k,v in image_out_dict_stats.items()])\n",
    "mean_num_DB_groups_without_MS_overlap = np.mean(num_DB_groups_without_MS_overlap) # not very useful\n",
    "rate_DB_groups_without_MS_overlap = num_DB_groups_without_MS_overlap / num_DB_groups_per_image\n",
    "mean_rate_DB_groups_without_MS_overlap = np.mean(rate_DB_groups_without_MS_overlap)\n",
    "std_rate_DB_groups_without_MS_overlap = np.std(rate_DB_groups_without_MS_overlap)\n",
    "\n",
    "\n",
    "# total number of rejects per image\n",
    "total_rejects = np.sum([v['num_noMS_but_DB_reject'] + \n",
    "                        v['num_singleMS_multipleDB_reject'] +\n",
    "                        v['num_oneDBbbox_multipleMSoverlap_ambiguity_reject'] \n",
    "                        for k,v in image_out_dict_stats.items()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#print the number of groups per image in huge_db_dict\n",
    "for i,(k,v) in enumerate(huge_db_dict.items()):\n",
    "    idx =  i\n",
    "    img_name = k\n",
    "    db_num_groups = len(v['group_list'])\n",
    "    found_groups = len(huge_dict[k][\"meanshift\"]['centroids']) if k in huge_dict.keys() else 0\n",
    "    matched_groups = len(image_out_dict[k]['groups']) if k in image_out_dict.keys() else 0\n",
    "\n",
    "    \n",
    "    print(f'idx {idx}: {img_name} DB num_groups: {db_num_groups} Found Groups: {found_groups} Matched Groups: {matched_groups}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_json_dir = 'feb2023'\n",
    "out_json_filename = 'dataset'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(f'{root_dir}/test/{out_json_filename}.json', 'w') as f:\n",
    "    json.dump(image_out_dict, f, cls=NpEncoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(f'{root_dir}/test/{out_json_filename}.json', 'r') as f:\n",
    "    image_out_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) analyse the distributions\n",
    "\n",
    "classes = ['A','B','C','D','E','F','G','H','I','J','X']\n",
    "\n",
    "distribs = {c: 0 for c in classes}\n",
    "group_types = {}\n",
    "group_types2 = {c:{} for c in classes}\n",
    "\n",
    "for bn, img_dict in tqdm(image_out_dict.items()):\n",
    "    groups = img_dict['groups']\n",
    "    \n",
    "    for i, g in enumerate(groups):\n",
    "        cur_c = g[\"Zurich\"]\n",
    "        distribs[cur_c] +=1\n",
    "        \n",
    "        new_group_infos= {\n",
    "            'angle': img_dict['angle'],\n",
    "            'deltashapeX': img_dict['deltashapeX'],\n",
    "            'deltashapeY':img_dict['deltashapeY'],\n",
    "            'centroid_px': g['centroid_px'],\n",
    "            'centroid_Lat': g['centroid_Lat'],\n",
    "            'centroid_Lon': g['centroid_Lon'],\n",
    "            'members': g['members'],\n",
    "            'members_mean_px': g['members_mean_px'],\n",
    "            'angular_excentricity_rad': g['angular_excentricity_rad'],\n",
    "            'angular_excentricity_deg': g['angular_excentricity_deg'],\n",
    "            'Zurich': g['Zurich'],\n",
    "            'McIntosh': g['McIntosh'],   \n",
    "        }\n",
    "        \n",
    "        new_goup_id = bn + '_' + str(i)\n",
    "        group_types[new_goup_id] = new_group_infos\n",
    "        group_types2[cur_c][new_goup_id] = new_group_infos\n",
    "        \n",
    "    \n",
    "print(distribs)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_based_dataset = deepcopy(group_types)\n",
    "group_based_dataset2 = deepcopy(group_types2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "First2superFirst = {\"A\":\"A\",\n",
    "                    \"B\":\"B\",\n",
    "                    \"C\":\"C\",\n",
    "                    \"D\":\"SuperGroup\",\n",
    "                    \"E\":\"SuperGroup\",\n",
    "                    \"F\":\"SuperGroup\",\n",
    "                    \"H\":\"H\",\n",
    "                    \"X\":\"X\"\n",
    "                   }\n",
    "Second2superSecond = {\"x\":\"x\",\n",
    "                      \"r\":\"r\",\n",
    "                      \"s\": \"sym\",\n",
    "                      \"h\": \"sym\",\n",
    "                      \"a\": \"asym\",\n",
    "                      \"k\": \"asym\",\n",
    "                     }\n",
    "Third2superThird = {\"x\": \"x\",\n",
    "                    \"o\": \"o\",\n",
    "                    \"i\": \"frag\",\n",
    "                    \"c\": \"frag\",\n",
    "                   }\n",
    "\n",
    "def add_superclasses(group_dict):\n",
    "    cpy = deepcopy(group_dict)    \n",
    "    # print(cpy)\n",
    "\n",
    "    cpy[\"SuperClass\"] = {\n",
    "        \"1\": First2superFirst[group_dict[\"McIntosh\"][0]],\n",
    "        \"2\": Second2superSecond[group_dict[\"McIntosh\"][1]],\n",
    "        \"3\": Third2superThird[group_dict[\"McIntosh\"][2]],\n",
    "    }\n",
    "    \n",
    "    return cpy\n",
    "    \n",
    "    \n",
    "\n",
    "grp_to_remove = []\n",
    "group_based_dataset_superclasses = {}\n",
    "for g in tqdm(group_based_dataset):\n",
    "    try : \n",
    "        group = group_based_dataset[g]\n",
    "        # print(group)\n",
    "        group = add_superclasses(group)\n",
    "#         print(group)\n",
    "        group_based_dataset_superclasses[g] = group\n",
    "        \n",
    "    except KeyError:\n",
    "        print(g)\n",
    "        print(group_based_dataset[g])\n",
    "        if group_based_dataset[g][\"McIntosh\"] == '   ':\n",
    "            print( \"error\")\n",
    "            grp_to_remove.append((g,group_based_dataset[g]['Zurich']))\n",
    "\n",
    "for k,k_type in grp_to_remove:\n",
    "    group_based_dataset.pop(k)\n",
    "    group_based_dataset2[k_type].pop(k)\n",
    "\n",
    "# group_based_dataset_superclasses\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split per type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distribs2 = {c:0 for c in classes}\n",
    "group_types2 = {c:{} for c in classes}\n",
    "\n",
    "for grp_id, grp_dict in tqdm(group_based_dataset_superclasses.items()):\n",
    "    cur_c = grp_dict[\"Zurich\"]\n",
    "    group_types2[cur_c][grp_id] = grp_dict\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2:  Split groups among train, val, test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "def splitPerc(l, perc):\n",
    "    # Turn percentages into values between 0 and 1\n",
    "    splits = np.cumsum(perc)/100.\n",
    "\n",
    "    if splits[-1] != 1:\n",
    "        raise ValueError(\"percents don't add up to 100\")\n",
    "\n",
    "    # Split doesn't need last percent, it will just take what is left\n",
    "    splits = splits[:-1]\n",
    "\n",
    "    # Turn values into indices\n",
    "    splits *= len(l)\n",
    "\n",
    "    # Turn double indices into integers.\n",
    "    # CAUTION: numpy rounds to closest EVEN number when a number is halfway\n",
    "    # between two integers. So 0.5 will become 0 and 1.5 will become 2!\n",
    "    # If you want to round up in all those cases, do\n",
    "    # splits += 0.5 instead of round() before casting to int\n",
    "    splits = splits.round().astype(np.int)\n",
    "\n",
    "    return np.split(l, splits)\n",
    "\n",
    "splits = ['train', 'val', 'test']\n",
    "\n",
    "splits_percentages = [70, 15, 15]\n",
    "\n",
    "assert np.array(splits_percentages).sum() == 100\n",
    "\n",
    "group_based_dataset_superclasses_splits = {sp:{} for sp in splits}\n",
    "\n",
    "for t, type_dict in group_types2.items():\n",
    "    list_type_groups = list(type_dict.keys()) \n",
    "    # shuffle\n",
    "    random.shuffle(list_type_groups)\n",
    "    \n",
    "    indices = np.array(range(len(list_type_groups)))\n",
    "    \n",
    "    s = splitPerc(indices, splits_percentages)\n",
    "\n",
    "    # take percentage and fill group_based_dataset\n",
    "    for i, sp in enumerate(splits):\n",
    "        split_indices = s[i]\n",
    "        split_groups = [list_type_groups[j] for j in split_indices]\n",
    "        \n",
    "#         print(split_groups)\n",
    "        for g in  split_groups:\n",
    "            group_based_dataset_superclasses_splits[sp][g] = type_dict[g] \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_json = \"dataset_nosplits\"\n",
    "\n",
    "with open(f'{root_dir}/test/{final_json}.json', 'w') as f:\n",
    "    json.dump(group_based_dataset_superclasses, f, cls=NpEncoder)\n",
    "final_json_split = \"dataset_final\"\n",
    "with open(f'{root_dir}/test/{final_json_split}.json', 'w') as f:\n",
    "    json.dump(group_based_dataset_superclasses_splits, f, cls=NpEncoder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "vscode": {
   "interpreter": {
    "hash": "1f3e85867ab3feeb73691fcc67a502ec8f0fc265745d17c9ab3a5329e7f22e4e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
