{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from email.mime import base\n",
    "from tokenize import group\n",
    "from bioblue.dataset.transform.pipelines import Compose\n",
    "import collections\n",
    "from functools import partial\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "from hydra.utils import call, instantiate\n",
    "from pathlib import Path\n",
    "import skimage.io as io\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import traceback\n",
    "import time\n",
    "\n",
    "from albumentations.core.transforms_interface import DualTransform\n",
    "\n",
    "from astropy.io import fits\n",
    "from bioblue.dataset.utils import *\n",
    "\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "import pickle\n",
    "\n",
    "import concurrent.futures\n",
    "from itertools import repeat\n",
    "import multiprocessing\n",
    "\n",
    "%matplotlib widget\n",
    "# %matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_elapsed_time(st, msg):\n",
    "        end_time = time.time()\n",
    "        print(f'Elapsed time {msg}: {end_time - st}')\n",
    "\n",
    "class ClassificationDatasetSuperclasses(Dataset):\n",
    "\n",
    "    def __init__(\n",
    "        self, root_dir, partition, dtypes, classes,\n",
    "        first_classes, second_classes, third_classes, \n",
    "        json_file, classification='SuperClass' , transforms=None) -> None:\n",
    "        super().__init__()\n",
    "        if isinstance(transforms, collections.Mapping):\n",
    "            transforms = partial(call, config=transforms)\n",
    "        elif isinstance(transforms, collections.Sequence):\n",
    "            transforms_init = []\n",
    "            for transform in transforms:\n",
    "                transforms_init.append(instantiate(transform))\n",
    "            transforms = Compose(transforms_init)\n",
    "\n",
    "        self.transforms = transforms\n",
    "        self.root_dir = Path(root_dir)\n",
    "\n",
    "        self.main_dtype = dtypes[0]\n",
    "        self.mask_dtype = dtypes[1]\n",
    "\n",
    "        self.json_file = self.root_dir / json_file \n",
    "        # print(self.json_file)\n",
    "        self.partition_dict = None\n",
    "\n",
    "        self.c1_mapper = {c: i for i,c in enumerate(first_classes)}\n",
    "        self.c2_mapper = {c: i for i,c in enumerate(second_classes)}\n",
    "        self.c3_mapper = {c: i for i,c in enumerate(third_classes)}\n",
    "\n",
    "\n",
    "        with open(self.json_file, 'r') as f:\n",
    "            self.partition_dict = json.load(f)[partition]\n",
    "\n",
    "        \n",
    "        assert (classification == 'Zurich') or (classification == 'McIntosh') or (classification == 'SuperClass')\n",
    "\n",
    "        self.classification = classification\n",
    "\n",
    "        self.FirstClass_mapper = {c: i for i,c in enumerate(first_classes)}\n",
    "        self.SecondClass_mapper = {c: i for i,c in enumerate(second_classes)}\n",
    "        self.ThirdClass_mapper = {c: i for i,c in enumerate(third_classes)}\n",
    "\n",
    "        # print(classes)\n",
    "        self.files = {}\n",
    "        for i, bn in enumerate(sorted(list(self.partition_dict.keys()))):\n",
    "            bn = bn.split('_')[0]\n",
    "            # print(bn)\n",
    "            cur = {}\n",
    "            image_basename = bn + '.FTS'\n",
    "            image_filename = self.root_dir / self.main_dtype / image_basename\n",
    "\n",
    "            sun_mask_filename = self.root_dir / 'sun_mask' / (bn + '.png')\n",
    "\n",
    "\n",
    "            mask_basename = bn + '.png'\n",
    "            mask_filename = self.root_dir / self.mask_dtype / mask_basename\n",
    "\n",
    "            conf_map_basename = bn + '_proba_map.npy'\n",
    "            conf_map_filename = self.root_dir / self.mask_dtype / conf_map_basename\n",
    "\n",
    "            cur[\"name\"] = bn\n",
    "            cur[self.main_dtype] = image_filename\n",
    "            cur[self.mask_dtype] = mask_filename\n",
    "            cur[self.mask_dtype+\"_conf_map\"] = conf_map_filename\n",
    "            cur[\"sun_mask\"] = sun_mask_filename\n",
    "\n",
    "            self.files[bn] = cur\n",
    "\n",
    "        # print(self.files)\n",
    "\n",
    "        self.partition_dict\n",
    "\n",
    "        # print(list(self.partition_dict.values())[0])\n",
    "\n",
    "        self.groups = {}\n",
    "        for k,v in self.partition_dict.items():\n",
    "\n",
    "            if v[self.classification][\"1\"] in classes:\n",
    "                    self.groups[k] = v\n",
    "            else:\n",
    "                # print(v[self.classification][\"1\"])\n",
    "                pass\n",
    "\n",
    "      \n",
    "\n",
    "        self.dataset_length = len(list(self.groups.keys()))\n",
    "\n",
    "\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        # raise NotImplementedError\n",
    "        # print(self.dataset_length)\n",
    "        return self.dataset_length\n",
    "        # return 10\n",
    "    \n",
    "    def __getitem__(self, index: int, do_transform=True):\n",
    "\n",
    "        # st = time.time()\n",
    "\n",
    "        sample = {} # dictionnary with 'image', 'class', 'angular_excentricity', 'centroid_lat'\n",
    "\n",
    "        # basename = self.files[index][\"name\"]\n",
    "        k = sorted(list(self.groups.keys()))[index]\n",
    "        # print(k)\n",
    "        basename = k.split('_')[0]\n",
    "\n",
    "\n",
    "        # image_out_dict = self.partition_dict[basename]\n",
    "        group_dict = self.groups[k]\n",
    "\n",
    "        # print(group_dict)\n",
    "\n",
    "        img_name = self.files[basename][self.main_dtype] # path of FITS file\n",
    "        mask_name = self.files[basename][self.mask_dtype]\n",
    "        conf_map_name = self.files[basename][self.mask_dtype+\"_conf_map\"]\n",
    "\n",
    "        # print(img_name)\n",
    "        # st =  time.time()\n",
    "        hdulst:fits.HDUList = fits.open(img_name)\n",
    "        image = hdulst[0]\n",
    "        header = image.header\n",
    "        center = np.array(image.shape)//2\n",
    "        radius = header['SOLAR_R']\n",
    "        \n",
    "        # st = time.time()\n",
    "        sample['solar_disk'] = io.imread(self.files[basename][\"sun_mask\"])\n",
    "        # print_elapsed_time(st, 'load sun mask')\n",
    "\n",
    "\n",
    "\n",
    "        # st = time.time()\n",
    "        sample['excentricity_map'] = create_excentricity_map(sample['solar_disk'], radius, value_outside=-1)\n",
    "        # print_elapsed_time(st, 'create_excentricity_map')\n",
    "\n",
    "        # st = time.time()\n",
    "        sample['mask'] = io.imread(mask_name)#.astype(float)\n",
    "        sample['confidence_map'] = np.load(conf_map_name)\n",
    "        # print_elapsed_time(st, 'load mask and conf map')\n",
    "\n",
    "        # st = time.time()\n",
    "        sample['image'] = (image.data).astype(float)\n",
    "\n",
    "        sample['members'] = np.array(group_dict['members']) if 'members' in group_dict else np.array([0])\n",
    "        sample['members_mean_px'] = np.array(group_dict['members_mean_px']) if 'members_mean_px' in group_dict else np.array([0])\n",
    "\n",
    "        sample['name'] = basename\n",
    "        sample['group_name'] = k\n",
    "\n",
    "        sample['solar_angle'] = group_dict['angle']\n",
    "        sample['deltashapeX'] = group_dict['deltashapeX']\n",
    "        sample['deltashapeY'] = group_dict['deltashapeY']\n",
    "        \n",
    "        sample['angular_excentricity'] = np.array([group_dict[\"angular_excentricity_deg\"]])\n",
    "        sample['centroid_px'] = np.array(group_dict[\"centroid_px\"])\n",
    "        # print(sample['centroid_px'])\n",
    "        sample['centroid_Lat'] = np.array([group_dict[\"centroid_Lat\"]])\n",
    "\n",
    "        # sample['class'] = np.array([self.classes_mapper[group_dict[self.classification]]])\n",
    "        sample['class1'] = group_dict[self.classification]['1']\n",
    "        sample['class2'] = group_dict[self.classification]['2']\n",
    "        sample['class3'] = group_dict[self.classification]['3']\n",
    "        # sample['class1'] = np.array([self.FirstClass_mapper[group_dict[self.classification]['1']]])\n",
    "        # sample['class2'] = np.array([self.SecondClass_mapper[group_dict[self.classification]['2']]])\n",
    "        # sample['class3'] = np.array([self.ThirdClass_mapper[group_dict[self.classification]['3']]])\n",
    "        # print_elapsed_time(st, 'remaining operations')\n",
    "\n",
    "        if sample[\"image\"].shape == (1024,1024):\n",
    "            fig,ax = plt.subplots(2, 5, figsize=(10, 4) )\n",
    "            ax[0,0].imshow(sample[\"image\"], cmap='gray', interpolation='none')\n",
    "            ax[0,1].imshow(sample[\"mask\"], cmap='gray', interpolation='none')\n",
    "            ax[0,2].imshow(sample[\"confidence_map\"], cmap='gray', interpolation='none')\n",
    "            ax[0,3].imshow(sample[\"solar_disk\"], cmap='gray', interpolation='none')\n",
    "            ax[0,4].imshow(sample[\"excentricity_map\"], cmap='gray', interpolation='none')\n",
    "            # scatter the centroid\n",
    "            ax[0,0].scatter(sample['centroid_px'][0], sample['centroid_px'][1], c='r', s=10)\n",
    "\n",
    "            # print(sample[\"image\"].shape)\n",
    "            # double the resolution to 2048x2048 of all visual data\n",
    "            sample[\"image\"] = np.repeat(np.repeat(sample[\"image\"], 2, axis=0), 2, axis=1)\n",
    "            sample[\"mask\"] = np.repeat(np.repeat(sample[\"mask\"], 2, axis=0), 2, axis=1)\n",
    "            sample[\"confidence_map\"] = np.repeat(np.repeat(sample[\"confidence_map\"], 2, axis=0), 2, axis=1)\n",
    "            sample[\"solar_disk\"] = np.repeat(np.repeat(sample[\"solar_disk\"], 2, axis=0), 2, axis=1)\n",
    "            sample[\"excentricity_map\"] = np.repeat(np.repeat(sample[\"excentricity_map\"], 2, axis=0), 2, axis=1)\n",
    "            # print(sample[\"image\"].shape)\n",
    "            \n",
    "            # # also double the delta shape values\n",
    "            sample['deltashapeX'] = sample['deltashapeX']*2\n",
    "            sample['deltashapeY'] = sample['deltashapeY']*2\n",
    "\n",
    "            # also double the centroid values\n",
    "            sample['centroid_px'] = sample['centroid_px']*2\n",
    "\n",
    "\n",
    "            ax[1,0].imshow(sample[\"image\"], cmap='gray', interpolation='none')\n",
    "            ax[1,1].imshow(sample[\"mask\"], cmap='gray', interpolation='none')\n",
    "            ax[1,2].imshow(sample[\"confidence_map\"], cmap='gray', interpolation='none')\n",
    "            ax[1,3].imshow(sample[\"solar_disk\"], cmap='gray', interpolation='none')\n",
    "            ax[1,4].imshow(sample[\"excentricity_map\"], cmap='gray', interpolation='none')\n",
    "            # scatter the centroid\n",
    "            ax[1,0].scatter(sample['centroid_px'][0], sample['centroid_px'][1], c='r', s=10)\n",
    "\n",
    "\n",
    "\n",
    "            fig.tight_layout()\n",
    "\n",
    "            # show differences\n",
    "            \n",
    "        flip_time = \"2003-03-08T00:00:00\"\n",
    "        date = whitelight_to_datetime(basename)\n",
    "        datetime_str = datetime_to_db_string(date).replace(' ', 'T')\n",
    "        # print(datetime_str)\n",
    "        should_flip = (datetime.fromisoformat(datetime_str) - datetime.fromisoformat(flip_time)) < timedelta(0)\n",
    "        sample['should_flip'] = should_flip\n",
    "\n",
    "        if should_flip:\n",
    "            sample['image'] = np.flip(sample['image'],axis=0)\n",
    "            sample['solar_disk'] = np.flip(sample['solar_disk'],axis=0)\n",
    "            sample['mask'] = np.flip(sample['mask'],axis=0)\n",
    "            sample['confidence_map'] = np.flip(sample['confidence_map'],axis=0)\n",
    "            sample['excentricity_map'] = np.flip(sample['excentricity_map'],axis=0)\n",
    "\n",
    "\n",
    "#         st = time.time()\n",
    "        if self.transforms is not None and do_transform:\n",
    "            sample = self.transforms(**sample)\n",
    "#         print_elapsed_time(st, 'transform')\n",
    "\n",
    "        # fig,ax = plt.subplots(1, 5, figsize=(10, 4) )\n",
    "        # ax[0].imshow(sample[\"image\"], cmap='gray', interpolation='none')\n",
    "        # ax[1].imshow(sample[\"mask\"], cmap='gray', interpolation='none')\n",
    "        # ax[2].imshow(sample[\"confidence_map\"], cmap='gray', interpolation='none')\n",
    "        # ax[3].imshow(sample[\"solar_disk\"], cmap='gray', interpolation='none')\n",
    "        # ax[4].imshow(sample[\"excentricity_map\"], cmap='gray', interpolation='none')\n",
    "        \n",
    "\n",
    "        return sample\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'_target_': 'bioblue.transforms.DeepsunScaleWhitelight'}, {'_target_': 'bioblue.transforms.DeepsunScaleExcentricityMap'}, {'_target_': 'bioblue.transforms.DeepsunScaleConfidenceMap'}, {'_target_': 'bioblue.transforms.DeepsunRotateAndCropAroundGroup_Focus_Move', 'standard_height': 350, 'standard_width': 350, 'focus_on_group': '${focus_on_group}', 'random_move': '${random_move}', 'random_move_percent': '${random_move_percent}'}, {'_target_': 'bioblue.transforms.DeepsunMcIntoshScaleAdditionalInfo'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.10 it will stop working\n",
      "Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.10 it will stop working\n"
     ]
    }
   ],
   "source": [
    "# root_dir, partition, dtypes, classes,\n",
    "# first_classes, second_classes, third_classes, \n",
    "# json_file, classification='SuperClass' , transforms=None\n",
    "# root_dir = \"/globalscratch/users/n/s/nsayez/Classification_dataset/2002-2019\"\n",
    "root_dir = \"/globalscratch/users/n/s/nsayez/Classification_dataset/2002-2019_2\"\n",
    "partition = 'train'\n",
    "dtypes = ['image', 'T425-T375-T325_fgbg']\n",
    "# dtypes = ['image', 'feb2023/T425-T375-T325_fgbg']\n",
    "classes = ['A','B','C','SuperGroup','H']\n",
    "first_classes = [ 'A','B','C','SuperGroup','H']\n",
    "second_classes= [ 'x','r','sym','asym']\n",
    "third_classes= [ \"x\",\"o\",\"frag\"]\n",
    "json_file = 'test/dataset_final.json'\n",
    "classification = 'SuperClass'\n",
    "transforms = OmegaConf.load('/home/ucl/elen/nsayez/bio-blueprints/bioblue/conf/exp/Classification_Superclasses4.yaml').dataset.train_dataset.transforms\n",
    "\n",
    "transforms[3].standard_height = 350\n",
    "transforms[3].standard_width = 350\n",
    "\n",
    "print(transforms)\n",
    "\n",
    "dataset =  ClassificationDatasetSuperclasses(root_dir, partition, dtypes, classes, first_classes, second_classes, third_classes, json_file, classification, transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset[259]\n",
    "# pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition_samples = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b70acc4c29d7457898e83f8514b80e3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping sample 288\n",
      "skipping sample 473\n",
      "skipping sample 477\n",
      "skipping sample 707\n",
      "skipping sample 777\n",
      "skipping sample 946\n",
      "skipping sample 1130\n",
      "skipping sample 1166\n",
      "skipping sample 2326\n"
     ]
    }
   ],
   "source": [
    "def get_sample(idx, dataset):\n",
    "    tmp = dataset[idx]\n",
    "    center  = tmp['image'].shape[0]//2 , tmp['image'].shape[1]//2\n",
    "    slice_x = center[0]-( 256 //2 ), center[0]+ (256 //2 )\n",
    "    slice_y = center[1]-( 256 //2 ), center[1]+ (256 //2 )\n",
    "    center_region = tmp['confidence_map'][slice_x[0]:slice_x[1], slice_y[0]:slice_y[1]]\n",
    "    if np.sum(center_region) == 0:\n",
    "        print('skipping sample', idx)\n",
    "        return None\n",
    "    return dataset[idx]\n",
    "\n",
    "num_cpu = 15\n",
    "\n",
    "\n",
    "with concurrent.futures.ProcessPoolExecutor(max_workers=int(num_cpu)) as executor:\n",
    "    for sample in tqdm(executor.map(get_sample, range(len(dataset)) , repeat(deepcopy(dataset)))):\n",
    "    # for sample in tqdm(executor.map(get_sample, range(10) , repeat(deepcopy(dataset)))):\n",
    "        if sample is not None:\n",
    "            if sample['group_name'] not in partition_samples:\n",
    "                partition_samples[sample['group_name']] = sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npy_file = os.path.join(root_dir, 'test', f'all_samples_{partition}.npy')\n",
    "\n",
    "np.save(npy_file, partition_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loaded = np.load(npy_file, allow_pickle=True).item()\n",
    "\n",
    "# for k, v in loaded.items():\n",
    "#     print(k)\n",
    "#     print(v.keys())\n",
    "#     print(v['class1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Join the train, val, test .npy files\n",
    "\n",
    "Je sais pas pourqueoi mais le kernel meurt à chaque fois pendant le dump.... à la place, exécuter le script 'dataset_joiner.py' cans ce dossier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading train\n",
      "Elapsed time 15.252368927001953\n",
      "loading val\n",
      "Elapsed time 3.4969048500061035\n",
      "loading test\n",
      "Elapsed time 3.5309314727783203\n"
     ]
    }
   ],
   "source": [
    "all_samples ={'train':{},'val':{},'test':{}}\n",
    "for p in all_samples.keys():\n",
    "    print('loading', p)\n",
    "    st = time.time()    \n",
    "    filename = os.path.join(root_dir,'test',f'all_samples_{p}.npy' )\n",
    "    tmp = np.load(filename, allow_pickle=True).item()\n",
    "    print('Elapsed time', time.time()-st)\n",
    "    \n",
    "    all_samples[p] = tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dumping\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    }
   ],
   "source": [
    "print(\"Dumping\")\n",
    "st = time.time()    \n",
    "tot_npy_file = os.path.join(root_dir, 'test', f'all_samples.npy')\n",
    "np.save(tot_npy_file, all_samples)\n",
    "print('Elapsed time', time.time()-st)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_elapsed_time(st, msg):\n",
    "        end_time = time.time()\n",
    "        print(f'Elapsed time {msg}: {end_time - st}')\n",
    "        \n",
    "class Deepsun_Focus_Move(DualTransform):\n",
    "    def __init__(self, standard_height=256, standard_width=256,\n",
    "                        focus_on_group=True,\n",
    "                        random_move=False, random_move_percent=0.1,  \n",
    "                        always_apply=False, p=1.0) -> None:\n",
    "\n",
    "        super().__init__(always_apply, p)\n",
    "\n",
    "        self.standard_height = standard_height\n",
    "        self.standard_width = standard_width\n",
    "        \n",
    "        self.focus_on_group = focus_on_group\n",
    "        self.random_move = random_move\n",
    "        self.random_move_percent = random_move_percent\n",
    "\n",
    "    def get_bounding_box_around_group_with_padding(self, mask, offset):\n",
    "        # Get the bounding box around non-zero pixels in mask\n",
    "        x, y = np.nonzero(mask)\n",
    "        # print(x, y)\n",
    "        x1, x2 = (np.min(x), np.max(x)) if len(x) > 0 else (None, None)\n",
    "        y1, y2 = (np.min(y), np.max(y)) if len(y) > 0 else (None, None)\n",
    "\n",
    "        if (x1 is None) or (y1 is None):\n",
    "            return 0, mask.shape[0]-1, 0, mask.shape[1]-1\n",
    "\n",
    "        # Add padding\n",
    "        x1 -= offset\n",
    "        x2 += offset\n",
    "        y1 -= offset\n",
    "        y2 += offset\n",
    "\n",
    "\n",
    "        # Make sure the bounding box is not outside the image\n",
    "        x1 = max(x1, 0)\n",
    "        x2 = min(x2, mask.shape[0])\n",
    "        y1 = max(y1, 0)\n",
    "        y2 = min(y2, mask.shape[1])\n",
    "\n",
    "        return x1, x2, y1, y2\n",
    "\n",
    "    def adapt_bbox_to_image_size(self, bbox, image_size):\n",
    "        bbox_center = ((bbox[0] + bbox[1]) // 2, (bbox[2] + bbox[3]) // 2)\n",
    "        bbox_size = (bbox[1] - bbox[0], bbox[3] - bbox[2])\n",
    "\n",
    "        # if bbox is too small, expand it\n",
    "        minimal_percentage = .4\n",
    "\n",
    "        bbox_size = (max(bbox_size[0], image_size[0] * minimal_percentage),\n",
    "                     max(bbox_size[1], image_size[1] * minimal_percentage))\n",
    "        \n",
    "        return (int(bbox_center[0] - bbox_size[0] // 2), int(bbox_center[0] + bbox_size[0] // 2),\n",
    "                int(bbox_center[1] - bbox_size[1] // 2), int(bbox_center[1] + bbox_size[1] // 2))\n",
    "\n",
    "    def crop_img(self, img, bbox):\n",
    "        # Crop image\n",
    "        x1, x2, y1, y2 = bbox\n",
    "        img = img[x1:x2, y1:y2]\n",
    "        return img\n",
    "        \n",
    "    def padding(self, array, xx, yy):\n",
    "        \"\"\"\n",
    "        :param array: numpy array\n",
    "        :param xx: desired height\n",
    "        :param yy: desirex width\n",
    "        :return: padded array\n",
    "        \"\"\"\n",
    "\n",
    "        h = array.shape[0]\n",
    "        w = array.shape[1]\n",
    "\n",
    "        a = (xx - h) // 2\n",
    "        aa = xx - a - h\n",
    "\n",
    "        b = (yy - w) // 2\n",
    "        bb = yy - b - w\n",
    "\n",
    "        # print(a,aa,b,bb)\n",
    "\n",
    "        a = max(a,0)\n",
    "        b = max(b,0)\n",
    "        aa = max(aa,0)\n",
    "        bb = max(bb,0)\n",
    "\n",
    "\n",
    "        # print('->',a,aa,b,bb)\n",
    "\n",
    "        return np.pad(array, pad_width=((a, aa), (b, bb)), mode='constant')\n",
    "\n",
    "    def crop_and_pad(self, img, bbox, image_size):\n",
    "        # print('crop_and_pad')\n",
    "        # print(f'bbox: {bbox}, image_size: {image_size}, img.shape: {img.shape}')\n",
    "        # Crop image\n",
    "        img = self.crop_img(img, bbox)\n",
    "        # Pad image\n",
    "        img = self.padding(img, image_size[0], image_size[1])\n",
    "        return img\n",
    "\n",
    "    def data_aug_random_move(self, bbox, max_offset):\n",
    "        '''\n",
    "        Randomly move the bounding box\n",
    "        param bbox: bounding box\n",
    "        param max_offset: maximum offset in portion of the bbox size\n",
    "        '''\n",
    "        # Randomly move the bounding box\n",
    "        x1, x2, y1, y2 = bbox\n",
    "        horizontal_offset = (np.random.random(1) * 2*max_offset) - max_offset\n",
    "        vertical_offset = (np.random.random(1) * 2*max_offset) - max_offset\n",
    "        # print(f'horizontal_offset: {horizontal_offset}, vertical_offset: {vertical_offset}')\n",
    "\n",
    "        x1 += int(horizontal_offset * (bbox[1] - bbox[0]))\n",
    "        x2 += int(horizontal_offset * (bbox[1] - bbox[0]))\n",
    "        y1 += int(vertical_offset * (bbox[3] - bbox[2]))\n",
    "        y2 += int(vertical_offset * (bbox[3] - bbox[2]))\n",
    "        \n",
    "        return x1, x2, y1, y2\n",
    "        \n",
    "    def __call__(self, *args, force_apply=False, **kwargs):\n",
    "\n",
    "        img_group_crop = kwargs['image'].copy()\n",
    "        msk_group_crop = kwargs['mask'].copy()\n",
    "        grp_msk_group_crop = kwargs['group_mask'].copy()\n",
    "        disk_group_crop = kwargs['solar_disk'].copy()\n",
    "        excentricity_group_crop = kwargs['excentricity_map'].copy()\n",
    "        confidence_group_crop = kwargs['confidence_map'].copy()\n",
    "        grp_confidence_group_crop = kwargs['group_confidence_map'].copy()\n",
    "        \n",
    "        shape  = img_group_crop.shape\n",
    "        # minX, maxX, minY, maxY =  ((shape[0]//2)-self.standard_height//2, \n",
    "        #                             (shape[0]//2)+self.standard_height//2, \n",
    "        #                             (shape[1]//2)-self.standard_width//2, \n",
    "        #                             (shape[1]//2)+self.standard_width//2)\n",
    "\n",
    "        # bbox format = x1, x2, y1, y2\n",
    "        bbox = self.get_bounding_box_around_group_with_padding((grp_confidence_group_crop>0), 10)\n",
    "\n",
    "        # print(bbox)\n",
    "        # print(bbox[1]-bbox[0], bbox[3]-bbox[2])\n",
    "        \n",
    "        minX, maxX, minY, maxY =  (\n",
    "                                    ((bbox[1]+bbox[0])//2)-(self.standard_height//2), \n",
    "                                    ((bbox[1]+bbox[0])//2)+(self.standard_height//2), \n",
    "                                    ((bbox[3]+bbox[2])//2)-(self.standard_width//2), \n",
    "                                    ((bbox[3]+bbox[2])//2)+(self.standard_width//2)\n",
    "                                    \n",
    "                                    )\n",
    "        # print(\"new_shape minmax \",minX, maxX, minY, maxY)\n",
    "        \n",
    "            \n",
    "        if self.focus_on_group:\n",
    "            # focus on the group\n",
    "            # print('focus on group')\n",
    "            # Modify the bounding box if data augmentation is enabled\n",
    "            if self.random_move:\n",
    "                # print('random_move')\n",
    "                bbox = self.data_aug_random_move(bbox, max_offset=self.random_move_percent)\n",
    "                                    \n",
    "                # Make sure the bounding box is not outside the image\n",
    "                x1, x2, y1, y2 = bbox\n",
    "                x1 = max(x1, 0)\n",
    "                x2 = min(x2, self.standard_width)\n",
    "                y1 = max(y1, 0)\n",
    "                y2 = min(y2, self.standard_height)\n",
    "                bbox = x1, x2, y1, y2\n",
    "            else:\n",
    "                # print('no random_move')\n",
    "                pass\n",
    "                \n",
    "            bbox = self.adapt_bbox_to_image_size( bbox, (self.standard_height, self.standard_width))\n",
    "            # print(bbox)\n",
    "            img_group_crop = self.crop_and_pad(img_group_crop, bbox, (self.standard_height, self.standard_width))\n",
    "            msk_group_crop = self.crop_and_pad(msk_group_crop, bbox, (self.standard_height, self.standard_width))\n",
    "            grp_msk_group_crop = self.crop_and_pad(grp_msk_group_crop, bbox, (self.standard_height, self.standard_width))\n",
    "            disk_group_crop = self.crop_and_pad(disk_group_crop, bbox, (self.standard_height, self.standard_width))\n",
    "            excentricity_group_crop = self.crop_and_pad(excentricity_group_crop, bbox, (self.standard_height, self.standard_width))\n",
    "            confidence_group_crop = self.crop_and_pad(confidence_group_crop, bbox, (self.standard_height, self.standard_width))\n",
    "            grp_confidence_group_crop = self.crop_and_pad(grp_confidence_group_crop, bbox, (self.standard_height, self.standard_width))\n",
    "        else:\n",
    "            # print('NO focus on group')\n",
    "            if self.random_move:\n",
    "                # print('random_move')\n",
    "                frac = np.max([(bbox[1]-bbox[0]) /self.standard_height, (bbox[3]-bbox[2]) /self.standard_width])\n",
    "                frac = np.sqrt(frac)\n",
    "                # print(minX, maxX, minY, maxY , frac, self.random_move_percent)\n",
    "                bbox = self.data_aug_random_move([minX,maxX,minY,maxY], max_offset=self.random_move_percent*frac)\n",
    "                # print('bbox',bbox)\n",
    "                if not ((bbox[1] > shape[0]) or (bbox[3] > shape[1]) or (bbox[0] < 0) or (bbox[2] < 0)):\n",
    "                    bbox = [bbox[0], bbox[0]+self.standard_height, bbox[2], bbox[3]]\n",
    "                    # x1, x2, y1, y2 = bbox\n",
    "                    # x1 = max(x1, 0)\n",
    "                    # x2 = min(x2, self.standard_width)\n",
    "                    # y1 = max(y1, 0)\n",
    "                    # y2 = min(y2, self.standard_height)\n",
    "                    minX,maxX,minY,maxY = bbox\n",
    "                # print(minX, maxX, minY, maxY )\n",
    "            else:\n",
    "                # print('no random_move')\n",
    "                pass\n",
    "                \n",
    "            img_group_crop = img_group_crop[minX:maxX,minY:maxY]\n",
    "            msk_group_crop = msk_group_crop[minX:maxX,minY:maxY]\n",
    "            grp_msk_group_crop = grp_msk_group_crop[minX:maxX,minY:maxY]\n",
    "            disk_group_crop = disk_group_crop[minX:maxX,minY:maxY]\n",
    "            excentricity_group_crop = excentricity_group_crop[minX:maxX,minY:maxY]\n",
    "            confidence_group_crop = confidence_group_crop[minX:maxX,minY:maxY]\n",
    "            grp_confidence_group_crop = grp_confidence_group_crop[minX:maxX,minY:maxY]\n",
    "\n",
    "        if img_group_crop.shape != (self.standard_height, self.standard_width):\n",
    "            img_group_crop = self.padding(img_group_crop, self.standard_height, self.standard_width)\n",
    "            msk_group_crop = self.padding(msk_group_crop, self.standard_height, self.standard_width)\n",
    "            grp_msk_group_crop = self.padding(grp_msk_group_crop, self.standard_height, self.standard_width)\n",
    "            disk_group_crop = self.padding(disk_group_crop, self.standard_height, self.standard_width)\n",
    "            excentricity_group_crop = self.padding(excentricity_group_crop, self.standard_height, self.standard_width)\n",
    "            confidence_group_crop = self.padding(confidence_group_crop, self.standard_height, self.standard_width)\n",
    "            grp_confidence_group_crop = self.padding(grp_confidence_group_crop, self.standard_height, self.standard_width)\n",
    "\n",
    "\n",
    "        assert img_group_crop.shape == (self.standard_height, self.standard_width)\n",
    "        assert msk_group_crop.shape == (self.standard_height, self.standard_width)\n",
    "        assert grp_msk_group_crop.shape == (self.standard_height, self.standard_width)\n",
    "        assert disk_group_crop.shape == (self.standard_height, self.standard_width)\n",
    "        assert excentricity_group_crop.shape == (self.standard_height, self.standard_width)\n",
    "        assert confidence_group_crop.shape == (self.standard_height, self.standard_width)\n",
    "        assert grp_confidence_group_crop.shape == (self.standard_height, self.standard_width)\n",
    "\n",
    "        # self.print_elapsed_time(st, 'focusMove Operations')\n",
    "        # print('after focus_move call',img_group_crop.shape)\n",
    "\n",
    "\n",
    "        kwargs['image'] = img_group_crop.copy()\n",
    "        kwargs['mask'] = msk_group_crop.copy()\n",
    "        kwargs['group_mask'] = grp_msk_group_crop.copy()\n",
    "        kwargs['solar_disk'] = disk_group_crop.copy()\n",
    "        kwargs['excentricity_map'] = excentricity_group_crop.copy()\n",
    "        kwargs['confidence_map'] = confidence_group_crop.copy()\n",
    "        kwargs['group_confidence_map'] = grp_confidence_group_crop.copy()\n",
    "        \n",
    "        return kwargs\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "class ClassificationDatasetSuperclasses_fast(Dataset):\n",
    "\n",
    "    def __init__(\n",
    "        self, root_dir, partition, dtypes, classes,\n",
    "        first_classes, second_classes, third_classes, \n",
    "        dataset_file, classification='SuperClass' , transforms=None) -> None:\n",
    "        super().__init__()\n",
    "        if isinstance(transforms, collections.Mapping):\n",
    "            transforms = partial(call, config=transforms)\n",
    "        elif isinstance(transforms, collections.Sequence):\n",
    "            transforms_init = []\n",
    "            for transform in transforms:\n",
    "                transforms_init.append(instantiate(transform))\n",
    "            transforms = Compose(transforms_init)\n",
    "\n",
    "        self.transforms = transforms\n",
    "        self.root_dir = Path(root_dir)\n",
    "\n",
    "        self.main_dtype = dtypes[0]\n",
    "        self.mask_dtype = dtypes[1]\n",
    "\n",
    "        # self.json_file = self.root_dir / json_file \n",
    "        self.disk_file = self.root_dir / dataset_file.replace('.', '_'+partition+'.')\n",
    "        # print(self.json_file)\n",
    "        self.partition_dict = None\n",
    "\n",
    "        self.c1_mapper = {c: i for i,c in enumerate(first_classes)}\n",
    "        self.c2_mapper = {c: i for i,c in enumerate(second_classes)}\n",
    "        self.c3_mapper = {c: i for i,c in enumerate(third_classes)}\n",
    "        \n",
    "        st = time.time()\n",
    "        # print('Loading npy dataset')\n",
    "        dataset = np.load(self.disk_file, allow_pickle=True).item()\n",
    "        # print_elapsed_time(st, 'Loading npy dataset')\n",
    "        \n",
    "        # print(dataset.keys())\n",
    "    \n",
    "        # self.partition_dict = dataset[partition]\n",
    "        self.partition_dict = dataset\n",
    "\n",
    "        self.classification = classification\n",
    "\n",
    "        self.FirstClass_mapper = {c: i for i,c in enumerate(first_classes)}\n",
    "        self.SecondClass_mapper = {c: i for i,c in enumerate(second_classes)}\n",
    "        self.ThirdClass_mapper = {c: i for i,c in enumerate(third_classes)}\n",
    "\n",
    "        # print(self.files)\n",
    "\n",
    "        # print(list(self.partition_dict.values())[0])\n",
    "\n",
    "        self.groups = {}\n",
    "        for k,v in self.partition_dict.items():\n",
    "\n",
    "            if v[\"class1\"] in classes:\n",
    "                    self.groups[k] = v\n",
    "            else:\n",
    "                # print(v[self.classification][\"1\"])\n",
    "                pass\n",
    "\n",
    "        self.dataset_length = len(list(self.groups.keys()))\n",
    "\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        # raise NotImplementedError\n",
    "        # print(self.dataset_length)\n",
    "        return self.dataset_length\n",
    "        # return 10\n",
    "    \n",
    "    def __getitem__(self, index: int, do_transform=True):\n",
    "\n",
    "        idx = list(self.groups.keys())[index]\n",
    "        \n",
    "        sample = deepcopy(self.groups[idx])\n",
    "\n",
    "        # ['image']\n",
    "        # ['mask']\n",
    "        # ['group_mask']\n",
    "        # ['solar_disk']\n",
    "        # ['excentricity_map']\n",
    "        # ['confidence_map']\n",
    "        # ['group_confidence_map']\n",
    "\n",
    "        # print(sample['group_name'])\n",
    "\n",
    "        sample['class1'] = np.array([self.FirstClass_mapper[sample['class1']]])\n",
    "        sample['class2'] = np.array([self.SecondClass_mapper[sample['class2']]])\n",
    "        sample['class3'] = np.array([self.ThirdClass_mapper[sample['class3']]])\n",
    "\n",
    "       \n",
    "        fig,ax = plt.subplots(2, 5, figsize=(10, 4) )\n",
    "        ax[0,0].set_title(sample['group_name'])\n",
    "        ax[0,0].imshow(sample[\"image\"], cmap='gray', interpolation='none')\n",
    "        ax[0,1].imshow(sample[\"mask\"], cmap='gray', interpolation='none')\n",
    "        ax[0,2].imshow(sample[\"confidence_map\"], cmap='gray', interpolation='none')\n",
    "        ax[0,3].imshow(sample[\"solar_disk\"], cmap='gray', interpolation='none')\n",
    "        ax[0,4].imshow(sample[\"excentricity_map\"], cmap='gray', interpolation='none')\n",
    "\n",
    "\n",
    "        tmp = np.argwhere(sample['confidence_map'] > 0)\n",
    "        ax[0,2].scatter(tmp[:,1], tmp[:,0], s=1, c='r', alpha=0.5)\n",
    "\n",
    "        sample['image'][sample['excentricity_map'] < 0] = 0\n",
    "        sample['mask'][sample['excentricity_map'] < 0] = 0\n",
    "        sample['group_mask'][sample['excentricity_map'] < 0] = 0\n",
    "        sample['solar_disk'][sample['excentricity_map'] < 0] = 0\n",
    "        sample['confidence_map'][sample['excentricity_map'] < 0] = 0\n",
    "        sample['group_confidence_map'][sample['excentricity_map'] < 0] = 0\n",
    "        sample['group_confidence_map'][sample['excentricity_map'] > 0.95] = 0\n",
    "        \n",
    "        sample['excentricity_map'][sample['excentricity_map'] < 0] = 0\n",
    "        \n",
    "        # st = time.time()\n",
    "        if self.transforms is not None and do_transform:\n",
    "            sample = self.transforms(**sample)\n",
    "        # print_elapsed_time(st, 'transform')\n",
    "\n",
    "        \n",
    "\n",
    "        ax[1,0].imshow(sample[\"image\"], cmap='gray', interpolation='none')\n",
    "        ax[1,1].imshow(sample[\"mask\"], cmap='gray', interpolation='none')\n",
    "        ax[1,2].imshow(sample[\"confidence_map\"], cmap='gray', interpolation='none')\n",
    "        ax[1,3].imshow(sample[\"solar_disk\"], cmap='gray', interpolation='none')\n",
    "        ax[1,4].imshow(sample[\"excentricity_map\"], cmap='gray', interpolation='none')\n",
    "        \n",
    "        fig.tight_layout()\n",
    "\n",
    "        tmp = np.argwhere(sample['confidence_map'] > 0)\n",
    "        ax[1,2].scatter(tmp[:,1], tmp[:,0], s=1, c='r', alpha=0.5)\n",
    "\n",
    "        return sample\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "root_dir = \"/globalscratch/users/n/s/nsayez/Classification_dataset/2002-2019_2\"\n",
    "partition='train'\n",
    "dtypes = ['image', 'T425-T375-T325_fgbg']\n",
    "\n",
    "classes = ['A','B','C','SuperGroup','H']\n",
    "first_classes = [ 'A','B','C','SuperGroup','H']\n",
    "second_classes= [ 'x','r','sym','asym']\n",
    "third_classes= [ \"x\",\"o\",\"frag\"]\n",
    "classification='SuperClasses'\n",
    "\n",
    "transforms2 = [{'_target_': Deepsun_Focus_Move, 'standard_height': 256, 'standard_width': 256,\n",
    "                 'focus_on_group': False, 'random_move': False, 'random_move_percent': .2}]\n",
    "dataset_file = os.path.join(root_dir, 'test', f'all_samples.npy')\n",
    "d2 = ClassificationDatasetSuperclasses_fast(root_dir, partition, dtypes, classes, first_classes, second_classes, third_classes, \n",
    "                                                dataset_file, classification, transforms2)\n",
    "# for i in range(5):\n",
    "#     d2[i]\n",
    "# pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a19e35ebf1c740169bc2534f4649a2f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "d2[882]\n",
    "# d2[2227]\n",
    "# d2[2649]\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cc84b7fe65645f8ba3e561d0fd93c3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2704 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 256)\n",
      "882\n",
      "(256, 256)\n",
      "2227\n",
      "(256, 256)\n",
      "2649\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(len(d2))):\n",
    "    try:\n",
    "        tmp = d2[i]\n",
    "    except:\n",
    "        print(tmp['image'].shape)\n",
    "        print(i)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VAL avait des erruers aux indices 94 / 196\n",
    "# d2[94]\n",
    "# d2[96]\n",
    "# TRAIN avait des erreurs aux indices 288 / 473 / 477 / 707 / 777 / 946 / 1130 / 1166 / 2326\n",
    "# d2[288]\n",
    "# d2[473]\n",
    "# d2[477]\n",
    "# d2[707]\n",
    "# d2[777]\n",
    "# d2[946]\n",
    "# d2[1130]\n",
    "# d2[1166]\n",
    "# d2[2326]\n",
    "\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3343416694542c5bc2f9dd285582daf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7241b3139068455d9c6a31c3a97ee985",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b2b3f0f312f414584f112d1e68c681c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c777f1a188e46c29eda4e38bb1af00b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "668fbe1a81c94e3e93c91be05117c41e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d848ca9c8f54c0594ecf89a82c9779b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fee0990a960f4b8cbd2948c616fab47d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f59e8daabbf4aa4a68ab92bf492d2ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bae072fcdde453985344bb0104babdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5a9dfa143e54336959bc48e4f770ea5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    d2[i]\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time transform: 0.4652729034423828\n",
      "Elapsed time transform: 0.43789052963256836\n",
      "Elapsed time transform: 0.4377424716949463\n",
      "Elapsed time transform: 0.3622429370880127\n",
      "Elapsed time transform: 0.35805320739746094\n",
      "Elapsed time transform: 0.43010497093200684\n",
      "Elapsed time transform: 0.3954179286956787\n",
      "Elapsed time transform: 0.36830782890319824\n",
      "Elapsed time transform: 0.3276553153991699\n",
      "Elapsed time transform: 0.311431884765625\n"
     ]
    }
   ],
   "source": [
    "\n",
    "few_samples = [dataset[i] for i in range(2000,2010)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create a pickle file containing the samples in few_samples\n",
    "# with open('few_samples.pkl', 'wb') as f:\n",
    "#     pickle.dump(few_samples, f)\n",
    "\n",
    "npy_file = 'few_samples.npy'\n",
    "\n",
    "my_dict ={'train': {s[\"group_name\"]: s for s in few_samples}}\n",
    "np.save(npy_file, my_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mv few_samples.npy $root_dir\n",
    "# np.load(npy_file, allow_pickle=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "1f3e85867ab3feeb73691fcc67a502ec8f0fc265745d17c9ab3a5329e7f22e4e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
