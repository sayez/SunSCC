{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import glob\n",
    "import json\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "import sunscc.utils.clustering.clustering_utilities as c_utils\n",
    "\n",
    "import importlib\n",
    "importlib.reload(c_utils)\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "%matplotlib ipympl\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NpEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        if isinstance(obj, np.int64):\n",
    "            return int(obj)\n",
    "        if isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        if isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        if isinstance(obj, np.bool_):\n",
    "            return bool(obj)\n",
    "        return super(NpEncoder, self).default(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = '../../datasets/classification/2002-2019_2'\n",
    "\n",
    "db_dict_filepath = root_dir+'/wl_list2dbGroups_Classification.json'\n",
    "\n",
    "param_optim_p1_folder = root_dir + '/param_optimization'\n",
    "param_optim_p2_folder = root_dir + '/param_optimP2'\n",
    "\n",
    "db_dict = { }\n",
    "with open(db_dict_filepath, 'r') as f:\n",
    "    db_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Number of overlaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cur_dict_2002-19_dist0.1_Lon0.35_lat0.08_iter20.json\n"
     ]
    }
   ],
   "source": [
    "# fn = f'cur_dict_2002-19_dist0.1_Lon0.1_lat0.1_iter20.json'  \n",
    "fn = f'cur_dict_2002-19_dist0.1_Lon0.35_lat0.08_iter20.json'  \n",
    "print(fn)\n",
    "\n",
    "#     raise\n",
    "\n",
    "cur_huge_dict_filename = os.path.join(param_optim_p1_folder,fn)\n",
    "with open(cur_huge_dict_filename,'r') as f:\n",
    "    cur_huge_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5225cbd6bd2a4ab399ff64d5db8eb8d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2969 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6528 732\n",
      "5796\n",
      "0.11213235294117647\n"
     ]
    }
   ],
   "source": [
    "total_bbox = 0\n",
    "total_overlap = 0\n",
    "\n",
    "# cur_huge_dict = grid_image_out_dict[0]\n",
    "for basename in tqdm(list(cur_huge_dict.keys())[:]):\n",
    "    cur_image_dict = cur_huge_dict[basename]\n",
    "    \n",
    "    angle = cur_image_dict[\"SOLAR_P0\"]\n",
    "    deltashapeX = cur_image_dict[\"deltashapeX\"]\n",
    "    deltashapeY = cur_image_dict[\"deltashapeY\"]\n",
    "    \n",
    "    drawing_radius_px = db_dict[basename][\"dr_radius_px\"]\n",
    "    \n",
    "    group_list = cur_image_dict['db']\n",
    "    \n",
    "    ms_dict = cur_image_dict['meanshift']\n",
    "    \n",
    "    centroids = np.array(ms_dict[\"centroids\"])\n",
    "    centroids_px = np.array(ms_dict[\"centroids_px\"])\n",
    "    \n",
    "    db_classes = [{\"Zurich\":item['Zurich'], \"McIntosh\":item['McIntosh'] } for item in group_list]\n",
    "    db_bboxes = [np.array(item['bbox_wl']) for item in group_list]\n",
    "    db_centers_px = np.array([[(b[2]+b[0])/2,(b[3]+b[1])/2] for b in db_bboxes])\n",
    "        \n",
    "    # check that current bbox is does not overlap any\n",
    "    isolated_bboxes_bool = np.array(c_utils.get_intersecting_db_bboxes(db_bboxes)) == 0\n",
    "    isolated_bboxes_indices = np.where(isolated_bboxes_bool == True)[0]\n",
    "\n",
    "    total_bbox += len(db_bboxes)\n",
    "    total_overlap += len(db_bboxes) - len(isolated_bboxes_indices)\n",
    "\n",
    "print(total_bbox, total_overlap)\n",
    "print(total_bbox - total_overlap)\n",
    "print(total_overlap / total_bbox)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PHASE 1: Construire le dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = f'image_outdict_dist0.1_Lon0.35_lat0.08_iter20.json'  \n",
    "\n",
    "cur_image_out_dict_filename = os.path.join(param_optim_p2_folder,fn)\n",
    "with open(cur_image_out_dict_filename,'r') as f:\n",
    "    image_out_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41e89bd23a8e431abce99352d02c2d72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2969 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'A': 605, 'B': 714, 'C': 1039, 'D': 1012, 'E': 240, 'F': 41, 'G': 148, 'H': 383, 'I': 0, 'J': 1040, 'X': 21}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1) analyse the distributions\n",
    "\n",
    "classes = ['A','B','C','D','E','F','G','H','I','J','X']\n",
    "\n",
    "distribs = {c: 0 for c in classes}\n",
    "group_types = {}\n",
    "group_types2 = {c:{} for c in classes}\n",
    "\n",
    "for bn, img_dict in tqdm(image_out_dict.items()):\n",
    "    if 'groups' not in img_dict:\n",
    "        continue\n",
    "        \n",
    "    groups = img_dict['groups']\n",
    "    \n",
    "    for i, g in enumerate(groups):\n",
    "        cur_c = g[\"Zurich\"]\n",
    "        distribs[cur_c] +=1\n",
    "        \n",
    "        new_group_infos= {\n",
    "            'angle': img_dict['angle'],\n",
    "            'deltashapeX': img_dict['deltashapeX'],\n",
    "            'deltashapeY':img_dict['deltashapeY'],\n",
    "            'centroid_px': g['centroid_px'],\n",
    "            'centroid_Lat': g['centroid_Lat'],\n",
    "            'centroid_Lon': g['centroid_Lon'],\n",
    "            'members': g['members'],\n",
    "            'members_mean_px': g['members_mean_px'],\n",
    "            'angular_excentricity_rad': g['angular_excentricity_rad'],\n",
    "            'angular_excentricity_deg': g['angular_excentricity_deg'],\n",
    "            'Zurich': g['Zurich'],\n",
    "            'McIntosh': g['McIntosh'],   \n",
    "        }\n",
    "        \n",
    "        new_goup_id = bn + '_' + str(i)\n",
    "        group_types[new_goup_id] = new_group_infos\n",
    "        group_types2[cur_c][new_goup_id] = new_group_infos\n",
    "        \n",
    "    \n",
    "print(distribs)\n",
    "print()\n",
    "# print(group_types2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_based_dataset = deepcopy(group_types)\n",
    "group_based_dataset2 = deepcopy(group_types2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a93014c40854355bd52aab54b9a503c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5243 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UPH20040722080136_1\n",
      "{'angle': 6.950210373029591, 'deltashapeX': 226, 'deltashapeY': 226, 'centroid_px': [1763.984688376337, 789.7939222567339], 'centroid_Lat': -0.19138280535545324, 'centroid_Lon': 0.8858795170464564, 'members': [[789.1715265866209, 1766.2315608919382], [805.4, 1705.56], [804.0, 1700.5]], 'members_mean_px': [799.5238421955404, 1724.0971869639795], 'angular_excentricity_rad': 0.8332981619101842, 'angular_excentricity_deg': 47.74446775346269, 'Zurich': 'G', 'McIntosh': '   '}\n",
      "error"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "UPH20111104083629_2\n",
      "{'angle': 23.9656539855509, 'deltashapeX': 698, 'deltashapeY': 698, 'centroid_px': [278.653666143603, 1271.799855625567], 'centroid_Lat': 0.3251877192454918, 'centroid_Lon': 1.8424379034316671, 'members': [[1259.2236979166667, 292.2916666666667], [1242.1, 242.68333333333334], [1244.75, 213.5], [1278.3066933066932, 201.21578421578423], [1305.9180487804879, 240.3180487804878], [1297.0, 280.1666666666667], [1304.952380952381, 278.3333333333333], [1309.0969387755101, 294.5408163265306]], 'members_mean_px': [1280.1684699664675, 255.38120616535033], 'angular_excentricity_rad': 1.1306380698466285, 'angular_excentricity_deg': 64.7807895590294, 'Zurich': 'E', 'McIntosh': '   '}\n",
      "error\n",
      "UPH20111104083629_3\n",
      "{'angle': 23.9656539855509, 'deltashapeX': 698, 'deltashapeY': 698, 'centroid_px': [452.9394902751303, 708.7341490132685], 'centroid_Lat': -0.24894683817332025, 'centroid_Lon': 2.1239463299682932, 'members': [[682.0751028806584, 487.61831275720164], [691.4444444444445, 392.6666666666667], [726.2333333333333, 404.7], [750.4703891708967, 403.85109983079525]], 'members_mean_px': [712.5558174573332, 422.20901981366586], 'angular_excentricity_rad': 0.5084970712757209, 'angular_excentricity_deg': 29.13473607886181, 'Zurich': 'D', 'McIntosh': '   '}\n",
      "error\n"
     ]
    }
   ],
   "source": [
    "First2superFirst = {\"A\":\"A\",\n",
    "                    \"B\":\"B\",\n",
    "                    \"C\":\"C\",\n",
    "                    \"D\":\"SuperGroup\",\n",
    "                    \"E\":\"SuperGroup\",\n",
    "                    \"F\":\"SuperGroup\",\n",
    "                    \"H\":\"H\",\n",
    "                    \"X\":\"X\"\n",
    "                   }\n",
    "Second2superSecond = {\"x\":\"x\",\n",
    "                      \"r\":\"r\",\n",
    "                      \"s\": \"sym\",\n",
    "                      \"h\": \"sym\",\n",
    "                      \"a\": \"asym\",\n",
    "                      \"k\": \"asym\",\n",
    "                     }\n",
    "Third2superThird = {\"x\": \"x\",\n",
    "                    \"o\": \"o\",\n",
    "                    \"i\": \"frag\",\n",
    "                    \"c\": \"frag\",\n",
    "                   }\n",
    "\n",
    "def add_superclasses(group_dict):\n",
    "    cpy = deepcopy(group_dict)    \n",
    "    # print(cpy)\n",
    "\n",
    "    cpy[\"SuperClass\"] = {\n",
    "        \"1\": First2superFirst[group_dict[\"McIntosh\"][0]],\n",
    "        \"2\": Second2superSecond[group_dict[\"McIntosh\"][1]],\n",
    "        \"3\": Third2superThird[group_dict[\"McIntosh\"][2]],\n",
    "    }\n",
    "    \n",
    "    return cpy\n",
    "    \n",
    "    \n",
    "\n",
    "grp_to_remove = []\n",
    "group_based_dataset_superclasses = {}\n",
    "for g in tqdm(group_based_dataset):\n",
    "    try : \n",
    "        group = group_based_dataset[g]\n",
    "        # print(group)\n",
    "        group = add_superclasses(group)\n",
    "#         print(group)\n",
    "        group_based_dataset_superclasses[g] = group\n",
    "        \n",
    "    except KeyError:\n",
    "        print(g)\n",
    "        print(group_based_dataset[g])\n",
    "        if group_based_dataset[g][\"McIntosh\"] == '   ':\n",
    "            print( \"error\")\n",
    "            grp_to_remove.append((g,group_based_dataset[g]['Zurich']))\n",
    "\n",
    "for k,k_type in grp_to_remove:\n",
    "    group_based_dataset.pop(k)\n",
    "    group_based_dataset2[k_type].pop(k)\n",
    "\n",
    "# group_based_dataset_superclasses\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split per types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef18ceb3682d48ae8497a32c7f78c70b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5240 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "distribs2 = {c:0 for c in classes}\n",
    "group_types2 = {c:{} for c in classes}\n",
    "\n",
    "for grp_id, grp_dict in tqdm(group_based_dataset_superclasses.items()):\n",
    "    cur_c = grp_dict[\"Zurich\"]\n",
    "    group_types2[cur_c][grp_id] = grp_dict\n",
    "        \n",
    "# group_types2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2:  Split groups among train, val, test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "def splitPerc(l, perc):\n",
    "    # Turn percentages into values between 0 and 1\n",
    "    splits = np.cumsum(perc)/100.\n",
    "\n",
    "    if splits[-1] != 1:\n",
    "        raise ValueError(\"percents don't add up to 100\")\n",
    "\n",
    "    # Split doesn't need last percent, it will just take what is left\n",
    "    splits = splits[:-1]\n",
    "\n",
    "    # Turn values into indices\n",
    "    splits *= len(l)\n",
    "\n",
    "    # Turn double indices into integers.\n",
    "    # CAUTION: numpy rounds to closest EVEN number when a number is halfway\n",
    "    # between two integers. So 0.5 will become 0 and 1.5 will become 2!\n",
    "    # If you want to round up in all those cases, do\n",
    "    # splits += 0.5 instead of round() before casting to int\n",
    "    splits = splits.round().astype(int)\n",
    "\n",
    "    return np.split(l, splits)\n",
    "\n",
    "splits = ['train', 'val', 'test']\n",
    "\n",
    "splits_percentages = [70, 15, 15]\n",
    "\n",
    "assert np.array(splits_percentages).sum() == 100\n",
    "\n",
    "group_based_dataset_superclasses_splits = {sp:{} for sp in splits}\n",
    "\n",
    "for t, type_dict in group_types2.items():\n",
    "    list_type_groups = list(type_dict.keys()) \n",
    "    # shuffle\n",
    "    random.shuffle(list_type_groups)\n",
    "    \n",
    "    indices = np.array(range(len(list_type_groups)))\n",
    "    \n",
    "    s = splitPerc(indices, splits_percentages)\n",
    "\n",
    "    # take percentage and fill group_based_dataset\n",
    "    for i, sp in enumerate(splits):\n",
    "        split_indices = s[i]\n",
    "        split_groups = [list_type_groups[j] for j in split_indices]\n",
    "        \n",
    "#         print(split_groups)\n",
    "        for g in  split_groups:\n",
    "            group_based_dataset_superclasses_splits[sp][g] = type_dict[g] \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_subdir = 'sunscc'\n",
    "final_json = \"dataset_nosplits\"\n",
    "\n",
    "if not os.path.exists(f'{root_dir}/{out_subdir}'):\n",
    "    os.makedirs(f'{root_dir}/{out_subdir}')\n",
    "\n",
    "with open(f'{root_dir}/{out_subdir}/{final_json}.json', 'w') as f:\n",
    "    json.dump(group_based_dataset_superclasses, f, cls=NpEncoder)\n",
    "final_json_split = \"dataset_final\"\n",
    "with open(f'{root_dir}/{out_subdir}/{final_json_split}.json', 'w') as f:\n",
    "    json.dump(group_based_dataset_superclasses_splits, f, cls=NpEncoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for split in splits:\n",
    "    with open(f'{root_dir}/{out_subdir}/{split}.json', 'w') as f:\n",
    "        json.dump(group_based_dataset_superclasses_splits[split], f, cls=NpEncoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PHASE 2: Splitting Overlaps from Isolated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_path = '../../datasets/classification/2002-2019_2/test/dataset_nosplits.json'\n",
    "new_path = '../../datasets/classification/2002-2019_2/rebuttal/dataset_nosplits.json'\n",
    "\n",
    "#open the two files\n",
    "with open(old_path, 'r') as f:\n",
    "    old_dict = json.load(f)\n",
    "with open(new_path, 'r') as f:\n",
    "    new_dict = json.load(f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_new_entries:  1371\n"
     ]
    }
   ],
   "source": [
    "# find the entries that are in the new dict but not in the old one\n",
    "# Focusing on the keys is not sufficient, we should look at:\n",
    "# - the part before the firs underscore in the key\n",
    "# - the \"centroid_lon\" and \"centroid_lat\" fields in the value\n",
    "\n",
    "\n",
    "new_entries = {}\n",
    "num_new_entries = 0\n",
    "\n",
    "for cur_k,cur_v in new_dict.items():\n",
    "    # get the first part of the key\n",
    "    first_part = cur_k.split('_')[0]\n",
    "\n",
    "    centroid_lon = cur_v['centroid_Lon']\n",
    "    centroid_lat = cur_v['centroid_Lat']\n",
    "\n",
    "    # get all entries in the old dict that have the same first part\n",
    "    old_dict_keys = [k for k in old_dict.keys() if k.split('_')[0] == first_part]\n",
    "    \n",
    "    # check if any of these entries have the same centroid\n",
    "    for old_k in old_dict_keys:\n",
    "        old_centroid_lon = old_dict[old_k]['centroid_Lon']\n",
    "        old_centroid_lat = old_dict[old_k]['centroid_Lat']\n",
    "        \n",
    "        if centroid_lon == old_centroid_lon and centroid_lat == old_centroid_lat:\n",
    "            # print(\"Found a match!\")\n",
    "            # print(cur_k)\n",
    "            # print(old_k)\n",
    "            # print(cur_v)\n",
    "            # print(old_dict[old_k])\n",
    "            # print()\n",
    "            break\n",
    "    else:      \n",
    "        new_entries[cur_k] = cur_v\n",
    "\n",
    "        num_new_entries += 1\n",
    "        # print(\"Found a new entry!\")\n",
    "        # print(cur_k)\n",
    "        # print(cur_v)\n",
    "        # print()\n",
    "\n",
    "print(\"num_new_entries: \",num_new_entries)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dump the new dict\n",
    "new_overlap_dir = '../../datasets/classification/2002-2019_2/sunscc_overlap_only'\n",
    "out_file = new_overlap_dir + '/dataset_nosplits.json'\n",
    "\n",
    "if not os.path.exists(new_overlap_dir):\n",
    "    os.makedirs(new_overlap_dir)\n",
    "\n",
    "with open(out_file, 'w') as f:\n",
    "    json.dump(new_entries, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Separate train val and test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_path = '../../datasets/classification/2002-2019_2/test/dataset_nosplits.json'\n",
    "new_path = '../../datasets/classification/2002-2019_2/sunscc/dataset_final.json'\n",
    "\n",
    "split = 'test'\n",
    "\n",
    "#open the two files\n",
    "with open(old_path, 'r') as f:\n",
    "    old_dict = json.load(f)\n",
    "with open(new_path, 'r') as f:\n",
    "    new_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split:  train  len:  3669\n",
      "split:  val  len:  785\n",
      "split:  test  len:  786\n",
      "total:  5240\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "for split in ['train','val','test']:\n",
    "    print(\"split: \",split, \" len: \",len(new_dict[split]))\n",
    "    total += len(new_dict[split])\n",
    "print(\"total: \",total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train -> num_new_entries:  967\n",
      "val -> num_new_entries:  192\n",
      "test -> num_new_entries:  212\n"
     ]
    }
   ],
   "source": [
    "output = {}\n",
    "\n",
    "for split in ['train','val','test']:\n",
    "    new_dict_split  = new_dict[split]\n",
    "\n",
    "    # find the entries that are in the new dict but not in the old one\n",
    "    # Focusing on the keys is not sufficient, we should look at:\n",
    "    # - the part before the firs underscore in the key\n",
    "    # - the \"centroid_lon\" and \"centroid_lat\" fields in the value\n",
    "\n",
    "\n",
    "    new_entries = {}\n",
    "    num_new_entries = 0\n",
    "\n",
    "    for cur_k,cur_v in new_dict_split.items():\n",
    "        # get the first part of the key\n",
    "        first_part = cur_k.split('_')[0]\n",
    "\n",
    "        centroid_lon = cur_v['centroid_Lon']\n",
    "        centroid_lat = cur_v['centroid_Lat']\n",
    "\n",
    "        # get all entries in the old dict that have the same first part\n",
    "        old_dict_keys = [k for k in old_dict.keys() if k.split('_')[0] == first_part]\n",
    "        \n",
    "        # check if any of these entries have the same centroid\n",
    "        for old_k in old_dict_keys:\n",
    "            old_centroid_lon = old_dict[old_k]['centroid_Lon']\n",
    "            old_centroid_lat = old_dict[old_k]['centroid_Lat']\n",
    "            \n",
    "            if centroid_lon == old_centroid_lon and centroid_lat == old_centroid_lat:\n",
    "                # print(\"Found a match!\")\n",
    "                # print(cur_k)\n",
    "                # print(old_k)\n",
    "                # print(cur_v)\n",
    "                # print(old_dict[old_k])\n",
    "                # print()\n",
    "                break\n",
    "        else:      \n",
    "            new_entries[cur_k] = cur_v\n",
    "\n",
    "            num_new_entries += 1\n",
    "            # print(\"Found a new entry!\")\n",
    "            # print(cur_k)\n",
    "            # print(cur_v)\n",
    "            # print()\n",
    "\n",
    "    print(f\"{split} -> num_new_entries: \",num_new_entries)\n",
    "\n",
    "    #dump the new dict\n",
    "    out_file = f'../../datasets/classification/2002-2019_2/sunscc_overlap_only/overlaps_{split}.json'\n",
    "    with open(out_file, 'w') as f:\n",
    "        json.dump(new_entries, f)\n",
    "    \n",
    "    output[split] = new_entries\n",
    "\n",
    "#dump the new dict\n",
    "out_file = f'../../datasets/classification/2002-2019_2/sunscc_overlap_only/dataset_overlapsOnly.json'\n",
    "with open(out_file, 'w') as f:\n",
    "    json.dump(output, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find Difference between overlapOnly and Total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_path = '../../datasets/classification/2002-2019_2/test/dataset_nosplits.json'\n",
    "new_path = '../../datasets/classification/2002-2019_2/sunscc/dataset_final.json'\n",
    "with open(old_path, 'r') as f:\n",
    "    old_dict = json.load(f)\n",
    "with open(new_path, 'r') as f:\n",
    "    new_dict = json.load(f)\n",
    "\n",
    "nooverlap_file = '../../datasets/classification/2002-2019_2/test/dataset_final.json'\n",
    "overlapsonly_file = f'../../datasets/classification/2002-2019_2/sunscc_overlap_only/dataset_overlapsOnly.json'\n",
    "\n",
    "\n",
    "with open(nooverlap_file, 'r') as f:\n",
    "    nooverlap_dict = json.load(f)\n",
    "\n",
    "with open(overlapsonly_file, 'r') as f:\n",
    "    overlapsonly_dict = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in NoOverlap train: 2722\n",
      "Number of samples in OverlapsOnly train: 967\n",
      "Number of samples in Total train: 3689\n",
      "---------\n",
      "Number of samples in Fusion train: 3689\n",
      "---------\n",
      "---------\n",
      "Number of samples in NoOverlap val: 583\n",
      "Number of samples in OverlapsOnly val: 192\n",
      "Number of samples in Total val: 775\n",
      "---------\n",
      "Number of samples in Fusion val: 775\n",
      "---------\n",
      "---------\n",
      "Number of samples in NoOverlap test: 583\n",
      "Number of samples in OverlapsOnly test: 212\n",
      "Number of samples in Total test: 795\n",
      "---------\n",
      "Number of samples in Fusion test: 795\n",
      "---------\n",
      "---------\n"
     ]
    }
   ],
   "source": [
    "final_dict = {'train': [], 'val': [], 'test': []}\n",
    "\n",
    "for split in ['train', 'val', 'test']:\n",
    "    num_NoOverlap = len(nooverlap_dict[split])\n",
    "    num_OverlapsOnly = len(overlapsonly_dict[split])\n",
    "\n",
    "\n",
    "    print('Number of samples in NoOverlap %s: %d' % (split, num_NoOverlap))\n",
    "    print('Number of samples in OverlapsOnly %s: %d' % (split, num_OverlapsOnly))\n",
    "    print('Number of samples in Total %s: %d' % (split, num_NoOverlap + num_OverlapsOnly))\n",
    "    print(\"---------\")\n",
    "\n",
    "    fusion = nooverlap_dict[split].copy()\n",
    "\n",
    "    for key, value in overlapsonly_dict[split].items():\n",
    "        if key in fusion:\n",
    "            fusion[key + '_'] = value\n",
    "        else:\n",
    "            fusion[key] = value\n",
    "    # fusion.update(overlapsonly_dict[split])\n",
    "\n",
    "    print('Number of samples in Fusion %s: %d' % (split, len(fusion)))\n",
    "    print(\"---------\")\n",
    "    print(\"---------\")\n",
    "\n",
    "    # fuse the two lists\n",
    "    final_dict[split] = fusion\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "out_dir = f'../../datasets/classification/2002-2019_2/sunscc_all_revised'\n",
    "\n",
    "#mkdir if it does not exist\n",
    "if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir)\n",
    "\n",
    "# dump final_dict\n",
    "with open(f'{out_dir}/dataset_revised.json', 'w') as f:\n",
    "    json.dump(final_dict, f)\n",
    "\n",
    "for s in final_dict:\n",
    "    # dump final_dict\n",
    "    with open(f'{out_dir}/dataset_revised_{s}.json', 'w') as f:\n",
    "        json.dump(final_dict[s], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rebuttal_out_dir = f'../../datasets/classification/2002-2019_2/rebuttal_all_revised'\n",
    "# # dump final_dict\n",
    "# with open(f'{rebuttal_out_dir}/dataset_revised.json', 'r') as f:\n",
    "#     tmp_load = json.load(f)\n",
    "\n",
    "#     for s in tmp_load:\n",
    "#         # dump final_dict\n",
    "#         with open(f'{rebuttal_out_dir}/dataset_revised_{s}.json', 'w') as f:\n",
    "#             json.dump(tmp_load[s], f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the end of Notebook Step 0,  now go to Notebook Step 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sunscc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
