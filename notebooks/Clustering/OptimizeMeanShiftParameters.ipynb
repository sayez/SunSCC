{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install panel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import glob\n",
    "import json\n",
    "\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "from astropy.io import fits\n",
    "from astropy.wcs import WCS\n",
    "from astropy.wcs.utils import skycoord_to_pixel\n",
    "from astropy.coordinates import SkyCoord\n",
    "import astropy.units as u\n",
    "from astropy.coordinates import Angle\n",
    "\n",
    "from sunpy.coordinates import frames\n",
    "import sunpy.map as sunmap\n",
    "\n",
    "import skimage.io as io\n",
    "\n",
    "\n",
    "from copy import deepcopy\n",
    "import pandas as pd\n",
    "\n",
    "from ...utilities import  MeanShift as MS\n",
    "from ...utilities import clustering_utilities as c_utils\n",
    "from ...utilities import tracking_utilities as t_utils\n",
    "\n",
    "import importlib\n",
    "importlib.reload(c_utils)\n",
    "\n",
    "import ipywidgets as widgets\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "import concurrent.futures\n",
    "from itertools import repeat\n",
    "import multiprocessing\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "%matplotlib ipympl\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "print(matplotlib.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wl_dir = \"../datasets/Segmentation_dataset/ManualAnnotation/image\"\n",
    "wl_list = sorted(glob.glob(os.path.join(wl_dir, '**/*.FTS'),recursive=True))\n",
    "wl_basenames = [ os.path.basename(wl) for wl in wl_list ]\n",
    "\n",
    "masks_dir = '../datasets/Segmentation_dataset/ManualAnnotation/GroundTruth'\n",
    "\n",
    "sqlite_db_path = \"../datasets/Classification_dataset/drawings_sqlite.sqlite\"\n",
    "\n",
    "database = sqlite_db_path\n",
    "print(len(wl_list), )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rotten_list = [ ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = \"../datasets/Classification_dataset\"\n",
    "tmp = root_dir+'/ManualAnnotation/wl_list2dbGroups_Classification.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "huge_db_dict = { }\n",
    "with open(tmp, 'r') as f:\n",
    "    huge_db_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(huge_db_dict.keys()))\n",
    "print(huge_db_dict[list(huge_db_dict.keys())[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "huge_dict = { }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_rejected_to_distributions(distributions, rejected_class):\n",
    "    '''\n",
    "    Add rejected to distributions according to the class\n",
    "    '''\n",
    "    if rejected_class not in distributions:\n",
    "        distributions[rejected_class] = 0\n",
    "    distributions[rejected_class] += 1\n",
    "\n",
    "    return\n",
    "\n",
    "def matching_in_wl(basename, huge_dict, ax= None):\n",
    "    cur_image_dict = huge_dict[basename]\n",
    "    \n",
    "    angle = cur_image_dict[\"SOLAR_P0\"]\n",
    "    deltashapeX = cur_image_dict[\"deltashapeX\"]\n",
    "    deltashapeY = cur_image_dict[\"deltashapeY\"]\n",
    "    \n",
    "    drawing_radius_px = huge_db_dict[basename][\"dr_radius_px\"]\n",
    "    \n",
    "    group_list = cur_image_dict['db']\n",
    "    \n",
    "    ms_dict = cur_image_dict['meanshift']\n",
    "    \n",
    "#     print(ms_dict)\n",
    "    \n",
    "    centroids = np.array(ms_dict[\"centroids\"])\n",
    "    centroids_px = np.array(ms_dict[\"centroids_px\"])\n",
    "    \n",
    "    \n",
    "    ms_centroids, ms_members = centroids_px, ms_dict['groups_px']\n",
    "#     print('ms_members', ms_members)\n",
    "    \n",
    "    db_classes = [{\"Zurich\":item['Zurich'], \"McIntosh\":item['McIntosh'] } for item in group_list]\n",
    "    db_bboxes = [np.array(item['bbox_wl']) for item in group_list]\n",
    "\n",
    "\n",
    "    db_centers_px = np.array([[(b[2]+b[0])/2,(b[3]+b[1])/2] for b in db_bboxes])\n",
    "    \n",
    "    ########\n",
    "    \n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(5,5))\n",
    "        \n",
    "    for i, bbox in enumerate(db_bboxes):\n",
    "        ax.scatter(bbox[0],bbox[1])\n",
    "        linestyle = '-'\n",
    "        ax.add_patch(patches.Rectangle((bbox[0],bbox[1]),bbox[2]-bbox[0],bbox[3]-bbox[1],linewidth=1,edgecolor='b',facecolor='none', linestyle=linestyle))\n",
    "    \n",
    "    for g in ms_members:\n",
    "        g = np.array(g)\n",
    "        ax.scatter(g[:,1],g[:,0])\n",
    "    ax.set_xlim(0,2048)\n",
    "    ax.set_ylim(0,2048)\n",
    "\n",
    "    for c in ms_centroids:\n",
    "        ax.scatter(c[0],c[1], c='r', marker='x')\n",
    "    \n",
    "    ########\n",
    "        \n",
    "    # check that current bbox is does not overlap any\n",
    "    isolated_bboxes_bool = np.array(c_utils.get_intersecting_db_bboxes(db_bboxes)) == 0\n",
    "    isolated_bboxes_indices = np.where(isolated_bboxes_bool == True)[0]\n",
    "   \n",
    "    cur_rejected_class_distibutions = { \n",
    "            'noMS_but_DB': {},\n",
    "            'singleMS_multipleDB': {}, \n",
    "            'num_oneDBbbox_multipleMSoverlap_ambiguity':{},\n",
    "            # 'noDB_but_MS': {},\n",
    "        }\n",
    "    cur_out_stats = {\n",
    "        # General info\n",
    "        'num_DB_groups':len(db_bboxes),\n",
    "        'num_MS_groups':len(centroids_px),\n",
    "        'num_DB_isolated_groups':len(isolated_bboxes_indices),\n",
    "        'num_DB_overlaping_bboxes':len(db_bboxes) - len(isolated_bboxes_indices),\n",
    "        # MS with DB matching info\n",
    "        \"num_MSmatchesDB\":0,\n",
    "        # MS with DB rejection info\n",
    "        \"num_noMS_but_DB_reject\":0,\n",
    "        \"num_singleMS_multipleDB_reject\":0,\n",
    "        \"num_oneDBbbox_multipleMSoverlap_ambiguity_reject\":0,\n",
    "        # MS with DB no match info\n",
    "        \"num_noDB_but_MS\":0,\n",
    "        }\n",
    "    cur_out_groups = []\n",
    "    for i, (db_bbox, db_center, db_class) in enumerate(\n",
    "                                            zip([db_bboxes[a] for a in isolated_bboxes_indices.tolist() ],\n",
    "                                                [db_centers_px[a] for a in isolated_bboxes_indices.tolist()],\n",
    "                                                [db_classes[a] for a in isolated_bboxes_indices.tolist()],\n",
    "                                            )):\n",
    "        \n",
    "        \n",
    "        \n",
    "        intersect = c_utils.contains_ms_groups(db_bbox, db_center, ms_centroids, ms_members)\n",
    "        \n",
    "        if sum(intersect) == 0: # Il n'y a eu aucune détection dans cette zone\n",
    "            cur_out_stats['num_noMS_but_DB_reject'] += 1\n",
    "            cause = 'noMS_but_DB'\n",
    "            add_rejected_to_distributions(cur_rejected_class_distibutions[cause], db_class[\"McIntosh\"][0])\n",
    "            pass\n",
    "        elif sum(intersect) == 1: # il n'y a de l'overlap qu'avec un seul groupe meanshift            \n",
    "#             print('hit')\n",
    "            idx = intersect.index(True)\n",
    "#             print(idx)\n",
    "            # vérifier que le groupe meanshift n'intersecte aucune autre bbox\n",
    "            num_intersections = np.sum(c_utils.count_group_intersections(ms_members[idx], db_bboxes))\n",
    "            if num_intersections > 1:\n",
    "                cur_out_stats['num_singleMS_multipleDB_reject'] += 1\n",
    "                cause = 'singleMS_multipleDB'\n",
    "                add_rejected_to_distributions(cur_rejected_class_distibutions[cause], db_class[\"McIntosh\"][0])\n",
    "                continue\n",
    "            \n",
    "            Rmm = huge_db_dict[basename]['dr_radius_mm']\n",
    "            R_pixel = huge_db_dict[basename]['dr_radius_px']\n",
    "            sun_center = huge_db_dict[basename]['dr_center_px']\n",
    "            dr_pixpos = np.array([group_list[i]['posx'], group_list[i]['posy']])\n",
    "            \n",
    "            angular_excentricity =  c_utils.get_angle2(dr_pixpos, R_pixel, sun_center)\n",
    "            \n",
    "            cur_group_dict={\n",
    "                            \"centroid_px\": centroids_px[idx],\n",
    "                            \"centroid_Lat\": centroids[idx][0],\n",
    "                            \"centroid_Lon\": centroids[idx][1],\n",
    "                            \"angular_excentricity_rad\": angular_excentricity,\n",
    "                            \"angular_excentricity_deg\": np.rad2deg(angular_excentricity),\n",
    "                            \"Zurich\":   db_class[\"Zurich\"],\n",
    "                            \"McIntosh\": db_class[\"McIntosh\"],\n",
    "                            \"members\": ms_members[idx],\n",
    "                            \"members_mean_px\": np.mean(ms_members[idx], axis=0),\n",
    "                        }\n",
    "            \n",
    "            \n",
    "            cur_out_groups.append(cur_group_dict)\n",
    "            cur_out_stats['num_MSmatchesDB'] += 1\n",
    "\n",
    "        else: # db_bbox intersecte plusieurs groupes meanshift\n",
    "            cur_out_stats['num_oneDBbbox_multipleMSoverlap_ambiguity_reject'] += 1\n",
    "            cause = 'num_oneDBbbox_multipleMSoverlap_ambiguity'\n",
    "            add_rejected_to_distributions(cur_rejected_class_distibutions[cause], db_class[\"McIntosh\"][0])\n",
    "            pass\n",
    "\n",
    "    if len(centroids>0):\n",
    "        # print(centroids)\n",
    "        # print()\n",
    "        # count the number of MS groups that do not have any overlap with the DB\n",
    "        num_intersections_per_group = [np.sum(c_utils.count_group_intersections(ms_members[idx], db_bboxes)) for idx in range(len(ms_members))]\n",
    "        num_MS_without_DB_overlap = len(np.where(np.array(num_intersections_per_group) == 0)[0])\n",
    "        cur_out_stats['num_noDB_but_MS'] = num_MS_without_DB_overlap\n",
    "    \n",
    "    # print(cur_rejected_class_distibutions)\n",
    "    cur_out_stats['rejected_class_distributions'] = deepcopy(cur_rejected_class_distibutions)\n",
    "            \n",
    "\n",
    "    return cur_out_groups, cur_out_stats,  cur_rejected_class_distibutions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "look_distance = .1\n",
    "kernel_bandwidthLon = .35\n",
    "kernel_bandwidthLat = .08\n",
    "n_iterations = 20\n",
    "\n",
    "idx = 350 # 50\n",
    "basename = os.path.basename(wl_list[idx]).split(\".\")[0]\n",
    "###########\n",
    "huge_dict = {}\n",
    "# print(1)\n",
    "# print(huge_db_dict[os.path.splitext(os.path.basename(wl_list[idx]))[0]])\n",
    "# print(2)\n",
    "result_key, result_dict =  c_utils.process_one_image( wl_list[idx],\n",
    "                            huge_db_dict,\n",
    "                            huge_dict,\n",
    "                            wl_list,\n",
    "                            rotten_list,\n",
    "                            masks_dir,\n",
    "                            look_distance,\n",
    "                            kernel_bandwidthLon,\n",
    "                            kernel_bandwidthLat,\n",
    "                            n_iterations,\n",
    "                            input_type=input_type\n",
    "                        )\n",
    "\n",
    "\n",
    "huge_dict[result_key] = result_dict\n",
    "\n",
    "# print(result_dict['meanshift'])\n",
    "\n",
    "matchings, matchings_stats, rejects_distrib = matching_in_wl(basename,huge_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "colors = ['tab:blue','tab:orange','tab:green','tab:red',\n",
    "          'tab:purple','tab:brown','tab:pink',\n",
    "          'tab:olive','tab:cyan']\n",
    "\n",
    "\n",
    "def rotate_pt_around_center(point, rotation_pt, angle):\n",
    "    '''\n",
    "    @param point: (x,y) tuple of the point to rotate\n",
    "    @param rotation_pt: (x,y) tuple of the point to rotate around\n",
    "    @param angle: angle to rotate in degrees\n",
    "    '''\n",
    "    angle = np.deg2rad(angle)\n",
    "    x, y = point\n",
    "    x0, y0 = rotation_pt\n",
    "    x1 = x0 + np.cos(angle) * (x - x0) - np.sin(angle) * (y - y0)\n",
    "    y1 = y0 + np.sin(angle) * (x - x0) + np.cos(angle) * (y - y0)\n",
    "    return x1, y1\n",
    "\n",
    "\n",
    "def new_refresh(value):\n",
    "#     print('new_refresh',1)\n",
    "#     print('here')\n",
    "    look_distance = look_distance_slider.value\n",
    "    kernel_bandwidthLon = kernel_bandwidthLon_slider.value\n",
    "    kernel_bandwidthLat = kernel_bandwidthLat_slider.value\n",
    "    n_iterations = n_iterations_slider.value\n",
    "    \n",
    "#     print('new_refresh',1)\n",
    "    basename = os.path.basename(wl_list[img_slider.value]).split(\".\")[0]\n",
    "###########\n",
    "    huge_dict = {}\n",
    "    result_key, result_dict =  c_utils.process_one_image( wl_list[img_slider.value],\n",
    "                                huge_db_dict,\n",
    "                                huge_dict,\n",
    "                                wl_list,\n",
    "                                rotten_list,\n",
    "                                masks_dir,\n",
    "                                look_distance,\n",
    "                                kernel_bandwidthLon,\n",
    "                                kernel_bandwidthLat,\n",
    "                                n_iterations,\n",
    "                                input_type=input_type\n",
    "                            )\n",
    "    \n",
    "#     print(basename, result_key)\n",
    "#     print(result_dict)\n",
    "    \n",
    "    huge_dict[result_key] = result_dict\n",
    "#     print(huge_dict)\n",
    "    ax3[2].clear()\n",
    "    matchings, matchings_stats, rejects_distrib = matching_in_wl(basename,huge_dict,ax3[2])\n",
    "    # print(matchings)\n",
    "    # print(matchings_stats)\n",
    "    # print(rejects_distrib)\n",
    "    matchings_ta.value =  result_key + '\\n' + json.dumps(matchings_stats,sort_keys=True, indent=4)\n",
    "\n",
    "##################\n",
    "    cur_db_dict = huge_db_dict[basename]\n",
    "    date = cur_db_dict[\"wl_date\"] \n",
    "\n",
    "    m, h = t_utils.open_and_add_celestial(wl_list[img_slider.value])\n",
    "    corrected = False\n",
    "    if not 'DATE-OBS' in h:\n",
    "        # print('corrected')\n",
    "        m, h = t_utils.open_and_add_celestial2(wl_list[img_slider.value], date_obs=date)\n",
    "        corrected = True\n",
    "    # print('radius =', h['SOLAR_R'])\n",
    "    \n",
    "    \n",
    "#     mask = io.imread(os.path.join(masks_dir,basename+\".png\"))\n",
    "    \n",
    "    if input_type == \"mask\":\n",
    "        mask = io.imread(os.path.join(masks_dir,basename+\".png\"))\n",
    "    elif input_type == \"confidence_map\":\n",
    "        mask = np.load(os.path.join(masks_dir,basename+\"_proba_map.npy\"))\n",
    "    mask2 = mask.copy()\n",
    "    mask2[mask2>0] = 1\n",
    "    \n",
    "    flip_time = \"2003-03-08T00:00:00\"\n",
    "    should_flip = (datetime.fromisoformat(date) - datetime.fromisoformat(flip_time)) < timedelta(0)\n",
    "    if should_flip:\n",
    "        m = sunmap.Map(np.flip(m.data,axis=0), h)      \n",
    "        mask = np.flip(mask,axis=0)\n",
    "        mask2 = np.flip(mask2,axis=0)\n",
    "\n",
    "\n",
    "    cur_db_dict = huge_db_dict[basename]\n",
    "    group_list = cur_db_dict[\"group_list\"]\n",
    "    drawing_radius_px = cur_db_dict[\"dr_radius_px\"]\n",
    "    date = cur_db_dict[\"wl_date\"]\n",
    "    \n",
    "    Rmm = cur_db_dict[\"dr_radius_mm\"]\n",
    "\n",
    "    sunspots_sk, sunspots_areas_muHem = c_utils.get_sunspots4(h,m, mask2, Rmm, sky_coords=True)\n",
    "    sunspots_pixel, sunspots_areas = c_utils.get_sunspots4(h, m, mask2, Rmm, sky_coords=False)\n",
    "#     sunspots_sk, sunspots_areas = c_utils.get_sunspots3(h,m, mask2, sky_coords=True)\n",
    "#     sunspots_pixel, _ = c_utils.get_sunspots3(h, m, mask2, sky_coords=False)\n",
    "\n",
    "    wcs2 = WCS(h)\n",
    "    \n",
    "    if sunspots_pixel is not None:\n",
    "\n",
    "\n",
    "        sk_Lon = sunspots_sk.lon.rad\n",
    "        sk_Lat = sunspots_sk.lat.rad\n",
    "        sk_LatLon = np.stack((sk_Lat,sk_Lon),axis=1)\n",
    "\n",
    "        sunspots_areas_muHem = np.array(sunspots_areas_muHem)\n",
    "#         print(sunspots_sk)\n",
    "#         print(sunspots_pixel)\n",
    "#         print(sunspots_areas_muHem)\n",
    "\n",
    "\n",
    "        nan_indexes = np.unique(np.argwhere(np.isnan(sk_LatLon))[:,0])\n",
    "#         print('nan_indexes', nan_indexes)\n",
    "        clean = (~np.isnan(sk_Lon) & ~np.isnan(sk_Lat))\n",
    "        if len(nan_indexes) > 0:\n",
    "            sunspots_sk = sunspots_sk[clean]\n",
    "            sunspots_areas = (np.array(sunspots_areas)[clean]).tolist()\n",
    "            sunspots_areas_muHem = np.array(sunspots_areas_muHem)[clean]\n",
    "            sunspots_pixel = sunspots_pixel[clean]\n",
    "            sk_LatLon = sk_LatLon[clean]\n",
    "#             print(len(sk_LatLon),len(sunspots_areas_muHem))\n",
    "\n",
    "\n",
    "\n",
    "        global ms_model\n",
    "    #     print(sunspots_sk.radius.km[0])\n",
    "        ms_model = MS.Mean_Shift(look_distance, kernel_bandwidthLon, kernel_bandwidthLat, sunspots_sk.radius.km[0], n_iterations, max_scaled_area_muHem=200)\n",
    "\n",
    "    #     ms_model.fit(sk_LatLon, sunspots_areas)\n",
    "        ms_model.fit(sk_LatLon, sunspots_areas_muHem)\n",
    "        \n",
    "#         print(ms_model.history)\n",
    "\n",
    "        ms_centroids = ms_model.centroids\n",
    "\n",
    "        sk_sequ_meanshift = SkyCoord(ms_centroids[:,1]*u.rad, ms_centroids[:,0]*u.rad , frame=frames.HeliographicCarrington,\n",
    "                            obstime=m.date, observer=\"earth\")\n",
    "\n",
    "        pix_centers_meanshift = np.array(skycoord_to_pixel(sk_sequ_meanshift, wcs2, origin=0)).T.tolist()\n",
    "\n",
    "        ms_classifications = ms_model.predict(sk_LatLon)\n",
    "#         print(ms_classifications.shape, np.unique(ms_classifications))\n",
    "\n",
    "        ms_group_sunspots = [(sk_LatLon[ms_classifications == c].tolist()) for c in np.unique(ms_classifications)] \n",
    "        ms_group_sunspots_px = [sunspots_pixel[ms_classifications == c].tolist() for c in np.unique(ms_classifications)]\n",
    "\n",
    "        ms_group_sunspots_areas = [sunspots_areas_muHem[ms_classifications == c].tolist() for c in np.unique(ms_classifications)]\n",
    "        # print(len(ms_group_sunspots))\n",
    "\n",
    "    #latitude and longitudes in radians\n",
    "    extreme_values = -np.pi, np.pi, 0, 2*np.pi\n",
    "\n",
    "    dr_obstime = date+'.000'  \n",
    "    all_sks = []\n",
    "    all_pixels = []\n",
    "    for item in group_list:\n",
    "        cur_sk = SkyCoord(item[\"Longitude\"]*u.rad, item[\"Latitude\"]*u.rad , frame=frames.HeliographicCarrington,\n",
    "                      obstime=dr_obstime, observer=\"earth\") \n",
    "        coords_wl = skycoord_to_pixel(cur_sk, wcs2, origin=0)\n",
    "        all_sks.append(cur_sk)\n",
    "        all_pixels.append(coords_wl)\n",
    "\n",
    "    bboxes, bboxes_wl, rectangles, rectangles_wl = c_utils.grouplist2bboxes_and_rectangles(group_list, \n",
    "                                                                                   drawing_radius_px,\n",
    "                                                                                   h[\"SOLAR_R\"],\n",
    "                                                                                   all_pixels)\n",
    "\n",
    "    # rotated_bboxes = [plt.Rectangle((bbox_wl[0], bbox_wl[1]),\n",
    "    #                                 bbox_wl[2]-bbox_wl[0], bbox_wl[3]-bbox_wl[1],\n",
    "    #                               color='b', fill=False,) for bbox_wl in bboxes_wl]\n",
    "    #                             #   color='b', fill=False, angle=h[\"SOLAR_R\"]) for bbox_wl in bboxes_wl]\n",
    "                                \n",
    "    rotated_bbox_ref = [ rotate_pt_around_center((bbox_wl[0], bbox_wl[1]), \n",
    "                                                (bbox_wl[0]+(bbox_wl[2] - bbox_wl[0] )/2 , \n",
    "                                                    bbox_wl[1]+(bbox_wl[3] - bbox_wl[1] )/2), \n",
    "                                                -h[\"SOLAR_R\"]) for bbox_wl in bboxes_wl]\n",
    "    rotated_bboxes = [plt.Rectangle((rotated_bbox_ref[i][0], rotated_bbox_ref[i][1]),\n",
    "                                                 bbox_wl[3]-bbox_wl[1], bbox_wl[2]-bbox_wl[0],\n",
    "                                                angle=-h[\"SOLAR_R\"],color='b', fill=False,) \n",
    "                                        for i,bbox_wl in enumerate(bboxes_wl)]\n",
    "                                                                                   \n",
    "    # print(sk_LatLon.shape)\n",
    "    ax3[0].clear(), ax3[1].clear()#, ax3[2].clear()\n",
    "    ax3[0].set_title(basename)\n",
    "    ax3[0].imshow(m.data,cmap='gray')\n",
    "    ax3[0].imshow(mask,cmap='jet',alpha=0.5)\n",
    "    for i, r in enumerate(rotated_bboxes):\n",
    "        ax3[0].add_patch(r)\n",
    "        if info_cb.value:\n",
    "            ax3[0].text(rotated_bbox_ref[i][0], rotated_bbox_ref[i][1], \n",
    "                          f' {group_list[i][\"McIntosh\"]} : {group_list[i][\"area_muHem\"]}',color='b')\n",
    "    ax3[0].invert_yaxis()\n",
    "\n",
    "    ax3[1].imshow(m.data,cmap='gray')\n",
    "    ax3[1].invert_yaxis()\n",
    "    \n",
    "    ax3[1].set_title(f'angle: {h[\"SOLAR_P0\"]}, should_flip: {should_flip}')\n",
    "    if sunspots_pixel is not None:\n",
    "        for i in range(len(ms_group_sunspots)):\n",
    "            c = colors[np.unique(ms_classifications)[i]%len(colors)]\n",
    "            # c = colors[ms_classifications[i]%len(colors)]\n",
    "            cur = np.array(ms_group_sunspots_px[i])\n",
    "            ms_centers = pix_centers_meanshift[i]\n",
    "            ax3[1].scatter(ms_centers[0],ms_centers[1], s=10, c=c, marker='x')\n",
    "            ax3[1].scatter(cur[:,1], cur[:,0], color=c, s=2)\n",
    "    #         print(ms_group_sunspots_areas[i])\n",
    "            if info_cb.value:\n",
    "                for j in range(len(cur)):\n",
    "                    ax3[1].text(cur[j,1], cur[j,0],  '%.2f' % ms_group_sunspots_areas[i][j] ,va='top',c=c)\n",
    "\n",
    "    #     ax3[2].scatter(ms_centroids[:,1], ms_centroids[:,0], s=1)\n",
    "    #     ax3[2].set_ylim(extreme_values[0], extreme_values[1])\n",
    "    #     ax3[2].set_xlim(extreme_values[2], extreme_values[3])\n",
    "\n",
    "        # ax5[1].scatter(sk_LatLon[:,1], sk_LatLon[:,0], s=1)\n",
    "        # ax5[1].set_xlim(0, 2*np.pi)\n",
    "        # ax5[1].set_ylim(-np.pi/2, np.pi/2)\n",
    "\n",
    "    \n",
    "    \n",
    "    hist_refresh(None)\n",
    "\n",
    "def hist_refresh(change):\n",
    "    # if (xlims0, ylims0) != (0, 1):\n",
    "    xlims0 = ax5.get_xlim()\n",
    "    ylims0 = ax5.get_ylim()\n",
    "    # print(xlims0, ylims0)\n",
    "    \n",
    "    look_distance = look_distance_slider.value\n",
    "    kernel_bandwidthLon = kernel_bandwidthLon_slider.value\n",
    "    kernel_bandwidthLat = kernel_bandwidthLat_slider.value\n",
    "    n_iterations = n_iterations_slider.value\n",
    "\n",
    "    global ms_model\n",
    "    \n",
    "        \n",
    "    # print(ms_model.history)\n",
    "    step = hist_slider.value\n",
    "    ax5.clear()\n",
    "    ax5.set_title('History step {}'.format(step))\n",
    "    if ms_model is not None:\n",
    "        for i in range(len(ms_model.history[step])):\n",
    "            cur_width = ms_model.get_area_weighted_ellipsis_width( ms_model.areas[i], ms_model.areas)\n",
    "            cur_height = kernel_bandwidthLat\n",
    "            # if (ms_model.data[i][0] <= 0.28 and ms_model.data[i][0] >= 0.25) and (ms_model.data[i][1] >= 1.45 and ms_model.data[i][1] <= 1.48):\n",
    "            #     print(cur_width, cur_height)\n",
    "            ellipsis = matplotlib.patches.Ellipse((ms_model.history[step][i,1], ms_model.history[step][i,0]), 2*cur_width, 2*cur_height, fill=False, color='red')\n",
    "            # ellipsis = matplotlib.patches.Ellipse((ms_model.history[step][i,1], ms_model.history[step][i,0]), 2*kernel_bandwidthLon, 2*kernel_bandwidthLat, fill=False, color='red')\n",
    "            ax5.add_patch(ellipsis)\n",
    "        ax5.scatter(ms_model.history[0][:,1], ms_model.history[0][:,0], s=4, marker='X', c='g')\n",
    "        ax5.scatter(ms_model.history[step][:,1], ms_model.history[step][:,0], s=3)\n",
    "    ax5.set_ylim(-np.pi/2, np.pi/2)\n",
    "    ax5.set_xlim(0, 2*np.pi)\n",
    "    # ax5.set_xlim(np.min(ms_model.data[:,1]), np.max(ms_model.data[:,1]))\n",
    "    if (xlims0, ylims0) != ((0., 1.),(0., 1.)):\n",
    "        ax5.set_xlim(xlims0)\n",
    "        ax5.set_ylim(ylims0)\n",
    "    \n",
    "\n",
    "# look_distance = .1 # How far to look for neighbours.\n",
    "# kernel_bandwidthLon = .2  # Longitude Kernel parameter.\n",
    "# kernel_bandwidthLat = .1  # Latitude Kernel parameter.\n",
    "# n_iterations = 20 # Number of iterations\n",
    "input_type = 'confidence_map'\n",
    "    \n",
    "ms_model = None\n",
    "# 830, 536, 509, 24, 31, 183\n",
    "#30\n",
    "# img_slider = widgets.IntSlider(min=0, max=len(wl_list)-1, step=1, value=24, description='Image')\n",
    "#début des problèmes 349\n",
    "img_slider = widgets.IntSlider(min=0, max=len(wl_list)-1, step=1, value=2183, description='Image')\n",
    "# img_slider = widgets.IntSlider(min=0, max=len(wl_list)-1, step=1, value=1505, description='Image')\n",
    "info_cb = widgets.Checkbox(value=False, description='Info', disabled=False, indent=False)\n",
    "\n",
    "max_n_iterations = 20\n",
    "look_distance_slider = widgets.FloatSlider(min=.1,max=.1, description='look_distance')\n",
    "kernel_bandwidthLon_slider = widgets.FloatSlider(min=.35,max=.45,step=.05, description='kernel_bandwidthLon')\n",
    "kernel_bandwidthLat_slider = widgets.FloatSlider(min=.08,max=.2,step=.02, description='kernel_bandwidthLat')\n",
    "n_iterations_slider = widgets.IntSlider(min=20,max=max_n_iterations, description='look_distance')\n",
    "\n",
    "img_slider.observe(new_refresh, 'value')\n",
    "info_cb.observe(new_refresh, 'value')\n",
    "\n",
    "look_distance_slider.observe(new_refresh,'value')\n",
    "kernel_bandwidthLon_slider.observe(new_refresh,'value')\n",
    "kernel_bandwidthLat_slider.observe(new_refresh,'value')\n",
    "n_iterations_slider.observe(new_refresh,'value')\n",
    "\n",
    "hist_slider = widgets.IntSlider(min=0, max=max_n_iterations-1, step=1, value=0, description='History')\n",
    "hist_slider.observe(hist_refresh, 'value')\n",
    "\n",
    "matchings_ta = widgets.Textarea(description='Matchings', value='',layout=widgets.Layout(height=\"100%\", width=\"100%\"))\n",
    "def matchings_refresh(change):\n",
    "    matchings_ta.rows = matchings_ta.value.count('\\n') + 1\n",
    "matchings_ta.observe(matchings_refresh, 'value')\n",
    "\n",
    "plt.ioff()\n",
    "fig3,ax3 = plt.subplots(1,3,figsize=(9,3))\n",
    "fig5,ax5 = plt.subplots(1,1,figsize=(5,2.5))\n",
    "fig3.tight_layout()\n",
    "# print('ICI1')\n",
    "new_refresh(None)\n",
    "# print('ICI2')\n",
    "hist_refresh(None)\n",
    "plt.ion()\n",
    "\n",
    "rotten_list = [\n",
    "    \n",
    "    37,38,39,40,52, 64,65,69,70,\n",
    "    \n",
    "    72,97,99,100,101,102,103,104,142,159,160,161,169,187,190,210,211,212,218,264,300,312,314,316,319,322,327,339,\n",
    "    343,353,356,387,408,413,414,418,424,425,448,473,474,493,512,508,611,614,666,675,696,726,330,747,750,758,\n",
    "    761,784,804,823,832,840,855,914,935,940,948,990,1013,\n",
    "    \n",
    "    1025,1039,1040,1089,1172,1303,1332,1345,1397,1409,1413,1414,1421,1440,1444,1468,1469,1488,1576,1646,1692,\n",
    "    1735,1815,1840,1867,1893,1900,1905,1919,1924,1925,1930,1953,1969,1992,\n",
    "    \n",
    "    2007,2039,2043,2045,2049,2050,2078,2121,2133,2143,2185,2208,2220,2254,2266,2272,2298,2344,3262,3274,2375,\n",
    "    2445,2454,2468,2492,2494,2495,2500,2501,2503,2516,2518,2536,2568,2574,2598,2604,2633,2635,2749,2763,2815,\n",
    "    2818,2820,2821,2834,2835,2851,2857,2867,2896,2899,2848,2951,2952,2956,2964,2980,2981,2994,\n",
    "    \n",
    "    3018,3092,3093,3097,3099,3101,3106,3118,3122,3123,3124,3140,3148\n",
    "    \n",
    "    \n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "display(widgets.HBox([widgets.VBox([look_distance_slider,\n",
    "                          kernel_bandwidthLon_slider,\n",
    "                          kernel_bandwidthLat_slider,\n",
    "                          n_iterations_slider,img_slider, info_cb], layout=widgets.Layout(width='25%',object_position='bottom') ),\n",
    "                      fig3.canvas]))\n",
    "# display(widgets.HBox([matchings_ta, widgets.VBox([widgets.HBox([hist_slider]), fig5.canvas])]))\n",
    "display(widgets.HBox([widgets.HBox([matchings_ta],layout=widgets.Layout(width='50%')), \n",
    "                      widgets.HBox([widgets.VBox([widgets.HBox([hist_slider]), fig5.canvas])])\n",
    "                     ]))\n",
    "# display(matchings_ta)\n",
    "\n",
    "# text_area_input = pn.widgets.input.TextAreaInput(name='Text Area Input', placeholder='Enter a string here...')\n",
    "# display(text_area_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_values = {\n",
    "    'look_distance' : [0.1],\n",
    "    # 'kernel_bandwidthLon' : [ 0.05 , 0.1, 0.15, .2,.21,.22,.23,.24,.25, .3,.35,.45],\n",
    "    # 'kernel_bandwidthLat' : [.08,],\n",
    "    'kernel_bandwidthLon' : [ 0.1, 0.15, .2, .25, .3, .35, .4, .45, .5, .55, .6, .65, .7, .75],\n",
    "#     'kernel_bandwidthLon' : [ .25],\n",
    "#     'kernel_bandwidthLat' : [ 0.02, 0.04 , 0.06, .08, .1, .12, .16, .18, .2],\n",
    "    'kernel_bandwidthLat' : [ 0.02, 0.04 , 0.06, .08, .1, .12],\n",
    "    # 'kernel_bandwidthLat' : [ 0.05 , 0.06, 0.07, .08, .09, .1, .11, .12],\n",
    "    'n_iterations' : [20],\n",
    "}\n",
    "\n",
    "param_grid = ParameterGrid(param_grid_values)\n",
    "num_cpu = 20\n",
    "# num_cpu = 30\n",
    "input_type = \"mask\"\n",
    "\n",
    "grid_huge_dict = {str(i): {} for i,_ in enumerate(param_grid)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "for i, params in tqdm(enumerate(param_grid)):\n",
    "    # if i > 0:\n",
    "    #     break\n",
    "    print(i, params)\n",
    "    look_distance = params['look_distance']  # How far to look for neighbours.\n",
    "    kernel_bandwidthLon = params['kernel_bandwidthLon']  # Longitude Kernel parameter.\n",
    "    kernel_bandwidthLat = params['kernel_bandwidthLat']  # Latitude Kernel parameter.\n",
    "    n_iterations = params['n_iterations'] # Number of iterations\n",
    "\n",
    "    \n",
    "    cur_huge_dict = { } if str(i) not in grid_huge_dict else grid_huge_dict[str(i)]\n",
    "    print(len(list(cur_huge_dict.keys())))    \n",
    "#     grid_huge_dict[i] = {'a':0}\n",
    "\n",
    "\n",
    "    with concurrent.futures.ProcessPoolExecutor(max_workers=int(num_cpu)) as executor:\n",
    "        for result_key, result_dict in tqdm(executor.map(c_utils.process_one_image, \n",
    "                                                            wl_list[:],\n",
    "                                                            repeat(huge_db_dict),\n",
    "                                                            repeat(cur_huge_dict),\n",
    "                                                            repeat(wl_list),\n",
    "                                                            repeat(rotten_list),\n",
    "                                                            repeat(masks_dir),\n",
    "                                                            repeat(look_distance),\n",
    "                                                            repeat(kernel_bandwidthLon),\n",
    "                                                            repeat(kernel_bandwidthLat),\n",
    "                                                            repeat(n_iterations),\n",
    "                                                            repeat(input_type)\n",
    "                                                            )):\n",
    "            # print(result_key)\n",
    "            if not len(list(result_dict.keys())) == 0:\n",
    "                print('here')\n",
    "#                 cur_huge_dict[result_key] = deepcopy(result_dict)\n",
    "                grid_huge_dict[str(i)][result_key] = deepcopy(result_dict)\n",
    "\n",
    "#     grid_huge_dict[i] = deepcopy(cur_huge_dict)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NpEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        if isinstance(obj, np.int64):\n",
    "            return int(obj)\n",
    "        if isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        if isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        if isinstance(obj, np.bool_):\n",
    "            return bool(obj)\n",
    "        return super(NpEncoder, self).default(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "    \n",
    "import json \n",
    "tmp = './grid_search_huge_dict_2002-19.json'\n",
    "with open(tmp, 'w') as f:\n",
    "    json.dump(grid_huge_dict, f, cls=NpEncoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_values = {\n",
    "    'look_distance' : [0.1],\n",
    "    # 'kernel_bandwidthLon' : [ 0.05 , 0.1, 0.15, .2,.21,.22,.23,.24,.25, .3,.35,.45],\n",
    "    # 'kernel_bandwidthLat' : [.08,],\n",
    "    'kernel_bandwidthLon' : [ 0.1, 0.15, .2, .25, .3, .35, .4, .45, .5, .55, .6, .65, .7, .75],\n",
    "#     'kernel_bandwidthLon' : [ .25],\n",
    "#     'kernel_bandwidthLat' : [ 0.02, 0.04 , 0.06, .08, .1, .12, .16, .18, .2],\n",
    "    'kernel_bandwidthLat' : [ 0.02, 0.04 , 0.06, .08, .1, .12],\n",
    "    # 'kernel_bandwidthLat' : [ 0.05 , 0.06, 0.07, .08, .09, .1, .11, .12],\n",
    "    'n_iterations' : [20],\n",
    "}\n",
    "\n",
    "param_grid = ParameterGrid(param_grid_values)\n",
    "\n",
    "import json \n",
    "tmp2 = './grid_search_huge_dict.json'\n",
    "with open(tmp2, 'r') as f:\n",
    "    grid_huge_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(grid_huge_dict.keys())):\n",
    "    \n",
    "# #     print(np.array(grid_huge_dict[i]['UPH20130314131700']['meanshift']['centroids']))\n",
    "#     print(np.array(grid_huge_dict[i]['UPH20130314131700']['meanshift']['centroids']).shape)\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "def compute_IoUs(db_bboxes, ms_bboxes):\n",
    "    '''Compute IoUs between all bboxes in db_bboxes and ms_bboxes.'''\n",
    "    db_bboxes = np.array(db_bboxes)\n",
    "    ms_bboxes = np.array(ms_bboxes)\n",
    "    # print(db_bboxes.shape)\n",
    "    # print(ms_bboxes.shape)\n",
    "\n",
    "    # if db_bboxes is single bbox, convert to 2D array.\n",
    "    if db_bboxes.ndim == 1:\n",
    "        db_bboxes = np.array([db_bboxes])\n",
    "    # if ms_bboxes is single bbox, convert to 2D array.\n",
    "    if ms_bboxes.ndim == 1:\n",
    "        ms_bboxes = np.array([ms_bboxes])\n",
    "\n",
    "    # if db_bboxes is empty, return empty 2D array.\n",
    "    if db_bboxes.size == 0:\n",
    "        return np.array([[]])\n",
    "\n",
    "    # print(db_bboxes.shape)\n",
    "    # print(ms_bboxes.shape)\n",
    "    # db_bboxes has shape (N, 4) and ms_bboxes has shape (M, 4).\n",
    "    # each element has the form [x1, y1, x2, y2].\n",
    "    # We want to compute IoUs between all N and M boxes.\n",
    "    db_bboxes = db_bboxes.astype(np.float)\n",
    "    ms_bboxes = ms_bboxes.astype(np.float)\n",
    "    # print(db_bboxes.shape)\n",
    "    # print(ms_bboxes.shape)\n",
    "\n",
    "    # Compute areas of all db_bboxes and ms_bboxes.\n",
    "    area_db = (db_bboxes[:, 2] - db_bboxes[:, 0]) * (db_bboxes[:, 3] - db_bboxes[:, 1])\n",
    "    area_ms = (ms_bboxes[:, 2] - ms_bboxes[:, 0]) * (ms_bboxes[:, 3] - ms_bboxes[:, 1])\n",
    "    # print(area_db.shape, area_ms.shape)\n",
    "\n",
    "    # compute intersections\n",
    "    # intersections has shape (N, M) and intersections[i, j] is the intersection\n",
    "    # between db_bboxes[i] and ms_bboxes[j].\n",
    "    intersections = np.zeros((db_bboxes.shape[0], ms_bboxes.shape[0]))\n",
    "    for i in range(db_bboxes.shape[0]):\n",
    "        for j in range(ms_bboxes.shape[0]):\n",
    "            x1 = max(db_bboxes[i, 0], ms_bboxes[j, 0])\n",
    "            y1 = max(db_bboxes[i, 1], ms_bboxes[j, 1])\n",
    "            x2 = min(db_bboxes[i, 2], ms_bboxes[j, 2])\n",
    "            y2 = min(db_bboxes[i, 3], ms_bboxes[j, 3])\n",
    "            intersections[i, j] = max(x2 - x1, 0) * max(y2 - y1, 0)\n",
    "    # print(intersections.shape)\n",
    "    # print(intersections)\n",
    "\n",
    "    # compute unions\n",
    "    unions = area_db[:, np.newaxis] + area_ms[np.newaxis, :] - intersections\n",
    "\n",
    "    # compute IoUs\n",
    "    ious = intersections / unions\n",
    "\n",
    "\n",
    "    return ious\n",
    "\n",
    "def compute_distances(db_bboxes, ms_bboxes):\n",
    "    '''Compute distances between all bboxes in db_bboxes and ms_bboxes.'''\n",
    "    db_bboxes = np.array(db_bboxes)\n",
    "    ms_bboxes = np.array(ms_bboxes)\n",
    "\n",
    "    # if db_bboxes is single bbox, convert to 2D array.\n",
    "    if db_bboxes.ndim == 1:\n",
    "        db_bboxes = np.array([db_bboxes])\n",
    "    # if ms_bboxes is single bbox, convert to 2D array.\n",
    "    if ms_bboxes.ndim == 1:\n",
    "        ms_bboxes = np.array([ms_bboxes])\n",
    "\n",
    "    # if db_bboxes is empty, return empty 2D array.\n",
    "    if db_bboxes.size == 0:\n",
    "        return np.array([[]])\n",
    "\n",
    "    # db_bboxes has shape (N, 4) and ms_bboxes has shape (M, 4).\n",
    "    # each element has the form [x1, y1, x2, y2].\n",
    "    # We want to compute distances between all N and M boxes.\n",
    "    db_bboxes = db_bboxes.astype(np.float)\n",
    "    ms_bboxes = ms_bboxes.astype(np.float)\n",
    "\n",
    "    # Compute centers of all db_bboxes and ms_bboxes.\n",
    "    center_db = (db_bboxes[:, 2:] + db_bboxes[:, :2]) / 2\n",
    "    center_ms = (ms_bboxes[:, 2:] + ms_bboxes[:, :2]) / 2\n",
    "    # print('here')\n",
    "    # print(center_db.shape)\n",
    "    # print(center_ms.shape)\n",
    "\n",
    "    distances = np.repeat( center_ms[ np.newaxis,:,:], center_db.shape[0], axis=0)\n",
    "\n",
    "    # print(distances.shape)\n",
    "\n",
    "    distances = np.sqrt(np.sum((distances - center_db[:,np.newaxis,:])**2, axis=2))\n",
    "\n",
    "    # print(distances.shape)\n",
    "    # print(distances)\n",
    "\n",
    "    return distances\n",
    "\n",
    "\n",
    "def find_closest_ms_bbox(db_bboxes, ms_bboxes, maximum_distance=300):\n",
    "    '''Find the closest ms_bbox to db_bbox.'''\n",
    "    ious = compute_IoUs(db_bboxes, ms_bboxes)\n",
    "    distances = compute_distances(db_bboxes, ms_bboxes)\n",
    "    \n",
    "\n",
    "    # if ious is empty, closest_ms_bbox is empty.\n",
    "    if (ious.size == 0) and (distances.size == 0):\n",
    "        closest_ms_bbox = np.array([])\n",
    "        closest_ms_bbox_idx = np.array([])\n",
    "        \n",
    "        closest_db_bbox = np.array([])\n",
    "        closest_db_bbox_idx = np.array([])\n",
    "    else:\n",
    "        # Find the closest ms_bbox to each db_bbox.\n",
    "        closest_ms_bbox = np.min(distances, axis=1)\n",
    "        closest_ms_bbox_idx = np.argmin(distances, axis=1)\n",
    "        # print(\"closest_ms_bbox_idx\", closest_ms_bbox_idx)\n",
    "        \n",
    "        \n",
    "        # Find the closest db_bbox to each ms_bbox.\n",
    "        closest_db_bbox = np.min(distances.T, axis=1)\n",
    "        closest_db_bbox_idx = np.argmin(distances.T, axis=1)\n",
    "    \n",
    "    # print(closest_ms_bbox.shape)\n",
    "    # print(closest_ms_bbox_idx.shape)\n",
    "    # print(closest_ms_bbox)\n",
    "    # print(closest_ms_bbox_idx)\n",
    "\n",
    "    # replace the closest ms_bbox with -1 if it is too far away.\n",
    "    closest_ms_bbox[closest_ms_bbox > maximum_distance] = -1\n",
    "    closest_ms_bbox_idx[closest_ms_bbox == -1 ] = -1\n",
    "     \n",
    "    # replace the closest db_bbox with -1 if it is too far away.\n",
    "    closest_db_bbox[closest_db_bbox > maximum_distance] = -1\n",
    "    closest_db_bbox_idx[closest_db_bbox == -1 ] = -1\n",
    "\n",
    "    # count the number of db_bboxes that are too far away.\n",
    "    db_too_far = closest_ms_bbox == -1\n",
    "    num_db_too_far = np.sum(db_too_far)\n",
    "    db_too_far_idx = np.where(db_too_far)[0]\n",
    "    # count the number of ms_bboxes that are too far away.\n",
    "    ms_too_far = closest_db_bbox == -1\n",
    "    num_ms_too_far = np.sum(ms_too_far)\n",
    "    ms_too_far_idx = np.where(ms_too_far)[0]\n",
    "    \n",
    "\n",
    "    # get the ious of the closest ms_bbox to each db_bbox.\n",
    "    closest_ms_bbox_iou = np.array([ious[i,closest_ms_bbox_idx[i]] for i in range(len(db_bboxes))])\n",
    "    closest_ms_bbox_iou[closest_ms_bbox == -1] = -1\n",
    "\n",
    "\n",
    "    multiDB_singleMS_idx = []\n",
    "    num_multiDB_singleMS = 0\n",
    "    # if two db_bboxes have the same closest ms_bbox, keep the one with the highest IoU.\n",
    "    for i in np.unique(closest_ms_bbox_idx):\n",
    "        if i == -1:\n",
    "            continue\n",
    "        # get the indexes of the db_bboxes that have the same closest ms_bbox.\n",
    "        idx = np.where(closest_ms_bbox_idx == i)[0]\n",
    "        if len(idx) > 1:\n",
    "            # keep the db_bbox with the highest IoU.\n",
    "            highest_iou_idx = np.argmax(closest_ms_bbox_iou[idx])\n",
    "            # add the indexes of the other db_bboxes to multiDB_singleMS.\n",
    "            multiDB_singleMS_idx.extend(idx[np.arange(len(idx)) != highest_iou_idx])\n",
    "\n",
    "            # set the closest ms_bbox of the other db_bboxes to -1.\n",
    "            closest_ms_bbox_idx[idx[np.arange(len(idx)) != highest_iou_idx]] = -1\n",
    "            closest_ms_bbox[idx[np.arange(len(idx)) != highest_iou_idx]] = -1\n",
    "            closest_ms_bbox_iou[idx[np.arange(len(idx)) != highest_iou_idx]] = -1\n",
    "            # get the number of indexes that were set to -1.\n",
    "            num_multiDB_singleMS += len(idx) - 1\n",
    "    assert num_multiDB_singleMS == len(multiDB_singleMS_idx)\n",
    "    multiDB_singleMS_idx = np.array(multiDB_singleMS_idx)\n",
    "    \n",
    "\n",
    "    # get the indexes of the ms_bboxes that appear as the closest ms_bbox to some db_bbox.\n",
    "    ms_bbox_idx = np.unique(closest_ms_bbox_idx)\n",
    "    \n",
    "    # remove the -1 index.\n",
    "    ms_bbox_idx = ms_bbox_idx[ms_bbox_idx != -1]\n",
    "    # print(\"ms_bbox_idx\", ms_bbox_idx)\n",
    "\n",
    "    # print(closest_ms_bbox, closest_ms_bbox_idx)\n",
    "    # print(ms_bbox_idx)\n",
    "    \n",
    "    candidates_indexes = np.array(range(len(ms_bboxes)))\n",
    "    unmatched_ms = np.setdiff1d(candidates_indexes, ms_bbox_idx)\n",
    "\n",
    "    if (ious.size == 0) and (distances.size == 0):\n",
    "        bad_ms = np.array([])\n",
    "    else:   \n",
    "        # get the ms_bboxes in unmatched_ms that have iou > 0. with some db_bbox.\n",
    "        bad_ms = unmatched_ms[np.max(ious[:, unmatched_ms], axis=0) > 0.]\n",
    "\n",
    "    # remove bad_ms from unmatched_ms.\n",
    "    unmatched_ms = np.setdiff1d(unmatched_ms, bad_ms)\n",
    "\n",
    "    # unmatched_db contains: \n",
    "    # 1) the indexes of the db_bboxes that are too far away from any ms_bbox. \n",
    "    # 2) the indexes of the db_bboxes that have the same closest ms_bbox as another db_bbox but that have a lower IoU.\n",
    "    unmatched_db = np.where(closest_ms_bbox == -1)[0]\n",
    "\n",
    "    # print(\"unmatched_db\", unmatched_db)\n",
    "    # print(\"multiDB_singleMS\", multiDB_singleMS)\n",
    "    # print(\"db_too_far\", db_too_far)\n",
    "\n",
    "\n",
    "    # 2 types of 'rejected'  db bboxes (candidate too far + candidate better matched by another db bbox) : \n",
    "    # Make sure that the number of unmatched db + multiDB_singleMS is equal to the number of too far db.\n",
    "    assert len(unmatched_db) == num_db_too_far + num_multiDB_singleMS\n",
    "#     assert len(unmatched_ms) == num_ms_too_far +\n",
    "    \n",
    "    # Make sure that the number of unmatched db + ms_bbox_idx (number of 1 to 1 matches) is equal to the number of db_bboxes.\n",
    "    assert len(unmatched_db) + len(ms_bbox_idx) == len(db_bboxes)\n",
    "    # Make sure that the number of bad ms + unmatched ms + ms_bbox_idx is equal to the number of ms_bboxes.\n",
    "    assert len(unmatched_ms) + len(bad_ms) + len(ms_bbox_idx) == len(ms_bboxes)\n",
    "\n",
    "    matched_db = np.where(closest_ms_bbox != -1)[0]\n",
    "    matches = closest_ms_bbox_idx\n",
    "\n",
    "    \n",
    "    return  (ious, distances, \n",
    "            unmatched_ms, bad_ms, ms_too_far_idx, \n",
    "            unmatched_db, multiDB_singleMS_idx, db_too_far_idx, \n",
    "            matched_db, matches)\n",
    "\n",
    "def find_matchings_one_image(cur_huge_dict, basename, wl_dir, mask_dir, input_type, show=False):\n",
    "        cur_image_dict = cur_huge_dict[basename]\n",
    "        \n",
    "        angle = cur_image_dict[\"SOLAR_P0\"]\n",
    "        deltashapeX = cur_image_dict[\"deltashapeX\"]\n",
    "        deltashapeY = cur_image_dict[\"deltashapeY\"]\n",
    "        \n",
    "        drawing_radius_px = huge_db_dict[basename][\"dr_radius_px\"]\n",
    "        \n",
    "        group_list = cur_image_dict['db']\n",
    "        \n",
    "        ms_dict = cur_image_dict['meanshift']\n",
    "\n",
    "        ms_members = ms_dict['groups_px']\n",
    "\n",
    "        # print('ms_dict: ', ms_dict)\n",
    "        \n",
    "        centroids = np.array(ms_dict[\"centroids\"])\n",
    "        centroids_px = np.array(ms_dict[\"centroids_px\"])\n",
    "        \n",
    "        db_classes = [{\"Zurich\":item['Zurich'], \"McIntosh\":item['McIntosh'] } for item in group_list]\n",
    "        # Attention: bbox_wl is in the form [lat1, Lon1, lat2, Lon2] -> [y1, x1, y2, x2]\n",
    "        # x1 is \n",
    "        db_bboxes = [np.array(item['bbox_wl']) for item in group_list]\n",
    "        db_centers_px = np.array([[(b[2]+b[0])/2,(b[3]+b[1])/2] for b in db_bboxes])\n",
    "\n",
    "        # open the image\n",
    "        image = np.array(io.imread(os.path.join(wl_dir, basename + '.FTS')))\n",
    "        image = c_utils.rotate_CV_bound(image, angle, interpolation=cv2.INTER_NEAREST)\n",
    "        image = image[deltashapeX//2:image.shape[0]-deltashapeX//2,\n",
    "                            deltashapeY//2:image.shape[1]-deltashapeY//2]\n",
    "\n",
    "        # open the mask\n",
    "        # mask = np.array(io.imread(os.path.join(masks_dir, basename + '.png')))\n",
    "        if input_type == \"mask\":\n",
    "            mask = io.imread(os.path.join(masks_dir,basename+\".png\"))\n",
    "        elif input_type == \"confidence_map\":\n",
    "#             print(\"here\")\n",
    "            mask = np.load(os.path.join(masks_dir,basename+\"_proba_map.npy\"))\n",
    "            mask[mask>0] = 1   \n",
    "            \n",
    "        msk = c_utils.expand_small_spots(mask)\n",
    "\n",
    "        # rotate the mask\n",
    "        mask = c_utils.rotate_CV_bound(mask, angle, interpolation=cv2.INTER_NEAREST)\n",
    "        mask = mask[deltashapeX//2:mask.shape[0]-deltashapeX//2,\n",
    "                            deltashapeY//2:mask.shape[1]-deltashapeY//2] \n",
    "\n",
    "        group_masks = [c_utils.get_mask_from_coords(mask, members) for members in ms_dict['groups_px']]\n",
    "         \n",
    "        \n",
    "        groups_bboxes = [c_utils.get_bbox_from_mask(mask) for mask in group_masks]\n",
    "        groups_bboxes = [(b[1], b[0], b[3], b[2]) for b in groups_bboxes]\n",
    "\n",
    "        res = find_closest_ms_bbox(db_bboxes, groups_bboxes)\n",
    "        ious, distances, unmatched_ms, bad_ms, ms_too_far, unmatched_db, multiDB_singleMS, db_too_far, matched_db, matches = res\n",
    "        \n",
    "        unmatched_ms = unmatched_ms.tolist()\n",
    "        unmatched_db = unmatched_db.tolist()\n",
    "        bad_ms = bad_ms.tolist()\n",
    "        multiDB_singleMS = multiDB_singleMS.tolist()\n",
    "        db_too_far = db_too_far.tolist()\n",
    "        ms_too_far = ms_too_far.tolist()\n",
    "        \n",
    "\n",
    "        cur_out_stats = {\n",
    "            # General info\n",
    "            'num_DB_groups':len(db_bboxes),\n",
    "            'num_MS_groups':len(centroids_px),\n",
    "\n",
    "            'matches':matches,\n",
    "\n",
    "            # MS with DB matching info\n",
    "            'unmatched_db':unmatched_db,\n",
    "            'multiDB_singleMS': multiDB_singleMS,\n",
    "            'db_too_far':db_too_far,\n",
    "\n",
    "\n",
    "            'unmatched_ms':unmatched_ms,\n",
    "            'bad_ms':bad_ms,\n",
    "            'ms_too_far': ms_too_far,\n",
    "\n",
    "            \"ious\":ious.tolist(),\n",
    "            \"distances\":distances.tolist()\n",
    "\n",
    "            }\n",
    "        \n",
    "        Rmm = huge_db_dict[basename]['dr_radius_mm']\n",
    "        R_pixel = huge_db_dict[basename]['dr_radius_px']\n",
    "        sun_center = huge_db_dict[basename]['dr_center_px']\n",
    "        \n",
    "        cur_out_groups = []\n",
    "        for i, match in enumerate(matches):\n",
    "            # print('i: ', i, 'match: ', match)\n",
    "            if match != -1:\n",
    "                db_class = db_classes[i]\n",
    "\n",
    "                for pt in ms_members[match]:\n",
    "                    # print('pt: ', pt)\n",
    "                    assert c_utils.contains_sunspot(groups_bboxes[match],pt), \"pt: {} not in bbox: {}\".format(pt, groups_bboxes[match])\n",
    "\n",
    "\n",
    "                dr_pixpos = np.array([group_list[i]['posx'], group_list[i]['posy']])\n",
    "                \n",
    "                angular_excentricity =  c_utils.get_angle2(dr_pixpos, R_pixel, sun_center)\n",
    "                \n",
    "                cur_group_dict={\n",
    "                                \"centroid_px\": centroids_px[match],\n",
    "                                \"centroid_Lat\": centroids[match][0],\n",
    "                                \"centroid_Lon\": centroids[match][1],\n",
    "                                \"angular_excentricity_rad\": angular_excentricity,\n",
    "                                \"angular_excentricity_deg\": np.rad2deg(angular_excentricity),\n",
    "                                \"Zurich\":   db_class[\"Zurich\"],\n",
    "                                \"McIntosh\": db_class[\"McIntosh\"],\n",
    "                                \"members\": ms_members[match],\n",
    "                                \"members_mean_px\": np.mean(ms_members[match], axis=0),\n",
    "                            }\n",
    "                \n",
    "                \n",
    "                cur_out_groups.append(cur_group_dict)\n",
    "\n",
    "\n",
    "\n",
    "        out_groups = {}\n",
    "        if len(cur_out_groups) > 0:\n",
    "            out_groups = { \"angle\": angle,\n",
    "                                        \"deltashapeX\":deltashapeX,\n",
    "                                        \"deltashapeY\":deltashapeY,\n",
    "                                        \"groups\": cur_out_groups,\n",
    "                                    }\n",
    "\n",
    "\n",
    "        ############## SHOW THE RESULTS ################\n",
    "        if show:\n",
    "            print('unmatched db: ', unmatched_db)\n",
    "            print('multiDB_singleMS: ', multiDB_singleMS)\n",
    "            print('db_too_far: ', db_too_far)\n",
    "\n",
    "            print('unmatched ms', unmatched_ms)\n",
    "            print('bad_ms: ', bad_ms)\n",
    "\n",
    "            # if ious in not empty\n",
    "            if ious.size != 0:\n",
    "                # get the best iou for each db_bbox\n",
    "                best_ious = np.max(ious, axis=1)\n",
    "                best_ious_idx = np.argmax(ious, axis=1)\n",
    "            # show the mask\n",
    "            # plt.figure()\n",
    "            fig, ax = plt.subplots(1,1, figsize=(5,5))\n",
    "            ax.imshow(image,cmap='gray')\n",
    "            ax.imshow(mask, alpha=0.5)\n",
    "            # show the db_bboxes\n",
    "            for i, bbox in enumerate(db_bboxes):\n",
    "                linestyle = '-'\n",
    "                if i in unmatched_db:\n",
    "                    linestyle = '--'\n",
    "                ax.add_patch(patches.Rectangle((bbox[0],bbox[1]),bbox[2]-bbox[0],bbox[3]-bbox[1],linewidth=1,edgecolor='b',facecolor='none', linestyle=linestyle))\n",
    "                #format best iou to .2f\n",
    "                b=best_ious[i]\n",
    "                b = \"{:.2f}\".format(b)\n",
    "                ax.text(bbox[0],bbox[1], b, color='b')\n",
    "            # show the groups_bboxes\n",
    "            for i, bbox in enumerate(groups_bboxes):\n",
    "                color = 'g'\n",
    "                if i in bad_ms:\n",
    "                    color = 'r'\n",
    "                elif i in unmatched_ms:\n",
    "                    color = 'y'\n",
    "                ax.add_patch(patches.Rectangle((bbox[0],bbox[1]), bbox[2]-bbox[0], bbox[3]-bbox[1],linewidth=1,edgecolor=color,facecolor='none'))\n",
    "                if i in bad_ms:\n",
    "                    ax.text(bbox[0],bbox[1], 'bad', color='r')\n",
    "\n",
    "            for i, match in enumerate(matches):\n",
    "                if match != -1:\n",
    "                    ax.plot([db_bboxes[i][0], groups_bboxes[match][0]], [db_bboxes[i][1], groups_bboxes[match][1]], color='g')\n",
    "            \n",
    "            plt.show()\n",
    "\n",
    "        return basename, out_groups, cur_out_stats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nouvelle méthode de comptage de TP et FN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "grid_image_out_dict = { }\n",
    "grid_image_out_dict_stats = { }\n",
    "num_cpu = 15\n",
    "show=False\n",
    "input_type = \"mask\"\n",
    "# input_type = \"confidence_map\"\n",
    "\n",
    "for param_idx, params in tqdm(enumerate(param_grid)):\n",
    "    print(param_idx, params)\n",
    "\n",
    "    image_out_dict = {}\n",
    "    image_out_dict_stats = {}\n",
    "\n",
    "    cur_huge_dict = deepcopy(grid_huge_dict[str(param_idx)])\n",
    "\n",
    "    with concurrent.futures.ProcessPoolExecutor(max_workers=int(num_cpu)) as executor:\n",
    "        for result_key, result_dict, result_dict_stats in tqdm(executor.map(find_matchings_one_image, \n",
    "                                                            repeat(cur_huge_dict),\n",
    "                                                            list(cur_huge_dict.keys())[:],\n",
    "                                                            repeat(wl_dir),\n",
    "                                                            repeat(masks_dir),\n",
    "                                                            repeat(input_type),\n",
    "                                                            repeat(show)\n",
    "                                                            )):\n",
    "            # print(result_key)\n",
    "#             print(result_dict_stats[\"ms_too_far\"])\n",
    "            image_out_dict[result_key] = result_dict\n",
    "            image_out_dict_stats[result_key] = result_dict_stats\n",
    "            # image_out_dict_stats[result_key] = result_dict['stats']\n",
    "#             break\n",
    "   \n",
    "        \n",
    "        \n",
    "\n",
    "    print('num_images: ', len(list(image_out_dict.keys())))\n",
    "    num_groups = 0\n",
    "    for k,v in image_out_dict.items():\n",
    "        # print(k,v)\n",
    "        if v:\n",
    "            num_groups += len(v['groups'])\n",
    "    print(\"num_groups: \",num_groups)\n",
    "    # print(image_out_dict)\n",
    "\n",
    "    grid_image_out_dict[param_idx] = deepcopy(image_out_dict)\n",
    "    grid_image_out_dict_stats[param_idx] = deepcopy(image_out_dict_stats)\n",
    "\n",
    "#     break           \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "tmp = './grid_image_out_dict2.json'\n",
    "with open(tmp, 'w') as f:\n",
    "    json.dump(grid_image_out_dict, f, cls=NpEncoder)\n",
    "tmp = './grid_image_out_dict_stats2.json'\n",
    "with open(tmp, 'w') as f:\n",
    "    json.dump(grid_image_out_dict_stats, f, cls=NpEncoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp2 = './grid_image_out_dict2.json'\n",
    "with open(tmp2, 'r') as f:\n",
    "    grid_image_out_dict = json.load(f)\n",
    "tmp2 = './grid_image_out_dict_stats2.json'\n",
    "with open(tmp2, 'r') as f:\n",
    "    grid_image_out_dict_stats = json.load(f)\n",
    "\n",
    "print(len(grid_image_out_dict))\n",
    "print(grid_image_out_dict.keys())\n",
    "print(len(grid_image_out_dict_stats))\n",
    "print(grid_image_out_dict_stats.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "grid_image_out_dict = { }\n",
    "grid_image_out_dict_stats = { }\n",
    "num_cpu = 1\n",
    "show=False\n",
    "\n",
    "# input_type = \"mask\"\n",
    "input_type = \"confidence_map\"\n",
    "\n",
    "#####\n",
    "param_optim_folder = '/globalscratch/users/n/s/nsayez/Classification_dataset/2002-2019_2/param_optimization'\n",
    "#####\n",
    "\n",
    "for param_idx, params in tqdm(enumerate(param_grid)):\n",
    "    print(param_idx, params)\n",
    "\n",
    "    image_out_dict = {}\n",
    "    image_out_dict_stats = {}\n",
    "\n",
    "#     {'kernel_bandwidthLat': 0.12, 'kernel_bandwidthLon': 0.45, 'look_distance': 0.1, 'n_iterations': 20}\n",
    "\n",
    "#     cur_huge_dict = deepcopy(grid_huge_dict[str(param_idx)])\n",
    "    fn = f'cur_dict_2002-19_dist{params[\"look_distance\"]}_Lon{params[\"kernel_bandwidthLon\"]}_lat{params[\"kernel_bandwidthLat\"]}_iter{params[\"n_iterations\"]}.json'  \n",
    "    print(fn)\n",
    "    \n",
    "#     raise\n",
    "    \n",
    "    cur_huge_dict_filename = os.path.join(param_optim_folder,fn)\n",
    "    with open(cur_huge_dict_filename,'r') as f:\n",
    "        cur_huge_dict = json.load(f)\n",
    "        \n",
    "    print(len(cur_huge_dict))\n",
    "\n",
    "    with concurrent.futures.ProcessPoolExecutor(max_workers=int(num_cpu)) as executor:\n",
    "        for result_key, result_dict, result_dict_stats in tqdm(executor.map(find_matchings_one_image, \n",
    "                                                            repeat(cur_huge_dict),\n",
    "                                                            list(cur_huge_dict.keys())[:],\n",
    "                                                            repeat(wl_dir),\n",
    "                                                            repeat(masks_dir),\n",
    "                                                            repeat(input_type),\n",
    "                                                            repeat(show)\n",
    "                                                            )):\n",
    "            # print(result_key)\n",
    "#             print(result_dict_stats[\"ms_too_far\"])\n",
    "            image_out_dict[result_key] = result_dict\n",
    "            image_out_dict_stats[result_key] = result_dict_stats\n",
    "            # image_out_dict_stats[result_key] = result_dict['stats']\n",
    "#             break\n",
    "   \n",
    "        \n",
    "        \n",
    "\n",
    "    print('num_images: ', len(list(image_out_dict.keys())))\n",
    "    num_groups = 0\n",
    "    for k,v in image_out_dict.items():\n",
    "        # print(k,v)\n",
    "        if v:\n",
    "            num_groups += len(v['groups'])\n",
    "    print(\"num_groups: \",num_groups)\n",
    "    # print(image_out_dict)\n",
    "\n",
    "    grid_image_out_dict[param_idx] = deepcopy(image_out_dict)\n",
    "    grid_image_out_dict_stats[param_idx] = deepcopy(image_out_dict_stats)\n",
    "\n",
    "#     break           \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "from collections import Counter\n",
    "\n",
    "cur_out_stats = {\n",
    "    # # General info\n",
    "    # 'num_DB_groups': 0,\n",
    "    # 'num_MS_groups': 0,\n",
    "    # 'num_DB_isolated_groups': 0,\n",
    "    # 'num_DB_overlaping_bboxes': 0,\n",
    "    # # MS with DB matching info\n",
    "    # \"num_MSmatchesDB\":0,\n",
    "    # # MS with DB rejection info\n",
    "    # \"num_noMS_but_DB_reject\":0,\n",
    "    # \"num_singleMS_multipleDB_reject\":0,\n",
    "    # \"num_oneDBbbox_multipleMSoverlap_ambiguity_reject\":0,\n",
    "    # # MS with DB no match info\n",
    "    # \"num_noDB_but_MS\":0,\n",
    "            # General info\n",
    "            'num_DB_groups':0,\n",
    "            'num_MS_groups':0,\n",
    "\n",
    "            'matches':0,\n",
    "\n",
    "            # MS with DB matching info\n",
    "            'unmatched_db':0,\n",
    "            'multiDB_singleMS': 0,\n",
    "            'db_too_far':0,\n",
    "\n",
    "\n",
    "            'unmatched_ms':0,\n",
    "            'bad_ms':0,\n",
    "            'ms_too_far':0,\n",
    "\n",
    "            \"ious\":0,\n",
    "            \"distances\":0\n",
    "\n",
    "    }\n",
    "\n",
    "stats_keys = cur_out_stats.keys()\n",
    "\n",
    "should_check =  list(stats_keys) + [\n",
    "                                        'num_rejects_all',\n",
    "                                        'num_optimizable_rejects',\n",
    "                                        'rate_optimizable_rejects',\n",
    "                                    ]\n",
    "\n",
    "gridsearch_dict_per_img = { k : np.zeros((len(param_grid_values['kernel_bandwidthLon']), \n",
    "                                   len(param_grid_values['kernel_bandwidthLat']),\n",
    "                                    len(wl_list)\n",
    "                                   ))\n",
    "                    for k in should_check}\n",
    "\n",
    "should_check_global = list(stats_keys) + [\n",
    "                                            'num_rejects_all',\n",
    "                                            'num_optimizable_rejects',\n",
    "                                            'rate_optimizable_rejects',\n",
    "                                         ]\n",
    "gridsearch_dict_total = { k : np.zeros((len(param_grid_values['kernel_bandwidthLon']), \n",
    "                                   len(param_grid_values['kernel_bandwidthLat'])\n",
    "                                   ))\n",
    "                    for k in should_check_global}\n",
    "\n",
    "# grid_search_reject_distribs = { k : {} for param_idx, params in enumerate(param_grid)}\n",
    "grid_search_reject_distribs = {}\n",
    "\n",
    "\n",
    "for param_idx, params in enumerate(param_grid):\n",
    "    param_idx= str(param_idx)\n",
    "    image_out_dict = grid_image_out_dict[param_idx]\n",
    "    image_out_dict_stats = grid_image_out_dict_stats[param_idx]\n",
    "    \n",
    "    # General info\n",
    "    num_DB_groups_per_image = np.array([item['num_DB_groups'] for k,item in image_out_dict_stats.items()])\n",
    "    num_MS_groups_per_image = np.array([item['num_MS_groups'] for k,item in image_out_dict_stats.items()])\n",
    "\n",
    "    diff_num_groups = num_DB_groups_per_image - num_MS_groups_per_image # should be shown on histogram , closer to 0 is better\n",
    "\n",
    "\n",
    "    # matches\n",
    "    num_MSmatchesDB = np.array([len(np.where(v['matches'] != -1)[0]) for k,v in image_out_dict_stats.items()])\n",
    "    \n",
    "    num_unmatched_db = np.array([len(v['unmatched_db']) for k,v in image_out_dict_stats.items()])\n",
    "    num_multiDB_singleMS = np.array([len(v['multiDB_singleMS']) for k,v in image_out_dict_stats.items()])\n",
    "    num_db_too_far = np.array([len(v['db_too_far']) for k,v in image_out_dict_stats.items()])\n",
    "\n",
    "\n",
    "    num_unmatchedMS = np.array([len(v['unmatched_ms']) for k,v in image_out_dict_stats.items()])\n",
    "    num_badMS = np.array([len(v['bad_ms']) for k,v in image_out_dict_stats.items()])\n",
    "    num_ms_too_far = np.array([len(v['ms_too_far']) for k,v in image_out_dict_stats.items()])\n",
    "    \n",
    "  \n",
    "    num_optimizable_rejects = num_badMS + num_multiDB_singleMS \n",
    "    num_rejects_all = num_optimizable_rejects + num_db_too_far\n",
    "\n",
    "    # get indices of current params\n",
    "    kernel_bandwidthLon_idx = param_grid_values['kernel_bandwidthLon'].index(params['kernel_bandwidthLon'])\n",
    "    kernel_bandwidthLat_idx = param_grid_values['kernel_bandwidthLat'].index(params['kernel_bandwidthLat'])\n",
    "    ############################\n",
    "    # Fill per-image dictinnary \n",
    "    ############################\n",
    "\n",
    "    # number of DB groups\n",
    "    gridsearch_dict_per_img['num_DB_groups'][kernel_bandwidthLon_idx,kernel_bandwidthLat_idx] = num_DB_groups_per_image\n",
    "    # number of MS groups\n",
    "    gridsearch_dict_per_img['num_MS_groups'][kernel_bandwidthLon_idx,kernel_bandwidthLat_idx] = num_MS_groups_per_image\n",
    "\n",
    "    # number of matches\n",
    "    # print(num_MSmatchesDB.shape)\n",
    "    # print(num_MSmatchesDB)\n",
    "    gridsearch_dict_per_img['matches'][kernel_bandwidthLon_idx,kernel_bandwidthLat_idx] = num_MSmatchesDB\n",
    "    # number of unmatched DB groups\n",
    "    gridsearch_dict_per_img['unmatched_db'][kernel_bandwidthLon_idx,kernel_bandwidthLat_idx] = num_unmatched_db\n",
    "    # number of multi DB groups matched to single MS group\n",
    "    gridsearch_dict_per_img['multiDB_singleMS'][kernel_bandwidthLon_idx,kernel_bandwidthLat_idx] = num_multiDB_singleMS\n",
    "    # number of DB groups too far from MS group\n",
    "    gridsearch_dict_per_img['db_too_far'][kernel_bandwidthLon_idx,kernel_bandwidthLat_idx] = num_db_too_far\n",
    "    # number of unmatched MS groups\n",
    "    gridsearch_dict_per_img['unmatched_ms'][kernel_bandwidthLon_idx,kernel_bandwidthLat_idx] = num_unmatchedMS\n",
    "    # number of bad MS groups\n",
    "    gridsearch_dict_per_img['bad_ms'][kernel_bandwidthLon_idx,kernel_bandwidthLat_idx] = num_badMS\n",
    "    # number of MS groups too far from DB group\n",
    "    gridsearch_dict_per_img['ms_too_far'][kernel_bandwidthLon_idx,kernel_bandwidthLat_idx] = num_ms_too_far\n",
    "\n",
    "\n",
    "    gridsearch_dict_per_img['num_optimizable_rejects'][kernel_bandwidthLon_idx,kernel_bandwidthLat_idx] = num_optimizable_rejects\n",
    "    gridsearch_dict_per_img['num_rejects_all'][kernel_bandwidthLon_idx,kernel_bandwidthLat_idx] = num_rejects_all\n",
    "     \n",
    "    ############################\n",
    "    # Fill total dictionnary\n",
    "    ############################\n",
    "    # number of DB groups\n",
    "    gridsearch_dict_total['num_DB_groups'][kernel_bandwidthLon_idx,kernel_bandwidthLat_idx] = np.sum(num_DB_groups_per_image)\n",
    "    # number of MS groups\n",
    "    gridsearch_dict_total['num_MS_groups'][kernel_bandwidthLon_idx,kernel_bandwidthLat_idx] = np.sum(num_MS_groups_per_image)\n",
    "\n",
    "    # number of matches\n",
    "    gridsearch_dict_total['matches'][kernel_bandwidthLon_idx,kernel_bandwidthLat_idx] = np.sum(num_MSmatchesDB)\n",
    "    # number of unmatched DB groups\n",
    "    gridsearch_dict_total['unmatched_db'][kernel_bandwidthLon_idx,kernel_bandwidthLat_idx] = np.sum(num_unmatched_db)\n",
    "    # number of multi DB groups matched to single MS group\n",
    "    gridsearch_dict_total['multiDB_singleMS'][kernel_bandwidthLon_idx,kernel_bandwidthLat_idx] = np.sum(num_multiDB_singleMS)\n",
    "    # number of DB groups too far from MS group\n",
    "    gridsearch_dict_total['db_too_far'][kernel_bandwidthLon_idx,kernel_bandwidthLat_idx] = np.sum(num_db_too_far)\n",
    "    # number of unmatched MS groups\n",
    "    gridsearch_dict_total['unmatched_ms'][kernel_bandwidthLon_idx,kernel_bandwidthLat_idx] = np.sum(num_unmatchedMS)\n",
    "    # number of bad MS groups\n",
    "    gridsearch_dict_total['bad_ms'][kernel_bandwidthLon_idx,kernel_bandwidthLat_idx] = np.sum(num_badMS)\n",
    "    # number of MS groups too far from DB group\n",
    "    gridsearch_dict_total['ms_too_far'][kernel_bandwidthLon_idx,kernel_bandwidthLat_idx] = np.sum(num_ms_too_far)\n",
    "    \n",
    "    \n",
    "\n",
    "    gridsearch_dict_total['num_optimizable_rejects'][kernel_bandwidthLon_idx,kernel_bandwidthLat_idx] = np.sum(num_optimizable_rejects)\n",
    "    gridsearch_dict_total['num_rejects_all'][kernel_bandwidthLon_idx,kernel_bandwidthLat_idx] = np.sum(num_rejects_all)\n",
    "\n",
    "#     break\n",
    "    \n",
    "\n",
    "# gridsearch = { k : pd.DataFrame(v) }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib ipympl\n",
    "# accuracy\n",
    "# all_isolated = gridsearch_dict_total['num_DB_isolated_groups'] \n",
    "# all_overlapping = gridsearch_dict_total['num_DB_overlaping_bboxes'] \n",
    "\n",
    "# print('all_isolated: ', all_isolated[0,0],'all_overlapping: ' ,all_overlapping[0,0])\n",
    "# all_noMS_but_DB = gridsearch_dict_total['num_noMS_but_DB_reject']\n",
    "\n",
    "all_groups = gridsearch_dict_total['num_DB_groups']\n",
    "\n",
    "all_too_far = gridsearch_dict_total['db_too_far']\n",
    "\n",
    "to_count = all_groups - all_too_far\n",
    "\n",
    "all_optimizable_rejects = gridsearch_dict_total['num_optimizable_rejects']\n",
    "\n",
    "all_matches = gridsearch_dict_total['matches']\n",
    "\n",
    "all_unmatched_ms = gridsearch_dict_total['unmatched_ms']\n",
    "all_ms_too_far = gridsearch_dict_total['ms_too_far']\n",
    "\n",
    "\n",
    "\n",
    "accuracy = all_matches / (all_matches + all_optimizable_rejects )\n",
    "\n",
    "accuracy_df = pd.DataFrame(accuracy)\n",
    "accuracy_df.columns = param_grid_values['kernel_bandwidthLat']\n",
    "accuracy_df.index = param_grid_values['kernel_bandwidthLon']\n",
    "accuracy_df = accuracy_df.transpose()\n",
    "\n",
    "\n",
    "fig_, ax_ = plt.subplots(1,1, figsize=(8*1,5*1))\n",
    "ax_ = sns.heatmap(accuracy_df, ax=ax_, annot=True)\n",
    "# ax_ = sns.heatmap(accuracy_df, ax=ax_, vmin=0, annot=True)\n",
    "ax_.set_title('Accuracy')\n",
    "ax_.set_ylabel('kernel_bandwidthLat')\n",
    "ax_.set_xlabel('kernel_bandwidthLon')\n",
    "\n",
    "fig_.tight_layout()\n",
    "\n",
    "\n",
    "############\n",
    "print(all_unmatched_ms.T)\n",
    "print(all_ms_too_far.T)\n",
    "print((all_unmatched_ms - all_ms_too_far).T)\n",
    "\n",
    "accuracy = all_matches / (all_matches + all_optimizable_rejects + (all_unmatched_ms - all_ms_too_far)  )\n",
    "accuracy_df = pd.DataFrame(accuracy)\n",
    "accuracy_df.columns = param_grid_values['kernel_bandwidthLat']\n",
    "accuracy_df.index = param_grid_values['kernel_bandwidthLon']\n",
    "accuracy_df = accuracy_df.transpose()\n",
    "\n",
    "fig2_, ax2_ = plt.subplots(1,1, figsize=(8*1,5*1))\n",
    "ax2_ = sns.heatmap(accuracy_df, ax=ax2_, annot=True)\n",
    "# ax2_ = sns.heatmap(accuracy_df, ax=ax_, vmin=0, annot=True)\n",
    "ax2_.set_title('Accuracy')\n",
    "ax2_.set_ylabel('kernel_bandwidthLat')\n",
    "ax2_.set_xlabel('kernel_bandwidthLon')\n",
    "\n",
    "fig2_.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ancienne méthode de comptage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "grid_image_out_dict = { }\n",
    "grid_image_out_dict_stats = { }\n",
    "\n",
    "for param_idx, params in enumerate(param_grid):\n",
    "    print(param_idx, params)\n",
    "\n",
    "    image_out_dict = {}\n",
    "    image_out_dict_stats = {}\n",
    "\n",
    "    cur_huge_dict = deepcopy(grid_huge_dict[param_idx])\n",
    "\n",
    "    for basename in tqdm(list(cur_huge_dict.keys())[:]):\n",
    "        cur_image_dict = cur_huge_dict[basename]\n",
    "        \n",
    "        angle = cur_image_dict[\"SOLAR_P0\"]\n",
    "        deltashapeX = cur_image_dict[\"deltashapeX\"]\n",
    "        deltashapeY = cur_image_dict[\"deltashapeY\"]\n",
    "        \n",
    "        drawing_radius_px = huge_db_dict[basename][\"dr_radius_px\"]\n",
    "        \n",
    "        group_list = cur_image_dict['db']\n",
    "        \n",
    "        ms_dict = cur_image_dict['meanshift']\n",
    "        \n",
    "        centroids = np.array(ms_dict[\"centroids\"])\n",
    "        centroids_px = np.array(ms_dict[\"centroids_px\"])\n",
    "        \n",
    "        db_classes = [{\"Zurich\":item['Zurich'], \"McIntosh\":item['McIntosh'] } for item in group_list]\n",
    "        db_bboxes = [np.array(item['bbox_wl']) for item in group_list]\n",
    "        db_centers_px = np.array([[(b[2]+b[0])/2,(b[3]+b[1])/2] for b in db_bboxes])\n",
    "           \n",
    "        # check that current bbox is does not overlap any\n",
    "        isolated_bboxes_bool = np.array(c_utils.get_intersecting_db_bboxes(db_bboxes)) == 0\n",
    "        isolated_bboxes_indices = np.where(isolated_bboxes_bool == True)[0]\n",
    "    #     print(\"isolated_bboxes_bool\",isolated_bboxes_bool)\n",
    "    #     print(isolated_bboxes_indices)\n",
    "        \n",
    "        cur_rejected_class_distibutions = { \n",
    "                'noMS_but_DB': {},\n",
    "                'singleMS_multipleDB': {}, \n",
    "                'num_oneDBbbox_multipleMSoverlap_ambiguity':{},\n",
    "                # 'noDB_but_MS': {},\n",
    "            }\n",
    "        cur_out_stats = {\n",
    "            # General info\n",
    "            'num_DB_groups':len(db_bboxes),\n",
    "            'num_MS_groups':len(centroids_px),\n",
    "            'num_DB_isolated_groups':len(isolated_bboxes_indices),\n",
    "            'num_DB_overlaping_bboxes':len(db_bboxes) - len(isolated_bboxes_indices),\n",
    "            # MS with DB matching info\n",
    "            \"num_MSmatchesDB\":0,\n",
    "            # MS with DB rejection info\n",
    "            \"num_noMS_but_DB_reject\":0,\n",
    "            \"num_singleMS_multipleDB_reject\":0,\n",
    "            \"num_oneDBbbox_multipleMSoverlap_ambiguity_reject\":0,\n",
    "            # MS with DB no match info\n",
    "            \"num_noDB_but_MS\":0,\n",
    "            }\n",
    "        cur_out_groups = []\n",
    "        for i, (db_bbox, db_center, db_class) in enumerate(\n",
    "                                                zip([db_bboxes[a] for a in isolated_bboxes_indices.tolist() ],\n",
    "                                                    [db_centers_px[a] for a in isolated_bboxes_indices.tolist()],\n",
    "                                                    [db_classes[a] for a in isolated_bboxes_indices.tolist()],\n",
    "                                                )):\n",
    "            \n",
    "            \n",
    "            ms_centroids, ms_members = centroids_px, ms_dict['groups_px']\n",
    "            \n",
    "            intersect = c_utils.contains_ms_groups(db_bbox, db_center, ms_centroids, ms_members)\n",
    "            \n",
    "            if sum(intersect) == 0: # Il n'y a eu aucune détection dans cette zone\n",
    "                cur_out_stats['num_noMS_but_DB_reject'] += 1\n",
    "                cause = 'noMS_but_DB'\n",
    "                add_rejected_to_distributions(cur_rejected_class_distibutions[cause], db_class[\"McIntosh\"][0])\n",
    "                pass\n",
    "            elif sum(intersect) == 1: # il n'y a de l'overlap qu'avec un seul groupe meanshift            \n",
    "    #             print('hit')\n",
    "                idx = intersect.index(True)\n",
    "    #             print(idx)\n",
    "                # vérifier que le groupe meanshift n'intersecte aucune autre bbox\n",
    "                num_intersections = np.sum(c_utils.count_group_intersections(ms_members[idx], db_bboxes))\n",
    "                if num_intersections > 1:\n",
    "                    cur_out_stats['num_singleMS_multipleDB_reject'] += 1\n",
    "                    cause = 'singleMS_multipleDB'\n",
    "                    add_rejected_to_distributions(cur_rejected_class_distibutions[cause], db_class[\"McIntosh\"][0])\n",
    "                    continue\n",
    "                \n",
    "                Rmm = huge_db_dict[basename]['dr_radius_mm']\n",
    "                R_pixel = huge_db_dict[basename]['dr_radius_px']\n",
    "                sun_center = huge_db_dict[basename]['dr_center_px']\n",
    "                dr_pixpos = np.array([group_list[i]['posx'], group_list[i]['posy']])\n",
    "                \n",
    "                angular_excentricity =  c_utils.get_angle2(dr_pixpos, R_pixel, sun_center)\n",
    "                \n",
    "                cur_group_dict={\n",
    "                                \"centroid_px\": centroids_px[idx],\n",
    "                                \"centroid_Lat\": centroids[idx][0],\n",
    "                                \"centroid_Lon\": centroids[idx][1],\n",
    "                                \"angular_excentricity_rad\": angular_excentricity,\n",
    "                                \"angular_excentricity_deg\": np.rad2deg(angular_excentricity),\n",
    "                                \"Zurich\":   db_class[\"Zurich\"],\n",
    "                                \"McIntosh\": db_class[\"McIntosh\"],\n",
    "                                \"members\": ms_members[idx],\n",
    "                                \"members_mean_px\": np.mean(ms_members[idx], axis=0),\n",
    "                            }\n",
    "                \n",
    "                \n",
    "                cur_out_groups.append(cur_group_dict)\n",
    "                cur_out_stats['num_MSmatchesDB'] += 1\n",
    "\n",
    "            else: # db_bbox intersecte plusieurs groupes meanshift\n",
    "                cur_out_stats['num_oneDBbbox_multipleMSoverlap_ambiguity_reject'] += 1\n",
    "                cause = 'num_oneDBbbox_multipleMSoverlap_ambiguity'\n",
    "                add_rejected_to_distributions(cur_rejected_class_distibutions[cause], db_class[\"McIntosh\"][0])\n",
    "                pass\n",
    "                \n",
    "        if len(cur_out_groups) > 0:\n",
    "            image_out_dict[basename] = { \"angle\": angle,\n",
    "                                        \"deltashapeX\":deltashapeX,\n",
    "                                        \"deltashapeY\":deltashapeY,\n",
    "                                        \"groups\": cur_out_groups,\n",
    "                                    }\n",
    "\n",
    "        # count the number of MS groups that do not have any overlap with the DB\n",
    "        num_intersections_per_group = [np.sum(c_utils.count_group_intersections(ms_members[idx], db_bboxes)) for idx in range(len(ms_members))]\n",
    "        num_MS_without_DB_overlap = len(np.where(np.array(num_intersections_per_group) == 0)[0])\n",
    "        cur_out_stats['num_noDB_but_MS'] = num_MS_without_DB_overlap\n",
    "        \n",
    "        # print(cur_rejected_class_distibutions)\n",
    "        cur_out_stats['rejected_class_distributions'] = deepcopy(cur_rejected_class_distibutions)\n",
    "        # print(cur_out_stats)\n",
    "        image_out_dict_stats[basename] = deepcopy(cur_out_stats)\n",
    "\n",
    "        \n",
    "\n",
    "    print('num_images: ', len(list(image_out_dict.keys())))\n",
    "    num_groups = 0\n",
    "    for k,v in image_out_dict.items():\n",
    "        num_groups += len(v['groups'])\n",
    "    print(\"num_groups: \",num_groups)\n",
    "    # print(image_out_dict)\n",
    "\n",
    "    grid_image_out_dict[param_idx] = deepcopy(image_out_dict)\n",
    "    grid_image_out_dict_stats[param_idx] = deepcopy(image_out_dict_stats)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "from collections import Counter\n",
    "\n",
    "cur_out_stats = {\n",
    "    # General info\n",
    "    'num_DB_groups': 0,\n",
    "    'num_MS_groups': 0,\n",
    "    'num_DB_isolated_groups': 0,\n",
    "    'num_DB_overlaping_bboxes': 0,\n",
    "    # MS with DB matching info\n",
    "    \"num_MSmatchesDB\":0,\n",
    "    # MS with DB rejection info\n",
    "    \"num_noMS_but_DB_reject\":0,\n",
    "    \"num_singleMS_multipleDB_reject\":0,\n",
    "    \"num_oneDBbbox_multipleMSoverlap_ambiguity_reject\":0,\n",
    "    # MS with DB no match info\n",
    "    \"num_noDB_but_MS\":0,\n",
    "    }\n",
    "\n",
    "stats_keys = cur_out_stats.keys()\n",
    "\n",
    "should_check =  list(stats_keys) + [\n",
    "                                        'num_rejects_all',\n",
    "                                        'num_optimizable_rejects',\n",
    "                                        'rate_optimizable_rejects',\n",
    "                                    ]\n",
    "\n",
    "gridsearch_dict_per_img = { k : np.zeros((len(param_grid_values['kernel_bandwidthLon']), \n",
    "                                   len(param_grid_values['kernel_bandwidthLat']),\n",
    "                                    len(wl_list)\n",
    "                                   ))\n",
    "                    for k in should_check}\n",
    "\n",
    "should_check_global = list(stats_keys) + [\n",
    "                                            'num_rejects_all',\n",
    "                                            'num_optimizable_rejects',\n",
    "                                            'rate_optimizable_rejects',\n",
    "                                         ]\n",
    "gridsearch_dict_total = { k : np.zeros((len(param_grid_values['kernel_bandwidthLon']), \n",
    "                                   len(param_grid_values['kernel_bandwidthLat'])\n",
    "                                   ))\n",
    "                    for k in should_check_global}\n",
    "\n",
    "# grid_search_reject_distribs = { k : {} for param_idx, params in enumerate(param_grid)}\n",
    "grid_search_reject_distribs = {}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for param_idx, params in enumerate(param_grid):\n",
    "    image_out_dict = grid_image_out_dict[param_idx]\n",
    "    image_out_dict_stats = grid_image_out_dict_stats[param_idx]\n",
    "    \n",
    "    # General info\n",
    "    num_DB_groups_per_image = np.array([item['num_DB_groups'] for k,item in image_out_dict_stats.items()])\n",
    "    num_MS_groups_per_image = np.array([item['num_MS_groups'] for k,item in image_out_dict_stats.items()])\n",
    "\n",
    "    diff_num_groups = num_DB_groups_per_image - num_MS_groups_per_image # should be shown on histogram , closer to 0 is better\n",
    "\n",
    "    # number of isolated DB groups per image and mean number in dataset\n",
    "    num_isolated_DB_groups = np.array([v['num_DB_isolated_groups'] for k,v in image_out_dict_stats.items()])\n",
    "    # number of DB groups with overlap with other DB groups\n",
    "    num_DB_overlaping_bboxes = np.array([v['num_DB_overlaping_bboxes'] for k,v in image_out_dict_stats.items()])\n",
    "\n",
    "    # matches\n",
    "    num_MSmatchesDB = np.array([v['num_MSmatchesDB'] for k,v in image_out_dict_stats.items()])\n",
    "    \n",
    "    # rejects\n",
    "    # 1) numbers of rejects due to no MS group overlapping DB group (# cannot do anything to this, examples are discarded)\n",
    "    num_noMS_but_DB_reject = np.array([v['num_noMS_but_DB_reject'] for k,v in image_out_dict_stats.items()])\n",
    "    # 2) numbers of rejects due to single MS group overlapping multiple DB groups (# minimize this)\n",
    "    num_singleMS_multipleDB_reject = np.array([v['num_singleMS_multipleDB_reject'] for k,v in image_out_dict_stats.items()])\n",
    "    # 3) numbers of rejects due to single DB group overlapping multiple MS groups (# minimize this)\n",
    "    num_oneDBbbox_multipleMSoverlap_ambiguity_reject = np.array([v['num_oneDBbbox_multipleMSoverlap_ambiguity_reject'] for k,v in image_out_dict_stats.items()])\n",
    "\n",
    "    num_optimizable_rejects = num_singleMS_multipleDB_reject + num_oneDBbbox_multipleMSoverlap_ambiguity_reject\n",
    "    num_rejects_all = num_optimizable_rejects + num_noMS_but_DB_reject\n",
    "\n",
    "    # rate of optimizable rejects\n",
    "    rate_optimizable_rejects = num_optimizable_rejects / num_rejects_all\n",
    "\n",
    "    # print(num_optimizable_rejects)\n",
    "\n",
    "\n",
    "    # Aggregate reject distributions of images over the dataset\n",
    "    dict1 = [v['rejected_class_distributions']['noMS_but_DB'] for k,v in image_out_dict_stats.items()]\n",
    "    summed_dict1 = sum(map(collections.Counter, dict1),Counter())\n",
    "    dict2 = [v['rejected_class_distributions']['singleMS_multipleDB'] for k,v in image_out_dict_stats.items()]\n",
    "    summed_dict2 = sum(map(collections.Counter, dict2),Counter())\n",
    "    dict3 = [v['rejected_class_distributions']['num_oneDBbbox_multipleMSoverlap_ambiguity'] for k,v in image_out_dict_stats.items()]\n",
    "    summed_dict3 = sum(map(collections.Counter, dict3),Counter())\n",
    "    dataset_rejected_class_distibutions = { \n",
    "        'noMS_but_DB': summed_dict1,\n",
    "        'singleMS_multipleDB': summed_dict2, \n",
    "        'num_oneDBbbox_multipleMSoverlap_ambiguity': summed_dict3,\n",
    "    }\n",
    "\n",
    "    # print(dict1)\n",
    "    # print(summed_dict1)\n",
    "    \n",
    "\n",
    "\n",
    "    # get indices of current params\n",
    "    kernel_bandwidthLon_idx = param_grid_values['kernel_bandwidthLon'].index(params['kernel_bandwidthLon'])\n",
    "    kernel_bandwidthLat_idx = param_grid_values['kernel_bandwidthLat'].index(params['kernel_bandwidthLat'])\n",
    "    ############################\n",
    "    # Fill per-image dictinnary \n",
    "    ############################\n",
    "\n",
    "    # number of DB groups\n",
    "    gridsearch_dict_per_img['num_DB_groups'][kernel_bandwidthLon_idx,kernel_bandwidthLat_idx] = num_DB_groups_per_image\n",
    "    # number of MS groups\n",
    "    gridsearch_dict_per_img['num_MS_groups'][kernel_bandwidthLon_idx,kernel_bandwidthLat_idx] = num_MS_groups_per_image\n",
    "\n",
    "    # number of isolated DB groups\n",
    "    gridsearch_dict_per_img['num_DB_isolated_groups'][kernel_bandwidthLon_idx,kernel_bandwidthLat_idx] = num_isolated_DB_groups\n",
    "    gridsearch_dict_per_img['num_DB_overlaping_bboxes'][kernel_bandwidthLon_idx,kernel_bandwidthLat_idx] = num_DB_overlaping_bboxes\n",
    "\n",
    "    # matches\n",
    "    gridsearch_dict_per_img['num_MSmatchesDB'][kernel_bandwidthLon_idx,kernel_bandwidthLat_idx] = num_MSmatchesDB\n",
    "    \n",
    "    # not optimizable rejects\n",
    "    gridsearch_dict_per_img['num_noMS_but_DB_reject'][kernel_bandwidthLon_idx,kernel_bandwidthLat_idx] = num_noMS_but_DB_reject\n",
    "    \n",
    "    # optimizable rejects\n",
    "    gridsearch_dict_per_img['num_singleMS_multipleDB_reject'][kernel_bandwidthLon_idx,kernel_bandwidthLat_idx] = num_singleMS_multipleDB_reject\n",
    "    gridsearch_dict_per_img['num_oneDBbbox_multipleMSoverlap_ambiguity_reject'][kernel_bandwidthLon_idx,kernel_bandwidthLat_idx] = num_oneDBbbox_multipleMSoverlap_ambiguity_reject\n",
    "    gridsearch_dict_per_img['num_optimizable_rejects'][kernel_bandwidthLon_idx,kernel_bandwidthLat_idx] = num_optimizable_rejects\n",
    "    \n",
    "    gridsearch_dict_per_img['num_rejects_all'][kernel_bandwidthLon_idx,kernel_bandwidthLat_idx] = num_rejects_all\n",
    "    \n",
    "    # rate of optimizable rejects\n",
    "    gridsearch_dict_per_img['rate_optimizable_rejects'][kernel_bandwidthLon_idx,kernel_bandwidthLat_idx] = rate_optimizable_rejects\n",
    "    \n",
    "    ############################\n",
    "    # Fill total dictionnary\n",
    "    ############################\n",
    "    # number of DB groups\n",
    "    gridsearch_dict_total['num_DB_groups'][kernel_bandwidthLon_idx,kernel_bandwidthLat_idx] = np.sum(num_DB_groups_per_image)\n",
    "    # number of MS groups\n",
    "    gridsearch_dict_total['num_MS_groups'][kernel_bandwidthLon_idx,kernel_bandwidthLat_idx] = np.sum(num_MS_groups_per_image)\n",
    "\n",
    "    # number of isolated DB groups\n",
    "    gridsearch_dict_total['num_DB_isolated_groups'][kernel_bandwidthLon_idx,kernel_bandwidthLat_idx] = np.sum(num_isolated_DB_groups)\n",
    "    gridsearch_dict_total['num_DB_overlaping_bboxes'][kernel_bandwidthLon_idx,kernel_bandwidthLat_idx] = np.sum(num_DB_overlaping_bboxes)\n",
    "\n",
    "    # matches\n",
    "    gridsearch_dict_total['num_MSmatchesDB'][kernel_bandwidthLon_idx,kernel_bandwidthLat_idx] = np.sum(num_MSmatchesDB)\n",
    "\n",
    "    # not optimizable rejects\n",
    "    gridsearch_dict_total['num_noMS_but_DB_reject'][kernel_bandwidthLon_idx,kernel_bandwidthLat_idx] = np.sum(num_noMS_but_DB_reject)\n",
    "\n",
    "    # optimizable rejects\n",
    "    gridsearch_dict_total['num_singleMS_multipleDB_reject'][kernel_bandwidthLon_idx,kernel_bandwidthLat_idx] = np.sum(num_singleMS_multipleDB_reject)\n",
    "    gridsearch_dict_total['num_oneDBbbox_multipleMSoverlap_ambiguity_reject'][kernel_bandwidthLon_idx,kernel_bandwidthLat_idx] = np.sum(num_oneDBbbox_multipleMSoverlap_ambiguity_reject)\n",
    "    gridsearch_dict_total['num_optimizable_rejects'][kernel_bandwidthLon_idx,kernel_bandwidthLat_idx] = np.sum(num_optimizable_rejects)\n",
    "\n",
    "    gridsearch_dict_total['num_rejects_all'][kernel_bandwidthLon_idx,kernel_bandwidthLat_idx] = np.sum(num_rejects_all)\n",
    "\n",
    "\n",
    "    rate_optimizable_rejects_all = np.sum(num_optimizable_rejects) / np.sum(num_rejects_all)\n",
    "    gridsearch_dict_total['rate_optimizable_rejects'][kernel_bandwidthLon_idx,kernel_bandwidthLat_idx] = rate_optimizable_rejects_all\n",
    "\n",
    "    grid_search_reject_distribs[param_idx] = deepcopy(dataset_rejected_class_distibutions) \n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "gridsearch = { k : pd.DataFrame(v) }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy\n",
    "all_isolated = gridsearch_dict_total['num_DB_isolated_groups'] \n",
    "all_overlapping = gridsearch_dict_total['num_DB_overlaping_bboxes'] \n",
    "\n",
    "print('all_isolated: ', all_isolated[0,0],'all_overlapping: ' ,all_overlapping[0,0])\n",
    "\n",
    "all_noMS_but_DB = gridsearch_dict_total['num_noMS_but_DB_reject']\n",
    "\n",
    "optimizable = all_isolated - all_noMS_but_DB\n",
    "\n",
    "print('all_optimizable', optimizable[0,0])\n",
    "\n",
    "\n",
    "matches = gridsearch_dict_total['num_MSmatchesDB']\n",
    "\n",
    "accuracy = matches / optimizable\n",
    "\n",
    "accuracy_df = pd.DataFrame(accuracy)\n",
    "accuracy_df.columns = param_grid_values['kernel_bandwidthLat']\n",
    "accuracy_df.index = param_grid_values['kernel_bandwidthLon']\n",
    "\n",
    "\n",
    "fig_, ax_ = plt.subplots(1,1, figsize=(3.5*1,3*1))\n",
    "ax_ = sns.heatmap(accuracy_df, ax=ax_, vmin=0, annot=True)\n",
    "ax_.set_title('Accuracy')\n",
    "ax_.set_xlabel('kernel_bandwidthLat')\n",
    "ax_.set_ylabel('kernel_bandwidthLon')\n",
    "\n",
    "fig_.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_rows_2 = 1\n",
    "num_cols_2 = 2\n",
    "fig2, ax2 = plt.subplots(num_rows_2,num_cols_2, figsize=(3.5*num_cols_2,3*num_rows_2))\n",
    "\n",
    "\n",
    "df2 = pd.DataFrame(np.sum(gridsearch_dict_per_img['num_optimizable_rejects'],axis=-1))\n",
    "df2.columns = param_grid_values['kernel_bandwidthLat']\n",
    "df2.index = param_grid_values['kernel_bandwidthLon']\n",
    "ax_ = sns.heatmap(df2, ax=ax2[0], vmin=0, annot=True)\n",
    "ax_.set_title('Number of optimizable rejects')\n",
    "ax_.set_xlabel('kernel_bandwidthLat')\n",
    "ax_.set_ylabel('kernel_bandwidthLon')\n",
    "\n",
    "df2 = pd.DataFrame(gridsearch_dict_total['rate_optimizable_rejects'])\n",
    "df2.columns = param_grid_values['kernel_bandwidthLat']\n",
    "df2.index = param_grid_values['kernel_bandwidthLon']\n",
    "ax_ = sns.heatmap(df2, ax=ax2[1], vmin=0, annot=True)\n",
    "ax_.set_title('Rate of optimizable rejects')\n",
    "ax_.set_xlabel('kernel_bandwidthLat')\n",
    "ax_.set_ylabel('kernel_bandwidthLon')\n",
    "\n",
    "fig2.tight_layout()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# dataset_rejected_class_distibutions = { \n",
    "#     'noMS_but_DB': summed_dict1,\n",
    "#     'singleMS_multipleDB': summed_dict2, \n",
    "#     'num_oneDBbbox_multipleMSoverlap_ambiguity': summed_dict3,\n",
    "# }\n",
    "\n",
    "# param_grid_values = {\n",
    "#     'look_distance' : [0.1],\n",
    "#     'kernel_bandwidthLon' : [ 0.05 , 0.1, 0.15, .2,.21,.22,.23,.24,.25, .3,.35,.45],\n",
    "#     'kernel_bandwidthLat' : [.08,],\n",
    "#     # 'kernel_bandwidthLon' : [ 0.05 , 0.1, 0.15, .2, .25, .3,.35,.45],\n",
    "#     # 'kernel_bandwidthLat' : [ 0.04 , 0.06, .08, .1, .12],\n",
    "#     # 'kernel_bandwidthLat' : [ 0.05 , 0.06, 0.07, .08, .09, .1, .11, .12],\n",
    "#     'n_iterations' : [20],\n",
    "# }\n",
    "\n",
    "\n",
    "num_rows_distrib = len(param_grid_values['kernel_bandwidthLon'])\n",
    "num_cols_distrib = len(param_grid_values['kernel_bandwidthLat'])\n",
    "fig_distrib, ax_distrib = plt.subplots(num_rows_distrib,num_cols_distrib, figsize=(5*num_cols_distrib,3*num_rows_distrib))\n",
    "\n",
    "# for i in range(num_rows_distrib):\n",
    "#     for j in range(num_cols_distrib):\n",
    "#         cur_ax = ax_distrib[i,j]\n",
    "\n",
    "for param_idx, params in enumerate(param_grid):\n",
    "#     print(params)\n",
    "    cur_lat_idx = param_grid_values['kernel_bandwidthLat'].index(params['kernel_bandwidthLat'])\n",
    "    cur_lon_idx = param_grid_values['kernel_bandwidthLon'].index(params['kernel_bandwidthLon'])\n",
    "\n",
    "    data = grid_search_reject_distribs[param_idx]\n",
    "    data_df = pd.DataFrame(data)\n",
    "\n",
    "    # reorder rows alphabetically\n",
    "    data_df = data_df.reindex(sorted(data_df.columns), axis=1)\n",
    "    data_df.sort_index( inplace=True)\n",
    "    data_df = data_df.fillna(0)\n",
    "    # data_df = data_df.T\n",
    "\n",
    "\n",
    "#     display(data_df)\n",
    "\n",
    "    cur_ax = ax_distrib[cur_lon_idx,cur_lat_idx]\n",
    "#     cur_ax = ax_distrib[cur_lon_idx]\n",
    "\n",
    "    # ax_ = data_df['noMS_but_DB'].plot(kind='bar' , ax=cur_ax, stacked=False, alpha=0.5, color='k' )\n",
    "    # ax_ = data_df['singleMS_multipleDB'].plot(kind='bar' , ax=cur_ax, stacked=False, alpha=0.5 , color='r')\n",
    "    # ax_ = data_df['num_oneDBbbox_multipleMSoverlap_ambiguity'].plot(kind='bar' , ax=cur_ax, stacked=False, alpha=0.5 , color='b')\n",
    "\n",
    "\n",
    "    # ax_ = data_df.plot(kind='bar' , ax=cur_ax, stacked=True, alpha=0.5 )\n",
    "#     deta_df2 = \n",
    "    ax_ = data_df.plot(kind='bar' , ax=cur_ax, alpha=0.5, color=['k','r','b'],label=['O','U','I'] )\n",
    "\n",
    "    ax_.set_title(f'Rejected classes distribution \\n Lon = {params[\"kernel_bandwidthLon\"]}, Lat = {params[\"kernel_bandwidthLat\"]}')\n",
    "    # ax_.set_xlabel(f'kernel_bandwidthLat = {params[\"kernel_bandwidthLat\"]}')\n",
    "    # ax_.set_ylabel(f'kernel_bandwidthLon = {params[\"kernel_bandwidthLon\"]}')\n",
    "\n",
    "    # break\n",
    "\n",
    "    \n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "fig_distrib.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def on_value_change(change):\n",
    "   \n",
    "\n",
    "    # print(ax)\n",
    "    # print(len(ax.collections))\n",
    "    for i in range(num_cols):\n",
    "        if len(ax[i].collections) > 0:\n",
    "            cb = ax[i].collections[-1].colorbar\n",
    "            if cb is not None:\n",
    "                cb.remove()\n",
    "            ax[i].clear()\n",
    "\n",
    "    df = pd.DataFrame(gridsearch_dict_per_img['num_optimizable_rejects'][:, :, a_slider.value])\n",
    "    df.columns = param_grid_values['kernel_bandwidthLat']\n",
    "    df.index = param_grid_values['kernel_bandwidthLon']\n",
    "    # ax0 = sns.heatmap(df, ax=ax, cbar=False,)\n",
    "    ax0 = sns.heatmap(df, ax=ax[0], vmin=0, annot=True)\n",
    "    ax0.set_title('Number of optimizable rejects')\n",
    "    ax0.set_xlabel('kernel_bandwidthLat')\n",
    "    ax0.set_ylabel('kernel_bandwidthLon')\n",
    "\n",
    "    df = pd.DataFrame(gridsearch_dict_per_img['num_singleMS_multipleDB_reject'][:, :, a_slider.value])\n",
    "    df.columns = param_grid_values['kernel_bandwidthLat']\n",
    "    df.index = param_grid_values['kernel_bandwidthLon']\n",
    "    # ax1 = sns.heatmap(df, ax=ax, cbar=False,)\n",
    "    ax1 = sns.heatmap(df, ax=ax[1], vmin=0, annot=True)\n",
    "    ax1.set_title('Only MS candidate covers \\n2+ DB bboxes')\n",
    "    ax1.set_xlabel('kernel_bandwidthLat')\n",
    "    ax1.set_ylabel('kernel_bandwidthLon')\n",
    "    \n",
    "    df = pd.DataFrame(gridsearch_dict_per_img['num_oneDBbbox_multipleMSoverlap_ambiguity_reject'][:, :, a_slider.value])\n",
    "    df.columns = param_grid_values['kernel_bandwidthLat']\n",
    "    df.index = param_grid_values['kernel_bandwidthLon']\n",
    "    # ax0 = sns.heatmap(df, ax=ax, cbar=False,)\n",
    "    ax2 = sns.heatmap(df, ax=ax[2], vmin=0, annot=True)\n",
    "    ax2.set_title('DB group matches \\nmultiple MS groups')\n",
    "    ax2.set_xlabel('kernel_bandwidthLat')\n",
    "    ax2.set_ylabel('kernel_bandwidthLon')\n",
    "\n",
    "    fig.tight_layout()\n",
    "\n",
    "    # pass\n",
    "\n",
    "a_slider = widgets.IntSlider(min=0, max=len(wl_list)-1, step=1, value=0)\n",
    "a_slider.observe(on_value_change, names='value')\n",
    "\n",
    "plt.ioff()\n",
    "num_rows = 1\n",
    "num_cols = 3\n",
    "fig, ax = plt.subplots(num_rows,num_cols, figsize=(3*num_cols,3*num_rows))\n",
    "on_value_change(None)\n",
    "plt.ion()\n",
    "\n",
    "display(widgets.VBox([a_slider, fig.canvas]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "look_distance = .05  # How far to look for neighbours.\n",
    "kernel_bandwidthLon = .25  # Longitude Kernel parameter.\n",
    "kernel_bandwidthLat = .08  # Latitude Kernel parameter.\n",
    "n_iterations = 1000 # Number of iterations\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create set of 5 ints\n",
    "def create_set_of_5_ints():\n",
    "    return set(range(300,525,25))\n",
    "\n",
    "def all_pairs(s):\n",
    "    '''Returns all pairs of elements in the set s such that (a, b) == (b, a), \n",
    "    and (a, a) is not included in the result.'''\n",
    "    return set((min(x, y), max(x, y)) for x in s for y in s if (x != y) and np.abs(x-y)>25)\n",
    "\n",
    "my_set = create_set_of_5_ints()\n",
    "print(my_set)\n",
    "print(all_pairs(my_set))\n",
    "print(len(all_pairs(my_set)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test overlapping bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "grid_image_out_dict_overlapping = { }\n",
    "grid_image_out_dict_stats_overlapping = { }\n",
    "\n",
    "for param_idx, params in enumerate(param_grid):\n",
    "    print(param_idx, params)\n",
    "\n",
    "    image_out_dict = {}\n",
    "    image_out_dict_stats = {}\n",
    "\n",
    "    cur_huge_dict = deepcopy(grid_huge_dict[param_idx])\n",
    "\n",
    "    for basename in tqdm(list(cur_huge_dict.keys())[:]):\n",
    "        cur_image_dict = cur_huge_dict[basename]\n",
    "        \n",
    "        angle = cur_image_dict[\"SOLAR_P0\"]\n",
    "        deltashapeX = cur_image_dict[\"deltashapeX\"]\n",
    "        deltashapeY = cur_image_dict[\"deltashapeY\"]\n",
    "        \n",
    "        drawing_radius_px = huge_db_dict[basename][\"dr_radius_px\"]\n",
    "        \n",
    "        group_list = cur_image_dict['db']\n",
    "        \n",
    "        ms_dict = cur_image_dict['meanshift']\n",
    "        \n",
    "        centroids = np.array(ms_dict[\"centroids\"])\n",
    "        centroids_px = np.array(ms_dict[\"centroids_px\"])\n",
    "        \n",
    "        db_classes = [{\"Zurich\":item['Zurich'], \"McIntosh\":item['McIntosh'] } for item in group_list]\n",
    "        db_bboxes = [np.array(item['bbox_wl']) for item in group_list]\n",
    "        db_centers_px = np.array([[(b[2]+b[0])/2,(b[3]+b[1])/2] for b in db_bboxes])\n",
    "           \n",
    "        # check that current bbox is does not overlap any\n",
    "        isolated_bboxes_bool = np.array(c_utils.get_intersecting_db_bboxes(db_bboxes)) == 0\n",
    "        overlapping_bboxes_indices = np.where(isolated_bboxes_bool == False)[0]\n",
    "\n",
    "        \n",
    "        cur_rejected_class_distibutions = { \n",
    "                'noMS_but_DB': {},\n",
    "                'singleMS_multipleDB': {}, \n",
    "                'num_oneDBbbox_multipleMSoverlap_ambiguity':{},\n",
    "                # 'noDB_but_MS': {},\n",
    "            }\n",
    "        cur_out_stats = {\n",
    "            # General info\n",
    "            'num_DB_groups':len(db_bboxes),\n",
    "            'num_MS_groups':len(centroids_px),\n",
    "            'num_DB_isolated_groups':len(db_bboxes) - len(overlapping_bboxes_indices),\n",
    "            'num_DB_overlaping_bboxes':len(overlapping_bboxes_indices),\n",
    "            # MS with DB matching info\n",
    "            \"num_MSmatchesDB\":0,\n",
    "            # MS with DB rejection info\n",
    "            \"num_noMS_but_DB_reject\":0,\n",
    "            \"num_singleMS_multipleDB_reject\":0,\n",
    "            \"num_oneDBbbox_multipleMSoverlap_ambiguity_reject\":0,\n",
    "            # MS with DB no match info\n",
    "            \"num_noDB_but_MS\":0,\n",
    "            }\n",
    "        cur_out_groups = []\n",
    "        for i, (db_bbox, db_center, db_class) in enumerate(\n",
    "                                                zip([db_bboxes[a] for a in overlapping_bboxes_indices.tolist() ],\n",
    "                                                    [db_centers_px[a] for a in overlapping_bboxes_indices.tolist()],\n",
    "                                                    [db_classes[a] for a in overlapping_bboxes_indices.tolist()],\n",
    "                                                )):\n",
    "            \n",
    "            # matcher avec le ms gorup le plus proche + garde fou de distance\n",
    "            ms_centroids, ms_members = centroids_px, ms_dict['groups_px']\n",
    "            \n",
    "            intersect = c_utils.contains_ms_groups(db_bbox, db_center, ms_centroids, ms_members)\n",
    "            \n",
    "            if sum(intersect) == 0: # Il n'y a eu aucune détection dans cette zone\n",
    "                cur_out_stats['num_noMS_but_DB_reject'] += 1\n",
    "                cause = 'noMS_but_DB'\n",
    "                add_rejected_to_distributions(cur_rejected_class_distibutions[cause], db_class[\"McIntosh\"][0])\n",
    "                pass\n",
    "            elif sum(intersect) == 1: # il n'y a de l'overlap qu'avec un seul groupe meanshift            \n",
    "    #             print('hit')\n",
    "                idx = intersect.index(True)\n",
    "    #             print(idx)\n",
    "                # vérifier que le groupe meanshift n'intersecte aucune autre bbox\n",
    "                num_intersections = np.sum(c_utils.count_group_intersections(ms_members[idx], db_bboxes))\n",
    "                if num_intersections > 1:\n",
    "                    cur_out_stats['num_singleMS_multipleDB_reject'] += 1\n",
    "                    cause = 'singleMS_multipleDB'\n",
    "                    add_rejected_to_distributions(cur_rejected_class_distibutions[cause], db_class[\"McIntosh\"][0])\n",
    "                    continue\n",
    "                \n",
    "                Rmm = huge_db_dict[basename]['dr_radius_mm']\n",
    "                R_pixel = huge_db_dict[basename]['dr_radius_px']\n",
    "                sun_center = huge_db_dict[basename]['dr_center_px']\n",
    "                dr_pixpos = np.array([group_list[i]['posx'], group_list[i]['posy']])\n",
    "                \n",
    "                angular_excentricity =  c_utils.get_angle2(dr_pixpos, R_pixel, sun_center)\n",
    "                \n",
    "                cur_group_dict={\n",
    "                                \"centroid_px\": centroids_px[idx],\n",
    "                                \"centroid_Lat\": centroids[idx][0],\n",
    "                                \"centroid_Lon\": centroids[idx][1],\n",
    "                                \"angular_excentricity_rad\": angular_excentricity,\n",
    "                                \"angular_excentricity_deg\": np.rad2deg(angular_excentricity),\n",
    "                                \"Zurich\":   db_class[\"Zurich\"],\n",
    "                                \"McIntosh\": db_class[\"McIntosh\"],\n",
    "                                \"members\": ms_members[idx],\n",
    "                                \"members_mean_px\": np.mean(ms_members[idx], axis=0),\n",
    "                            }\n",
    "                \n",
    "                \n",
    "                cur_out_groups.append(cur_group_dict)\n",
    "                cur_out_stats['num_MSmatchesDB'] += 1\n",
    "\n",
    "            else: # db_bbox intersecte plusieurs groupes meanshift\n",
    "                cur_out_stats['num_oneDBbbox_multipleMSoverlap_ambiguity_reject'] += 1\n",
    "                cause = 'num_oneDBbbox_multipleMSoverlap_ambiguity'\n",
    "                add_rejected_to_distributions(cur_rejected_class_distibutions[cause], db_class[\"McIntosh\"][0])\n",
    "                pass\n",
    "                \n",
    "        if len(cur_out_groups) > 0:\n",
    "            image_out_dict[basename] = { \"angle\": angle,\n",
    "                                        \"deltashapeX\":deltashapeX,\n",
    "                                        \"deltashapeY\":deltashapeY,\n",
    "                                        \"groups\": cur_out_groups,\n",
    "                                    }\n",
    "\n",
    "        # count the number of MS groups that do not have any overlap with the DB\n",
    "        num_intersections_per_group = [np.sum(c_utils.count_group_intersections(ms_members[idx], db_bboxes)) for idx in range(len(ms_members))]\n",
    "        num_MS_without_DB_overlap = len(np.where(np.array(num_intersections_per_group) == 0)[0])\n",
    "        cur_out_stats['num_noDB_but_MS'] = num_MS_without_DB_overlap\n",
    "        \n",
    "        # print(cur_rejected_class_distibutions)\n",
    "        cur_out_stats['rejected_class_distributions'] = deepcopy(cur_rejected_class_distibutions)\n",
    "        # print(cur_out_stats)\n",
    "        image_out_dict_stats[basename] = deepcopy(cur_out_stats)\n",
    "\n",
    "        \n",
    "\n",
    "    print('num_images: ', len(list(image_out_dict.keys())))\n",
    "    num_groups = 0\n",
    "    for k,v in image_out_dict.items():\n",
    "        num_groups += len(v['groups'])\n",
    "    print(\"num_groups: \",num_groups)\n",
    "    # print(image_out_dict)\n",
    "\n",
    "    grid_image_out_dict_overlapping[param_idx] = deepcopy(image_out_dict)\n",
    "    grid_image_out_dict_stats_overlapping[param_idx] = deepcopy(image_out_dict_stats)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "from collections import Counter\n",
    "\n",
    "cur_out_stats = {\n",
    "    # General info\n",
    "    'num_DB_groups': 0,\n",
    "    'num_MS_groups': 0,\n",
    "    'num_DB_isolated_groups': 0,\n",
    "    'num_DB_overlaping_bboxes': 0,\n",
    "    # MS with DB matching info\n",
    "    \"num_MSmatchesDB\":0,\n",
    "    # MS with DB rejection info\n",
    "    \"num_noMS_but_DB_reject\":0,\n",
    "    \"num_singleMS_multipleDB_reject\":0,\n",
    "    \"num_oneDBbbox_multipleMSoverlap_ambiguity_reject\":0,\n",
    "    # MS with DB no match info\n",
    "    \"num_noDB_but_MS\":0,\n",
    "    }\n",
    "\n",
    "stats_keys = cur_out_stats.keys()\n",
    "\n",
    "should_check =  list(stats_keys) + [\n",
    "                                        'num_rejects_all',\n",
    "                                        'num_optimizable_rejects',\n",
    "                                        'rate_optimizable_rejects',\n",
    "                                    ]\n",
    "\n",
    "gridsearch_dict_per_img_overlapping = { k : np.zeros((len(param_grid_values['kernel_bandwidthLon']), \n",
    "                                   len(param_grid_values['kernel_bandwidthLat']),\n",
    "                                    len(wl_list)\n",
    "                                   ))\n",
    "                    for k in should_check}\n",
    "\n",
    "should_check_global = list(stats_keys) + [\n",
    "                                            'num_rejects_all',\n",
    "                                            'num_optimizable_rejects',\n",
    "                                            'rate_optimizable_rejects',\n",
    "                                         ]\n",
    "gridsearch_dict_total_overlapping = { k : np.zeros((len(param_grid_values['kernel_bandwidthLon']), \n",
    "                                   len(param_grid_values['kernel_bandwidthLat'])\n",
    "                                   ))\n",
    "                    for k in should_check_global}\n",
    "\n",
    "# grid_search_reject_distribs = { k : {} for param_idx, params in enumerate(param_grid)}\n",
    "grid_search_reject_distribs_overlapping = {}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for param_idx, params in enumerate(param_grid):\n",
    "    image_out_dict = grid_image_out_dict_overlapping[param_idx]\n",
    "    image_out_dict_stats = grid_image_out_dict_stats_overlapping[param_idx]\n",
    "    \n",
    "    # General info\n",
    "    num_DB_groups_per_image = np.array([item['num_DB_groups'] for k,item in image_out_dict_stats.items()])\n",
    "    num_MS_groups_per_image = np.array([item['num_MS_groups'] for k,item in image_out_dict_stats.items()])\n",
    "\n",
    "    diff_num_groups = num_DB_groups_per_image - num_MS_groups_per_image # should be shown on histogram , closer to 0 is better\n",
    "\n",
    "    # number of isolated DB groups per image and mean number in dataset\n",
    "    num_isolated_DB_groups = np.array([v['num_DB_isolated_groups'] for k,v in image_out_dict_stats.items()])\n",
    "    # number of DB groups with overlap with other DB groups\n",
    "    num_DB_overlaping_bboxes = np.array([v['num_DB_overlaping_bboxes'] for k,v in image_out_dict_stats.items()])\n",
    "\n",
    "    # matches\n",
    "    num_MSmatchesDB = np.array([v['num_MSmatchesDB'] for k,v in image_out_dict_stats.items()])\n",
    "    \n",
    "    # rejects\n",
    "    # 1) numbers of rejects due to no MS group overlapping DB group (# cannot do anything to this, examples are discarded)\n",
    "    num_noMS_but_DB_reject = np.array([v['num_noMS_but_DB_reject'] for k,v in image_out_dict_stats.items()])\n",
    "    # 2) numbers of rejects due to single MS group overlapping multiple DB groups (# minimize this)\n",
    "    num_singleMS_multipleDB_reject = np.array([v['num_singleMS_multipleDB_reject'] for k,v in image_out_dict_stats.items()])\n",
    "    # 3) numbers of rejects due to single DB group overlapping multiple MS groups (# minimize this)\n",
    "    num_oneDBbbox_multipleMSoverlap_ambiguity_reject = np.array([v['num_oneDBbbox_multipleMSoverlap_ambiguity_reject'] for k,v in image_out_dict_stats.items()])\n",
    "\n",
    "    num_optimizable_rejects = num_singleMS_multipleDB_reject + num_oneDBbbox_multipleMSoverlap_ambiguity_reject\n",
    "    num_rejects_all = num_optimizable_rejects + num_noMS_but_DB_reject\n",
    "\n",
    "    # rate of optimizable rejects\n",
    "    rate_optimizable_rejects = num_optimizable_rejects / num_rejects_all\n",
    "\n",
    "    # print(num_optimizable_rejects)\n",
    "\n",
    "\n",
    "    # Aggregate reject distributions of images over the dataset\n",
    "    dict1 = [v['rejected_class_distributions']['noMS_but_DB'] for k,v in image_out_dict_stats.items()]\n",
    "    summed_dict1 = sum(map(collections.Counter, dict1),Counter())\n",
    "    dict2 = [v['rejected_class_distributions']['singleMS_multipleDB'] for k,v in image_out_dict_stats.items()]\n",
    "    summed_dict2 = sum(map(collections.Counter, dict2),Counter())\n",
    "    dict3 = [v['rejected_class_distributions']['num_oneDBbbox_multipleMSoverlap_ambiguity'] for k,v in image_out_dict_stats.items()]\n",
    "    summed_dict3 = sum(map(collections.Counter, dict3),Counter())\n",
    "    dataset_rejected_class_distibutions = { \n",
    "        'noMS_but_DB': summed_dict1,\n",
    "        'singleMS_multipleDB': summed_dict2, \n",
    "        'num_oneDBbbox_multipleMSoverlap_ambiguity': summed_dict3,\n",
    "    }\n",
    "\n",
    "    # print(dict1)\n",
    "    # print(summed_dict1)\n",
    "    \n",
    "\n",
    "\n",
    "    # get indices of current params\n",
    "    kernel_bandwidthLon_idx = param_grid_values['kernel_bandwidthLon'].index(params['kernel_bandwidthLon'])\n",
    "    kernel_bandwidthLat_idx = param_grid_values['kernel_bandwidthLat'].index(params['kernel_bandwidthLat'])\n",
    "    ############################\n",
    "    # Fill per-image dictinnary \n",
    "    ############################\n",
    "\n",
    "    # number of DB groups\n",
    "    gridsearch_dict_per_img_overlapping['num_DB_groups'][kernel_bandwidthLon_idx,kernel_bandwidthLat_idx] = num_DB_groups_per_image\n",
    "    # number of MS groups\n",
    "    gridsearch_dict_per_img_overlapping['num_MS_groups'][kernel_bandwidthLon_idx,kernel_bandwidthLat_idx] = num_MS_groups_per_image\n",
    "\n",
    "    # number of isolated DB groups\n",
    "    gridsearch_dict_per_img_overlapping['num_DB_isolated_groups'][kernel_bandwidthLon_idx,kernel_bandwidthLat_idx] = num_isolated_DB_groups\n",
    "    gridsearch_dict_per_img_overlapping['num_DB_overlaping_bboxes'][kernel_bandwidthLon_idx,kernel_bandwidthLat_idx] = num_DB_overlaping_bboxes\n",
    "\n",
    "    # matches\n",
    "    gridsearch_dict_per_img_overlapping['num_MSmatchesDB'][kernel_bandwidthLon_idx,kernel_bandwidthLat_idx] = num_MSmatchesDB\n",
    "    \n",
    "    # not optimizable rejects\n",
    "    gridsearch_dict_per_img_overlapping['num_noMS_but_DB_reject'][kernel_bandwidthLon_idx,kernel_bandwidthLat_idx] = num_noMS_but_DB_reject\n",
    "    \n",
    "    # optimizable rejects\n",
    "    gridsearch_dict_per_img_overlapping['num_singleMS_multipleDB_reject'][kernel_bandwidthLon_idx,kernel_bandwidthLat_idx] = num_singleMS_multipleDB_reject\n",
    "    gridsearch_dict_per_img_overlapping['num_oneDBbbox_multipleMSoverlap_ambiguity_reject'][kernel_bandwidthLon_idx,kernel_bandwidthLat_idx] = num_oneDBbbox_multipleMSoverlap_ambiguity_reject\n",
    "    gridsearch_dict_per_img_overlapping['num_optimizable_rejects'][kernel_bandwidthLon_idx,kernel_bandwidthLat_idx] = num_optimizable_rejects\n",
    "    \n",
    "    gridsearch_dict_per_img_overlapping['num_rejects_all'][kernel_bandwidthLon_idx,kernel_bandwidthLat_idx] = num_rejects_all\n",
    "    \n",
    "    # rate of optimizable rejects\n",
    "    gridsearch_dict_per_img_overlapping['rate_optimizable_rejects'][kernel_bandwidthLon_idx,kernel_bandwidthLat_idx] = rate_optimizable_rejects\n",
    "    \n",
    "    ############################\n",
    "    # Fill total dictionnary\n",
    "    ############################\n",
    "    # number of DB groups\n",
    "    gridsearch_dict_total_overlapping['num_DB_groups'][kernel_bandwidthLon_idx,kernel_bandwidthLat_idx] = np.sum(num_DB_groups_per_image)\n",
    "    # number of MS groups\n",
    "    gridsearch_dict_total_overlapping['num_MS_groups'][kernel_bandwidthLon_idx,kernel_bandwidthLat_idx] = np.sum(num_MS_groups_per_image)\n",
    "\n",
    "    # number of isolated DB groups\n",
    "    gridsearch_dict_total_overlapping['num_DB_isolated_groups'][kernel_bandwidthLon_idx,kernel_bandwidthLat_idx] = np.sum(num_isolated_DB_groups)\n",
    "    gridsearch_dict_total_overlapping['num_DB_overlaping_bboxes'][kernel_bandwidthLon_idx,kernel_bandwidthLat_idx] = np.sum(num_DB_overlaping_bboxes)\n",
    "\n",
    "    # matches\n",
    "    gridsearch_dict_total_overlapping['num_MSmatchesDB'][kernel_bandwidthLon_idx,kernel_bandwidthLat_idx] = np.sum(num_MSmatchesDB)\n",
    "\n",
    "    # not optimizable rejects\n",
    "    gridsearch_dict_total_overlapping['num_noMS_but_DB_reject'][kernel_bandwidthLon_idx,kernel_bandwidthLat_idx] = np.sum(num_noMS_but_DB_reject)\n",
    "\n",
    "    # optimizable rejects\n",
    "    gridsearch_dict_total_overlapping['num_singleMS_multipleDB_reject'][kernel_bandwidthLon_idx,kernel_bandwidthLat_idx] = np.sum(num_singleMS_multipleDB_reject)\n",
    "    gridsearch_dict_total_overlapping['num_oneDBbbox_multipleMSoverlap_ambiguity_reject'][kernel_bandwidthLon_idx,kernel_bandwidthLat_idx] = np.sum(num_oneDBbbox_multipleMSoverlap_ambiguity_reject)\n",
    "    gridsearch_dict_total_overlapping['num_optimizable_rejects'][kernel_bandwidthLon_idx,kernel_bandwidthLat_idx] = np.sum(num_optimizable_rejects)\n",
    "\n",
    "    gridsearch_dict_total_overlapping['num_rejects_all'][kernel_bandwidthLon_idx,kernel_bandwidthLat_idx] = np.sum(num_rejects_all)\n",
    "\n",
    "\n",
    "    rate_optimizable_rejects_all = np.sum(num_optimizable_rejects) / np.sum(num_rejects_all)\n",
    "    gridsearch_dict_total_overlapping['rate_optimizable_rejects'][kernel_bandwidthLon_idx,kernel_bandwidthLat_idx] = rate_optimizable_rejects_all\n",
    "\n",
    "    grid_search_reject_distribs_overlapping[param_idx] = deepcopy(dataset_rejected_class_distibutions) \n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "gridsearch = { k : pd.DataFrame(v) }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy\n",
    "all_isolated = gridsearch_dict_total_overlapping['num_DB_isolated_groups'] \n",
    "all_overlapping = gridsearch_dict_total_overlapping['num_DB_overlaping_bboxes'] \n",
    "\n",
    "print('all_isolated: ', all_isolated[0,0],'all_overlapping: ' ,all_overlapping[0,0])\n",
    "\n",
    "all_noMS_but_DB = gridsearch_dict_total_overlapping['num_noMS_but_DB_reject']\n",
    "\n",
    "optimizable = all_overlapping - all_noMS_but_DB\n",
    "\n",
    "print('all_optimizable', optimizable[0,0])\n",
    "\n",
    "\n",
    "matches = gridsearch_dict_total_overlapping['num_MSmatchesDB']\n",
    "\n",
    "accuracy = matches / optimizable\n",
    "\n",
    "accuracy_df = pd.DataFrame(accuracy)\n",
    "accuracy_df.columns = param_grid_values['kernel_bandwidthLat']\n",
    "accuracy_df.index = param_grid_values['kernel_bandwidthLon']\n",
    "\n",
    "\n",
    "fig_, ax_ = plt.subplots(1,1, figsize=(3.5*1,3*1))\n",
    "ax_ = sns.heatmap(accuracy_df, ax=ax_, vmin=0, annot=True)\n",
    "ax_.set_title('Accuracy')\n",
    "ax_.set_xlabel('kernel_bandwidthLat')\n",
    "ax_.set_ylabel('kernel_bandwidthLon')\n",
    "\n",
    "fig_.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def on_value_change_overlapping(change):\n",
    "   \n",
    "\n",
    "    # print(ax)\n",
    "    # print(len(ax.collections))\n",
    "    for i in range(num_cols):\n",
    "        if len(ax[i].collections) > 0:\n",
    "            cb = ax[i].collections[-1].colorbar\n",
    "            if cb is not None:\n",
    "                cb.remove()\n",
    "            ax[i].clear()\n",
    "\n",
    "    df = pd.DataFrame(gridsearch_dict_per_img_overlapping['num_optimizable_rejects'][:, :, a_slider.value])\n",
    "    df.columns = param_grid_values['kernel_bandwidthLat']\n",
    "    df.index = param_grid_values['kernel_bandwidthLon']\n",
    "    # ax0 = sns.heatmap(df, ax=ax, cbar=False,)\n",
    "    ax0 = sns.heatmap(df, ax=ax[0], vmin=0, annot=True)\n",
    "    ax0.set_title('Number of optimizable rejects')\n",
    "    ax0.set_xlabel('kernel_bandwidthLat')\n",
    "    ax0.set_ylabel('kernel_bandwidthLon')\n",
    "\n",
    "    df = pd.DataFrame(gridsearch_dict_per_img_overlapping['num_singleMS_multipleDB_reject'][:, :, a_slider.value])\n",
    "    df.columns = param_grid_values['kernel_bandwidthLat']\n",
    "    df.index = param_grid_values['kernel_bandwidthLon']\n",
    "    # ax1 = sns.heatmap(df, ax=ax, cbar=False,)\n",
    "    ax1 = sns.heatmap(df, ax=ax[1], vmin=0, annot=True)\n",
    "    ax1.set_title('Only MS candidate covers \\n2+ DB bboxes')\n",
    "    ax1.set_xlabel('kernel_bandwidthLat')\n",
    "    ax1.set_ylabel('kernel_bandwidthLon')\n",
    "    \n",
    "    df = pd.DataFrame(gridsearch_dict_per_img_overlapping['num_oneDBbbox_multipleMSoverlap_ambiguity_reject'][:, :, a_slider.value])\n",
    "    df.columns = param_grid_values['kernel_bandwidthLat']\n",
    "    df.index = param_grid_values['kernel_bandwidthLon']\n",
    "    # ax0 = sns.heatmap(df, ax=ax, cbar=False,)\n",
    "    ax2 = sns.heatmap(df, ax=ax[2], vmin=0, annot=True)\n",
    "    ax2.set_title('DB group matches \\nmultiple MS groups')\n",
    "    ax2.set_xlabel('kernel_bandwidthLat')\n",
    "    ax2.set_ylabel('kernel_bandwidthLon')\n",
    "\n",
    "    fig.tight_layout()\n",
    "\n",
    "    # pass\n",
    "\n",
    "a_slider = widgets.IntSlider(min=0, max=len(wl_list)-1, step=1, value=0)\n",
    "a_slider.observe(on_value_change_overlapping, names='value')\n",
    "\n",
    "plt.ioff()\n",
    "num_rows = 1\n",
    "num_cols = 3\n",
    "fig, ax = plt.subplots(num_rows,num_cols, figsize=(3*num_cols,3*num_rows))\n",
    "on_value_change_overlapping(None)\n",
    "plt.ion()\n",
    "\n",
    "display(widgets.VBox([a_slider, fig.canvas]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Toy example\n",
    "look_distance = .1  # How far to look for neighbours.\n",
    "kernel_bandwidthLon = .25  # Longitude Kernel parameter.\n",
    "kernel_bandwidthLat = .08  # Latitude Kernel parameter.\n",
    "n_iterations = 5 # Number of iterations\n",
    "\n",
    "radius = 695700\n",
    "\n",
    "points_latLon = np.array([[1, 0],\n",
    "                          [0,2.6],\n",
    "                          [0,2.7]])\n",
    "areas = [1000, 500 , 510]\n",
    "\n",
    "\n",
    "ms_model = MS.Mean_Shift(look_distance, kernel_bandwidthLon, kernel_bandwidthLat, radius, n_iterations)\n",
    "ms_model.fit(points_latLon, areas)\n",
    "ms_centroids = ms_model.centroids\n",
    "print()\n",
    "print(ms_model.get_area_weighted_ellipsis_width(areas[2],areas))\n",
    "\n",
    "print(ms_centroids)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "1f3e85867ab3feeb73691fcc67a502ec8f0fc265745d17c9ab3a5329e7f22e4e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
