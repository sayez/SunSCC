{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "import os, sys\n",
    "print(os.path.abspath('..'))\n",
    "sys.path.append(os.path.abspath('../scripts'))\n",
    "\n",
    "import glob\n",
    "import cv2\n",
    "from skimage import data, filters, segmentation\n",
    "\n",
    "from skimage import data\n",
    "from skimage.filters.rank import entropy\n",
    "from skimage.morphology import disk, ball, black_tophat, white_tophat\n",
    "from skimage.measure import shannon_entropy, label, regionprops\n",
    "from skimage.morphology import square, disk\n",
    "from skimage.io import imread\n",
    "\n",
    "from scipy.signal import find_peaks\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib ipympl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from astropy.io import fits\n",
    "import math\n",
    "import importlib\n",
    "# importlib.reload(sys)\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import json\n",
    "\n",
    "\n",
    "import concurrent.futures\n",
    "from itertools import repeat\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "print(pandas.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install omegaconf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "!CUDA_VISIBLE_DEVICE=1\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize the generated/predicted Masks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "\n",
    "from matplotlib import cm\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "dataset_root = \"../datasets/Segmentation_dataset/ManualAnnotation\"\n",
    "\n",
    "seg_types = ['GroundTruth_single']\n",
    "st = 25\n",
    "tophat_thresh= range(250, 500 + st, st)\n",
    "gen_seg_types = [f'2023_T{t}_fgbg' for t in tophat_thresh]\n",
    "print(gen_seg_types)\n",
    "\n",
    "\n",
    "image_folder     = os.path.join(dataset_root, 'image')\n",
    "image_lst = sorted(glob.glob(os.path.join(image_folder, '**/*.FTS'), recursive=True))\n",
    "print(len(image_lst))\n",
    "\n",
    "gt_folder        = os.path.join(dataset_root, 'GroundTruth_single')\n",
    "gt_lst = sorted(glob.glob(os.path.join(gt_folder, '*.png')))\n",
    "\n",
    "generated_folders = {t : os.path.join(dataset_root, t) for t in gen_seg_types}\n",
    "# print(generated_folders)\n",
    "generated_lsts = {t : sorted(glob.glob(os.path.join(generated_folders[t], '*.png'))) for t in gen_seg_types}\n",
    "# print(generated_lsts)\n",
    "\n",
    "\n",
    "dir_lst = [\n",
    "    # Multiple Thresholds\n",
    "    # Path('/home/ucl/elen/nsayez/bio-blueprints/outputs/2023-01-22/01-18-26_2013-15_UNet_T425_T375_T325_StepLR_epoch_1_run0'),\n",
    "    Path('/home/ucl/elen/nsayez/bio-blueprints/outputs/2023-01-22/01-18-26_2013-15_UNet_T425_T375_T325_StepLR_epoch_1_run0'),\n",
    "    Path('/home/ucl/elen/nsayez/bio-blueprints/outputs/2023-01-22/01-18-26_2013-15_UNet_T425_T375_T325_StepLR_epoch_1_run2'),\n",
    "    Path('/home/ucl/elen/nsayez/bio-blueprints/outputs/2023-01-22/01-18-26_2013-15_UNet_T425_T375_T325_StepLR_epoch_1_run3'),\n",
    "    Path('/home/ucl/elen/nsayez/bio-blueprints/outputs/2023-01-22/01-18-26_2013-15_UNet_T425_T375_T325_StepLR_epoch_1_run4'),\n",
    "    Path('/home/ucl/elen/nsayez/bio-blueprints/outputs/2023-01-22/05-55-04_2013-15_UNet_T425_T375_T325_StepLR_epoch_1_run5'),\n",
    "    Path('/home/ucl/elen/nsayez/bio-blueprints/outputs/2023-01-22/05-55-04_2013-15_UNet_T425_T375_T325_StepLR_epoch_1_run6'),\n",
    "    Path('/home/ucl/elen/nsayez/bio-blueprints/outputs/2023-01-22/06-30-11_2013-15_UNet_T425_T375_T325_StepLR_epoch_1_run7'),\n",
    "    Path('/home/ucl/elen/nsayez/bio-blueprints/outputs/2023-01-22/10-23-14_2013-15_UNet_T425_T375_T325_StepLR_epoch_1_run9'),\n",
    "    Path('/home/ucl/elen/nsayez/bio-blueprints/outputs/2023-01-22/10-24-14_2013-15_UNet_T425_T375_T325_StepLR_epoch_1_run10'),\n",
    "    Path('/home/ucl/elen/nsayez/bio-blueprints/outputs/2023-01-22/13-15-38_2013-15_UNet_T425_T375_T325_StepLR_epoch_1_run8'),\n",
    "]\n",
    "\n",
    "run_dirs = [Path(d) for d in dir_lst]\n",
    "run_folders = { f\"CNN-{rdir.stem[17:].replace('_StepLR_epoch_1_','_')}\": str(rdir) for rdir in run_dirs}\n",
    "\n",
    "\n",
    "seg_folders = {t:generated_folders[t] if t in generated_folders.keys() else run_folders[t]+\"/predictions\"\n",
    "                        for t in list(generated_folders.keys())+list(run_folders.keys())}\n",
    "\n",
    "# num_TTA = [0,4]\n",
    "num_TTA = [4]\n",
    "# num_TTA = [0]\n",
    "pred_folders = {t+f'_{n}TTA': os.path.join(run_folders[t] , f'predictions_{n}TTA_ManualAnnotation' if n > 0 else f'predictions_solDisk') \n",
    "# pred_folders = {t+f'_{n}TTA': os.path.join(run_folders[t] , f'predictions_{n}TTA' if n > 0 else f'predictions') \n",
    "                            for n in num_TTA\n",
    "                            for t in run_folders.keys()}\n",
    "pred_lsts = {t : sorted(glob.glob(os.path.join(pred_folders[t], '*.npy'))) for t in pred_folders.keys()}\n",
    "# pred_lsts = {t : sorted(glob.glob(os.path.join(pred_folders[t], '*.png'))) for t in pred_folders.keys()}\n",
    "print()\n",
    "print(pred_folders)\n",
    "print()\n",
    "# print(pred_lsts)\n",
    "print({k:len(v) for k, v in pred_lsts.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "max_idx = len(image_lst)\n",
    "# \n",
    "# cmap_gen = cm.winter\n",
    "cmap_gen = cm.turbo\n",
    "cmap_gen = cmap_gen(range(255))\n",
    "cmap_gen = ListedColormap([(0, 0, 0, 0), *cmap_gen])\n",
    "\n",
    "cmap_pred = cm.cool\n",
    "cmap_pred = cmap_pred(range(255))\n",
    "cmap_pred = ListedColormap([(0, 0, 0, 0), *cmap_pred])\n",
    "\n",
    "\n",
    "cmap_circle = cm.jet\n",
    "cmap_circle = cmap_gen(range(255))\n",
    "cmap_circle = ListedColormap([(0, 0, 0, 0), *cmap_circle])\n",
    "\n",
    "cmap_gt = cm.Wistia\n",
    "cmap_gt = cmap_gt(range(255))\n",
    "cmap_gt = ListedColormap([(0, 0, 0, 0), *cmap_gt])\n",
    "\n",
    "def create_circular_mask(h, w, center=None, radius=None):\n",
    "\n",
    "    if center is None: # use the middle of the image\n",
    "        center = (int(w/2), int(h/2))\n",
    "    if radius is None: # use the smallest distance between the center and image walls\n",
    "        radius = min(center[0], center[1], w-center[0], h-center[1])\n",
    "\n",
    "    Y, X = np.ogrid[:h, :w]\n",
    "    dist_from_center = np.sqrt((X - center[0])**2 + (Y-center[1])**2)\n",
    "\n",
    "    mask = dist_from_center <= radius\n",
    "    return mask\n",
    "\n",
    "def refresh(slider): \n",
    "    xlims0 = axes0[0].get_xlim()\n",
    "    ylims0 = axes0[0].get_ylim()\n",
    "    \n",
    "    axes0[0].clear()\n",
    "    axes0[0].axis('Off')\n",
    "    \n",
    "    \n",
    "    hdulst:fits.HDUList = fits.open(image_lst[idx_slider.value])\n",
    "    test_img = hdulst[0]\n",
    "    header = test_img.header\n",
    "    center = np.array(test_img.shape)//2\n",
    "    radius = header['SOLAR_R']\n",
    "    solar_disk = 1-create_circular_mask( test_img.shape[0], test_img.shape[1] ,center,radius)\n",
    "#     test_img = imread(image_lst[idx_slider.value])\n",
    "    \n",
    "    \n",
    "    gt_label = imread(gt_lst[idx_slider.value])\n",
    "#     gen_label = imread(generated_lst[idx_slider.value])\n",
    "    \n",
    "#     axes0[0].set_title(os.path.basename(image_lst[idx_slider.value]))\n",
    "    \n",
    "    if img_cb.value:\n",
    "        axes0[0].imshow(test_img.data, cmap=\"gray\", interpolation=\"None\")\n",
    "        axes0[0].imshow(solar_disk, cmap=cmap_circle, interpolation=\"None\",alpha=.5)\n",
    "#         axes0[0].invert_yaxis()\n",
    "        axes0[0].get_xlim()[::-1]\n",
    "    if gt_cb.value:\n",
    "        axes0[0].imshow(gt_label, cmap=cmap_gt, interpolation=\"None\", alpha=.5)\n",
    "    for i, cb in enumerate(gen_cbs):\n",
    "        if cb.value:\n",
    "            tmp = imread(generated_lsts[gen_seg_types[i]][idx_slider.value])\n",
    "            cs = axes0[0].imshow(tmp, cmap=cmap_gen, interpolation=\"None\", alpha=.7)\n",
    "            \n",
    "    for i, cb in enumerate(pred_cbs):\n",
    "        if cb.value: \n",
    "            # tmp = imread(pred_lsts[list(pred_folders.keys())[i]][idx_slider.value])\n",
    "            tmp = np.load(pred_lsts[list(pred_folders.keys())[i]][idx_slider.value])\n",
    "            axes0[0].imshow(tmp, cmap=cmap_pred, interpolation=\"None\", alpha=.5)  \n",
    "        \n",
    "    if xlims0 != (0.0, 1.0):\n",
    "        axes0[0].set_xlim(xlims0)\n",
    "        axes0[0].set_ylim(ylims0)\n",
    "        \n",
    "    \n",
    "    return\n",
    "\n",
    "max_rows = 1\n",
    "max_cols = 1\n",
    "\n",
    "plt.ioff()\n",
    "plt.style.use('default')\n",
    "# plt.style.use('dark_background')\n",
    "fig_widget0, axes0 = plt.subplots(nrows=max_rows, ncols=max_cols, figsize=(8,5))\n",
    "# axes0.axis('Off')\n",
    "try:\n",
    "    len(axes0)\n",
    "except TypeError:\n",
    "    axes0 = [axes0]\n",
    "\n",
    "plt.ion()\n",
    "img_cb = widgets.Checkbox(value=True, description='img')\n",
    "gt_cb = widgets.Checkbox(value=False, description='gt')\n",
    "gen_cbs = [widgets.Checkbox(value=False, description=f'{t}') for t in gen_seg_types]\n",
    "\n",
    "pred_cbs = [widgets.Checkbox(value=False, description=f'{t}') for t in pred_folders.keys()]\n",
    "# print(pred_cbs)\n",
    "# gen_cb = widgets.Checkbox(value=False, description='Show Generation')\n",
    "idx_slider = widgets.IntSlider(value=0, min=0, max=max_idx-1, step=1, description=\"Image Index\")\n",
    "\n",
    "\n",
    "# Input image to predict\n",
    "test_img = imread(image_lst[0])\n",
    "#prediction\n",
    "gen_label = imread(generated_lsts[gen_seg_types[0]][0])\n",
    "\n",
    "# print(gen_cbs)\n",
    "# print(test_img.dtype)\n",
    "# print(gen_label.dtype)\n",
    "\n",
    "axes0[0].set_title(os.path.basename(image_lst[0]))\n",
    "axes0[0].imshow(test_img, cmap=\"gray\", interpolation=\"None\")\n",
    "\n",
    "# fig_widget0.colorbar(cm.ScalarMappable(norm=Normalize(vmin=0, vmax=3, clip=False), cmap=cmap_gen), ax=axes0)\n",
    "# fig_widget0.colorbar(cs)\n",
    "\n",
    "img_cb.observe(refresh, names='value')\n",
    "gt_cb.observe(refresh, names='value')\n",
    "for cb in gen_cbs:\n",
    "    cb.observe(refresh, names='value')\n",
    "for cb in pred_cbs:\n",
    "    cb.observe(refresh, names='value')\n",
    "idx_slider.observe(refresh, names='value')\n",
    "\n",
    "h_len = 3\n",
    "vbox_gen = widgets.VBox([ widgets.HBox(gen_cbs[i*h_len:(i+1)*h_len]) for i in range(len(gen_seg_types)//h_len)])\n",
    "vbox_pred = widgets.VBox([ widgets.HBox(pred_cbs[i*h_len:(i+1)*h_len]) for i in range(math.ceil(len(pred_cbs)/h_len))])\n",
    "widgets.VBox([vbox_gen, vbox_pred, widgets.HBox([idx_slider,gt_cb]),  fig_widget0.canvas])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split sunspots into size categories\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, multilabel_confusion_matrix\n",
    "\n",
    "def conf_matrix(y_true, y_pred, labels=None, sample_weight=None, samplewise=False):\n",
    "    MCM = multilabel_confusion_matrix(\n",
    "                y_true,\n",
    "                y_pred,\n",
    "                sample_weight=sample_weight,\n",
    "                labels=labels,\n",
    "                samplewise=samplewise,\n",
    "    )\n",
    "    return MCM\n",
    "\n",
    "def get_scores_dict(gt_label, pred_label, classes):\n",
    "    ###### Separate penumbra and umbra classes\n",
    "    perClassCM = conf_matrix(gt_label.flatten().cpu(),pred_label.flatten().cpu(), labels=fg_classes)\n",
    "    \n",
    "    CM_dict = {c: perClassCM[i] for i,c in enumerate(fg_classes)}\n",
    "    IoU = [ CM_dict[k][1,1] / ( CM_dict[k][1,1] + CM_dict[k][0,1] + CM_dict[k][1,0] + 1e-5) for k in CM_dict]\n",
    "    DSC = [ 2*CM_dict[k][1,1] / ( 2*CM_dict[k][1,1] + CM_dict[k][0,1] + CM_dict[k][1,0] + 1e-5) for k in CM_dict]\n",
    "        \n",
    "    scores_dict = {c: {\"DICE\": DSC[i], \"IoU\": IoU[i]} for i,c in enumerate(classes)}    \n",
    "    return scores_dict\n",
    "\n",
    "    \n",
    "def crop_in(y_true, y_pred, bbox, padding):\n",
    "        minX,maxX = max(bbox[0]-padding, 0), min( bbox[2]+padding, y_true.shape[0])\n",
    "        minY,maxY = max(bbox[1]-padding, 0), min( bbox[3]+padding, y_true.shape[1])\n",
    "        \n",
    "        bbox2 = [minX,minY,maxX,maxY]\n",
    "\n",
    "        deltasX = [bbox[0]-minX,bbox[2]-maxX]\n",
    "        deltasY = [bbox[1]-minY,bbox[3]-maxY]\n",
    "       \n",
    "        new_gt_mask = np.zeros((maxX-minX,maxY-minY))\n",
    "        new_gt_mask[deltasX[0]:deltasX[1],deltasY[0]:deltasY[1]] = y_true[bbox[0]:bbox[2], bbox[1]:bbox[3]]\n",
    "        \n",
    "        \n",
    "        new_gen_mask = np.zeros((maxX-minX,maxY-minY))\n",
    "        new_gen_mask[deltasX[0]:deltasX[1],deltasY[0]:deltasY[1]] = y_pred[bbox[0]:bbox[2], bbox[1]:bbox[3]]\n",
    "        \n",
    "        assert new_gt_mask.shape == new_gen_mask.shape\n",
    "        \n",
    "        return new_gt_mask, new_gen_mask\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.patches import Rectangle\n",
    "def show_bins(seg_type, bins, fg_bg, penumbra, umbra, metric='DICE'):\n",
    "    for i in range(len(bins)):\n",
    "        b = bins[i]\n",
    "\n",
    "        scores_fg_bg = [item['value'] for item in fg_bg[i][metric]]\n",
    "        scores_penumbra = [item['value'] for item in penumbra[i][metric]]\n",
    "        scores_umbra = [item['value'] for item in umbra[i][metric]]\n",
    "\n",
    "        fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(10, 5))  \n",
    "        fig.suptitle(f\"{seg_type}  : Sunspots of [{b[0]},{b[1]}] pixels\")\n",
    "        ax[0].hist(scores_fg_bg, bins=20)\n",
    "        ax[0].set_xlim([0,1])\n",
    "        ax[0].set_title(f\"{metric} score \\n FG vs BG\", fontsize=8)\n",
    "        ax[1].hist(scores_penumbra, bins=20)\n",
    "        ax[1].set_xlim([0,1])\n",
    "        ax[1].set_title(f\"{metric} score \\n Penumbra\", fontsize=8)\n",
    "        ax[2].hist(scores_umbra, bins=20)\n",
    "        ax[2].set_xlim([0,1])\n",
    "        ax[2].set_title(f\"{metric} score \\n Umbra\", fontsize=8)\n",
    "        \n",
    "def show_bins2(seg_type, bins, fg_bg, penumbra, umbra, metric='DICE'):\n",
    "    for i in range(len(bins)):\n",
    "        b = bins[i]\n",
    "\n",
    "        scores_fg_bg = [item['value'] for item in fg_bg[i][metric] if not item['details']['misdetected']]\n",
    "        misdetections_fg_bg = [item['value'] for item in fg_bg[i][metric] if item['details']['misdetected']]\n",
    "#         print(misdetections_fg_bg)\n",
    "        scores_penumbra = [item['value'] for item in penumbra[i][metric]]\n",
    "        scores_umbra = [item['value'] for item in umbra[i][metric]]\n",
    "\n",
    "        fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(10, 5))  \n",
    "        fig.suptitle(f\"{seg_type}  : Sunspots of [{b[0]},{b[1]}] pixels\")\n",
    "#         ax[0].hist(misdetections_fg_bg, bins=1, color='r')\n",
    "        ax[0].add_patch(Rectangle((-0.05, 0), 0.05, len(misdetections_fg_bg), color='r'))\n",
    "        ax[0].hist(scores_fg_bg, bins=20)\n",
    "        ax[0].set_xlim([-0.1,1])\n",
    "        ax[0].set_title(f\"{metric} score \\n FG vs BG\", fontsize=8)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_scores_dict(gt_label, pred_label, classes):\n",
    "    ###### Separate penumbra and umbra classes\n",
    "    \n",
    "#     perClassCM = conf_matrix(gt_label.flatten(),pred_label.flatten(), labels=fg_classes)\n",
    "    perClassCM = conf_matrix(gt_label.flatten(),pred_label.flatten(), labels=fg_classes)\n",
    "    \n",
    "#     perClassCM = conf_matrix(gt_label,pred_label, labels=fg_classes)\n",
    "    CM_dict = {c: perClassCM[i] for i,c in enumerate(fg_classes)}\n",
    "    \n",
    "    IoU = [ CM_dict[k][1,1] / ( CM_dict[k][1,1] + CM_dict[k][0,1] + CM_dict[k][1,0] + 1e-5) for k in CM_dict]\n",
    "#     print(\"IoU\",IoU)\n",
    "    \n",
    "    DSC = [ 2*CM_dict[k][1,1] / ( 2*CM_dict[k][1,1] + CM_dict[k][0,1] + CM_dict[k][1,0] + 1e-5) for k in CM_dict]\n",
    "#     print(\"DSC\",DSC)\n",
    "        \n",
    "    scores_dict = {c: {\"DICE\": DSC[i], \"IoU\": IoU[i]} for i,c in enumerate(classes)}\n",
    "#     scores_dict[\"foreground\"]= {\"DICE\": DSC_fg_bg[0], \"IoU\": IoU_fg_bg[0]}\n",
    "    \n",
    "    return scores_dict\n",
    "\n",
    "def find_bboxes2(labels):\n",
    "    props_labels = regionprops(labels)\n",
    "    \n",
    "    bboxes = []\n",
    "    masks = []\n",
    "    for prop in props_labels:\n",
    "#         if prop.area > 15:\n",
    "        bbox = np.array(prop.bbox)\n",
    "        bboxes.append(bbox)\n",
    "        masks.append(prop.image)\n",
    "\n",
    "    return bboxes , masks\n",
    "\n",
    "def is_false_detection(det_msk, gt_label):\n",
    "    intersection = (gt_label*det_msk).sum()\n",
    "    return intersection == 0.0\n",
    "\n",
    "def compute_false_positives_OLD(image_lst, gen_lst, device='cpu', bins=[], all_classes={'foreground':[1,2], 'penumbra':[1], 'umbra':[2]}):\n",
    "#     all_classes = {'foreground':[1,2], 'penumbra':[1], 'umbra':[2]}\n",
    "    \n",
    "    bin2FP_dict = {c: {str(i): {'num_false_positives': 0,\n",
    "                             'total_pixel_area': 0, \n",
    "                             \"false_positives\":[],\n",
    "                             \"true_positives\":[],\n",
    "                            }\n",
    "                             for i in range(len(bins))}\n",
    "                     for c,ids in all_classes.items()\n",
    "                    }\n",
    "    \n",
    "    for i in tqdm(range(len(image_lst)),leave=False):\n",
    "        basename = os.path.splitext(os.path.basename(image_lst[i]))[0]\n",
    "\n",
    "        test_img = imread(image_lst[i])\n",
    "        gt_label = imread(gt_lst[i])\n",
    "        candidate_label = imread(gen_lst[i])\n",
    "        \n",
    "        for c_name, c_ids in (pbar :=tqdm(all_classes.items(), leave=False)): # foreground, penumbra, umbra\n",
    "                pbar.set_description(f\"{c_name}\")\n",
    "                                     \n",
    "                # Get the binary version of the masks \n",
    "                gt_class_fg, candidate_class_fg = get_class_binary_mask(gt_label, candidate_label, c_ids)\n",
    "\n",
    "                # Get the regions (connected components) properties of masks\n",
    "                label_im = label(gt_class_fg)\n",
    "                props_bboxes, props_masks = find_bboxes2(label_im)\n",
    "\n",
    "                label_gen = label(candidate_class_fg)\n",
    "                props_bboxes_gen, props_masks_gen = find_bboxes2(label_gen)\n",
    "\n",
    "                # Create isolated masks for each connected component\n",
    "                whole_m2 = np.array([np.zeros_like(gt_class_fg) for item in props_masks_gen])\n",
    "                for idx, b in enumerate(props_bboxes_gen):\n",
    "                    whole_m2[idx][b[0]:b[2], b[1]:b[3]] = props_masks_gen[idx]\n",
    "                                     \n",
    "                for j in tqdm(range(len(props_bboxes_gen[:])),leave=False):\n",
    "                    bbox = props_bboxes_gen[j]\n",
    "\n",
    "                    cur_m_gen = whole_m2[j]\n",
    "                    m_gen_area = np.sum(props_masks_gen[j])\n",
    "\n",
    "\n",
    "                    cur_bin = -1\n",
    "                    for b in bins:\n",
    "                        if (b[0] <= m_gen_area ) and (b[1]>=m_gen_area):\n",
    "                            cur_bin = bins.index(b)\n",
    "                    \n",
    "                    is_false = is_false_detection(cur_m_gen, gt_class_fg)\n",
    "            \n",
    "                    details = {\"image\": basename, 'bbox': bbox, 'false_positive': is_false, \"area\": m_gen_area }\n",
    "\n",
    "                    if is_false:\n",
    "                        bin2FP_dict[c_name][str(cur_bin)]['num_false_positives'] += 1\n",
    "                        bin2FP_dict[c_name][str(cur_bin)]['total_pixel_area'] += m_gen_area\n",
    "\n",
    "                        bin2FP_dict[c_name][str(cur_bin)]['false_positives'].append(details)\n",
    "                    else:\n",
    "                        bin2FP_dict[c_name][str(cur_bin)]['true_positives'].append(details)\n",
    "            \n",
    "    return bin2FP_dict\n",
    "\n",
    "\n",
    "def compute_false_positives_img(i, image_lst, gt_lst, gen_lst, all_classes, bins, conf_thresh=.5):\n",
    "    basename = os.path.splitext(os.path.basename(image_lst[i]))[0]\n",
    "    \n",
    "    test_b = basename\n",
    "    gt_b = os.path.splitext(os.path.basename(gt_lst[i]))[0]\n",
    "    gen_b = os.path.splitext(os.path.basename(gen_lst[i]))[0]\n",
    "\n",
    "    is_proba_map = gen_b.endswith('_proba_map')\n",
    "    if is_proba_map:\n",
    "        gen_b = gen_b[:-len('_proba_map')]\n",
    "\n",
    "    assert test_b == gt_b == gen_b\n",
    "\n",
    "    test_img = imread(image_lst[i])\n",
    "    gt_label = imread(gt_lst[i])\n",
    "    if is_proba_map:\n",
    "        # Get the binary version of the candidate label mask\n",
    "        candidate_label = np.load(gen_lst[i])\n",
    "        candidate_label = (candidate_label > conf_thresh).astype(int)\n",
    "    else:\n",
    "        candidate_label = imread(gen_lst[i])\n",
    "\n",
    "    \n",
    "    \n",
    "    cur_FP_dict = {c:{str(b_i):{'num_false_positives': 0,\n",
    "                             'total_pixel_area': 0, \n",
    "                             \"false_positives\":[],\n",
    "                             \"true_positives\":[],\n",
    "                            } for b_i in range(len(bins))} for c in all_classes}\n",
    "\n",
    "    for c_name, c_ids in (pbar :=tqdm(all_classes.items(), leave=False)): # foreground, penumbra, umbra\n",
    "        pbar.set_description(f\"{c_name}\")\n",
    "\n",
    "        # Get the binary version of the masks \n",
    "        gt_class_fg, candidate_class_fg = get_class_binary_mask(gt_label, candidate_label, c_ids)\n",
    "\n",
    "        # Get the regions (connected components) properties of masks\n",
    "        label_im = label(gt_class_fg)\n",
    "        props_bboxes, props_masks = find_bboxes2(label_im)\n",
    "\n",
    "        label_gen = label(candidate_class_fg)\n",
    "        props_bboxes_gen, props_masks_gen = find_bboxes2(label_gen)\n",
    "\n",
    "        # Create isolated masks for each connected component\n",
    "        whole_m2 = np.array([np.zeros_like(gt_class_fg) for item in props_masks_gen])\n",
    "        for idx, b in enumerate(props_bboxes_gen):\n",
    "            whole_m2[idx][b[0]:b[2], b[1]:b[3]] = props_masks_gen[idx]\n",
    "\n",
    "        for j in tqdm(range(len(props_bboxes_gen[:])),leave=False):\n",
    "            bbox = props_bboxes_gen[j]\n",
    "\n",
    "            cur_m_gen = whole_m2[j]\n",
    "            m_gen_area = np.sum(props_masks_gen[j])\n",
    "\n",
    "\n",
    "            cur_bin = -1\n",
    "            for b in bins:\n",
    "                if (b[0] <= m_gen_area ) and (b[1]>=m_gen_area):\n",
    "                    cur_bin = bins.index(b)\n",
    "\n",
    "            is_false = is_false_detection(cur_m_gen, gt_class_fg)\n",
    "\n",
    "            details = {\"image\": basename, 'bbox': bbox, 'false_positive': is_false, \"area\": m_gen_area }\n",
    "\n",
    "            if is_false:\n",
    "#                 bin2FP_dict[c_name][str(cur_bin)]['num_false_positives'] += 1\n",
    "#                 bin2FP_dict[c_name][str(cur_bin)]['total_pixel_area'] += m_gen_area\n",
    "#                 bin2FP_dict[c_name][str(cur_bin)]['false_positives'].append(details)\n",
    "                \n",
    "                cur_FP_dict[c_name][str(cur_bin)]['num_false_positives'] += 1\n",
    "                cur_FP_dict[c_name][str(cur_bin)]['total_pixel_area'] += m_gen_area\n",
    "                cur_FP_dict[c_name][str(cur_bin)]['false_positives'].append(details)\n",
    "                \n",
    "                \n",
    "            else:\n",
    "#                 bin2FP_dict[c_name][str(cur_bin)]['true_positives'].append(details)\n",
    "                cur_FP_dict[c_name][str(cur_bin)]['true_positives'].append(details)\n",
    "                \n",
    "    return cur_FP_dict\n",
    "    \n",
    "\n",
    "\n",
    "def compute_false_positives(image_lst, gen_lst, device='cpu', bins=[], all_classes={'foreground':[1,2], 'penumbra':[1], 'umbra':[2]}):\n",
    "#     all_classes = {'foreground':[1,2], 'penumbra':[1], 'umbra':[2]}\n",
    "#     print(\"here\")\n",
    "    bin2FP_dict = {c: {str(i): {'num_false_positives': 0,\n",
    "                             'total_pixel_area': 0, \n",
    "                             \"false_positives\":[],\n",
    "                             \"true_positives\":[],\n",
    "                            }\n",
    "                             for i in range(len(bins))}\n",
    "                     for c,ids in all_classes.items()\n",
    "                    }\n",
    "    \n",
    "    \n",
    "    num_cpu = multiprocessing.cpu_count()\n",
    "    with concurrent.futures.ProcessPoolExecutor(max_workers=int(np.floor(0.9*num_cpu))) as executor:\n",
    "        \n",
    "        for cur_FP_dict in tqdm(executor.map(compute_false_positives_img, \n",
    "                                                  range(len(image_lst)),\n",
    "                                                  repeat(image_lst), repeat(gt_lst), repeat(gen_lst), \n",
    "                                                  repeat(all_classes), repeat(bins))):\n",
    "            for c in all_classes:\n",
    "                for b in range(len(bins)):\n",
    "                    bin2FP_dict[c][str(b)][\"num_false_positives\"] += cur_FP_dict[c][str(b)][\"num_false_positives\"]\n",
    "                    bin2FP_dict[c][str(b)][\"total_pixel_area\"] += cur_FP_dict[c][str(b)][\"total_pixel_area\"]\n",
    "                    bin2FP_dict[c][str(b)][\"false_positives\"].extend(cur_FP_dict[c][str(b)][\"false_positives\"])\n",
    "                    bin2FP_dict[c][str(b)][\"true_positives\"].extend(cur_FP_dict[c][str(b)][\"true_positives\"])\n",
    "\n",
    "def compute_false_positives2(image_lst, gen_lst, device='cpu', bins=[], all_classes={'foreground':[1,2], 'penumbra':[1], 'umbra':[2]}, conf_thresh=0.5):\n",
    "#     all_classes = {'foreground':[1,2], 'penumbra':[1], 'umbra':[2]}\n",
    "#     print(\"here\")\n",
    "    bin2FP_dict = {c: {str(i): {'num_false_positives': 0,\n",
    "                             'total_pixel_area': 0, \n",
    "                             \"false_positives\":[],\n",
    "                             \"true_positives\":[],\n",
    "                            }\n",
    "                             for i in range(len(bins))}\n",
    "                     for c,ids in all_classes.items()\n",
    "                    }\n",
    "    \n",
    "    for idx in tqdm(range(len(image_lst))):\n",
    "        cur_FP_dict = compute_false_positives_img (idx, image_lst, gt_lst, gen_lst, all_classes, bins, conf_thresh)\n",
    "        for c in all_classes:\n",
    "                for b in range(len(bins)):\n",
    "                    bin2FP_dict[c][str(b)][\"num_false_positives\"] += cur_FP_dict[c][str(b)][\"num_false_positives\"]\n",
    "                    bin2FP_dict[c][str(b)][\"total_pixel_area\"] += cur_FP_dict[c][str(b)][\"total_pixel_area\"]\n",
    "                    bin2FP_dict[c][str(b)][\"false_positives\"].extend(cur_FP_dict[c][str(b)][\"false_positives\"])\n",
    "                    bin2FP_dict[c][str(b)][\"true_positives\"].extend(cur_FP_dict[c][str(b)][\"true_positives\"])\n",
    "                       \n",
    "    return bin2FP_dict\n",
    "\n",
    "    \n",
    "  \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest_in_pred2(gt_m, gen_m_lst):\n",
    "\n",
    "    intersections = np.repeat(gt_m[None,:,:], gen_m_lst.shape[0], axis=0)*gen_m_lst   \n",
    "\n",
    "    sum_intersections = np.sum(intersections, axis=(1,2))\n",
    "\n",
    "    top_idx = np.argmax(sum_intersections)\n",
    "    top = np.max(sum_intersections)\n",
    "   \n",
    "    out_mask = None if top_idx == -1 else gen_m_lst[top_idx]\n",
    "\n",
    "    return out_mask, top, top_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gt_bin_sizes(image_lst, gt_lst, bins=[]):\n",
    "    binned_sunspots = {i: [] for i in range(len(bins))}\n",
    "    \n",
    "    for i in tqdm(range(len(image_lst)),leave=False):\n",
    "        basename = os.path.splitext(os.path.basename(image_lst[i]))[0]\n",
    "\n",
    "#         test_img = imread(image_lst[i])\n",
    "        gt_label = imread(gt_lst[i])\n",
    "        \n",
    "        gt_label_fg_bg = gt_label.copy()\n",
    "        gt_label_fg_bg[gt_label_fg_bg>0] = 1\n",
    "        \n",
    "        label_im = label(gt_label_fg_bg)\n",
    "        props_bboxes, props_masks = find_bboxes2(label_im)\n",
    "        \n",
    "        for j in tqdm(range(len(props_bboxes[:])),leave=False):\n",
    "            bbox = props_bboxes[j]\n",
    "            m = props_masks[j]\n",
    "            \n",
    "            area = np.sum(m)\n",
    "            \n",
    "            cur_bin = -1\n",
    "            for b in bins:\n",
    "                if (b[0] <= area ) and (b[1]>=area):\n",
    "                    cur_bin = bins.index(b)\n",
    "            \n",
    "            details = {\"image\": basename, 'bbox': bbox}\n",
    "            \n",
    "            binned_sunspots[cur_bin].append(details)\n",
    "    return binned_sunspots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binning_sunspots3_OLD(image_lst, gen_lst, device='cpu', bins =[], all_classes={'foreground':[1,2], 'penumbra':[1], 'umbra':[2]}):\n",
    "\n",
    "#     bin2bbox_dict_umbra = {i: {'DICE':[], 'IoU':[]} for i in range(len(bins))}\n",
    "#     bin2bbox_dict_penumbra = {i: {'DICE':[], 'IoU':[]} for i in range(len(bins))}\n",
    "#     bin2bbox_dict_fg_bg = {i: {'DICE':[], 'IoU':[]} for i in range(len(bins))}\n",
    "\n",
    "    bin2bbox_dict =  {c: {str(i): {'DICE':[], 'IoU':[]} for i in range(len(bins))}\n",
    "                         for c,ids in all_classes.items()}\n",
    "    \n",
    "    for i in tqdm(range(len(image_lst)),leave=False):\n",
    "        basename = os.path.splitext(os.path.basename(image_lst[i]))[0]\n",
    "        test_b = basename\n",
    "        gt_b = os.path.splitext(os.path.basename(gt_lst[i]))[0]\n",
    "        gen_b = os.path.splitext(os.path.basename(gen_lst[i]))[0]\n",
    "        assert test_b == gt_b == gen_b\n",
    "        \n",
    "        test_img = imread(image_lst[i])\n",
    "        gt_label = imread(gt_lst[i])\n",
    "        candidate_label = imread(gen_lst[i])\n",
    "        \n",
    "        for c_name, c_ids in (pbar :=tqdm(all_classes.items(), leave=False)): # foreground, penumbra, umbra\n",
    "            pbar.set_description(f\"{c_name}\")\n",
    "            \n",
    "            # Get the binary version of the masks \n",
    "            gt_class_fg, candidate_class_fg = get_class_binary_mask(gt_label, candidate_label, c_ids)\n",
    "            \n",
    "            # Get the regions (connected components) properties of masks\n",
    "            label_im = label(gt_class_fg)\n",
    "            props_bboxes, props_masks = find_bboxes2(label_im)\n",
    "\n",
    "            label_gen = label(candidate_class_fg)\n",
    "            props_bboxes_gen, props_masks_gen = find_bboxes2(label_gen)\n",
    "        \n",
    "            # Create isolated masks for each connected component\n",
    "            whole_m2 = np.array([np.zeros_like(gt_class_fg) for item in props_masks_gen])\n",
    "            for idx, b in enumerate(props_bboxes_gen):\n",
    "                whole_m2[idx][b[0]:b[2], b[1]:b[3]] = props_masks_gen[idx]\n",
    "            \n",
    "            # Loop over all regions\n",
    "            for j in tqdm(range(len(props_bboxes[:])),leave=False):\n",
    "                bbox = props_bboxes[j]\n",
    "                m = props_masks[j]\n",
    "\n",
    "                area = np.sum(m)\n",
    "\n",
    "                minX,maxX = max(bbox[0]-padding, 0), min( bbox[2]+padding, test_img.shape[0])\n",
    "                minY,maxY = max(bbox[1]-padding, 0), min( bbox[3]+padding, test_img.shape[1])\n",
    "\n",
    "                bbox2 = [minX,minY,maxX,maxY]\n",
    "                \n",
    "                # Identify the bins the region belongs to.\n",
    "                cur_bin = -1\n",
    "                for b in bins:\n",
    "                    if (b[0] <= area ) and (b[1]>=area):\n",
    "                        cur_bin = bins.index(b)\n",
    "                        \n",
    "                whole_m = np.zeros_like(gt_class_fg)\n",
    "                whole_m[bbox[0]:bbox[2], bbox[1]:bbox[3]] = m\n",
    "                \n",
    "                if whole_m2.shape[0] != 0: # generated mask is not empty\n",
    "                    # Find closest region in prediction/generated mask\n",
    "                    m_closest_in_gen, match_intersection, match_idx = find_closest_in_pred2(\n",
    "                                                                            whole_m, whole_m2)\n",
    "                    if m_closest_in_gen is not None:\n",
    "                        # Compute metric scores between gt and mask\n",
    "                        union = np.logical_or(m_closest_in_gen, whole_m)\n",
    "                        label_union = label(union)\n",
    "                        union_b = regionprops(label_union)[0].bbox\n",
    "\n",
    "                        reduced_m = whole_m[union_b[0]:union_b[2],\n",
    "                                                          union_b[1]:union_b[3] ]\n",
    "                        reduced_closest = m_closest_in_gen[union_b[0]:union_b[2],\n",
    "                                                          union_b[1]:union_b[3] ]\n",
    "\n",
    "                        score_dict_fg_bg = get_scores_dict(reduced_m, reduced_closest, [1]) \n",
    "#                         print(score_dict_fg_bg)\n",
    "                        \n",
    "#                        cur_score_dict = {k:v for k,v in score_dict.items()}\n",
    "                        \n",
    "                        cur_score_dict = {c_name:score_dict_fg_bg[1]}\n",
    "                        \n",
    "                    else: # No corresponding detection\n",
    "                        cur_score_dict = {c_name: {\"DICE\": 0.0, \"IoU\": 0.0}}\n",
    "\n",
    "                    details = {\"image\": basename, 'bbox': bbox2, 'misdetected': match_intersection == 0.0 }\n",
    "\n",
    "                else:\n",
    "                    cur_score_dict = {c_name: {\"DICE\": 0.0, \"IoU\": 0.0}}\n",
    "                    details = {\"image\": basename, 'bbox': bbox2, 'misdetected': True }\n",
    "                    \n",
    "                    \n",
    "                bin2bbox_dict[c_name][str(cur_bin)]['DICE'].append({'value': cur_score_dict[c_name]['DICE'],\n",
    "                                                               'details': details})\n",
    "                bin2bbox_dict[c_name][str(cur_bin)]['IoU'].append({'value': cur_score_dict[c_name]['IoU'],\n",
    "                                                           'details': details})\n",
    " \n",
    "    return     bin2bbox_dict\n",
    "\n",
    "\n",
    "def get_class_binary_mask(gt_mask, candidate_mask ,class_ids):\n",
    "    gt_label_class=np.zeros_like(gt_mask)\n",
    "    for i in class_ids:\n",
    "        gt_label_class[gt_mask == i] = 1\n",
    "    \n",
    "    candidate_label_class=np.zeros_like(gt_mask)\n",
    "    for i in class_ids:\n",
    "        candidate_label_class[candidate_mask == i] = 1\n",
    "    \n",
    "    return gt_label_class, candidate_label_class\n",
    "\n",
    "\n",
    "def binning_sunspot_img_OLD(i, image_lst, gt_lst, gen_lst, all_classes, bins):\n",
    "    basename = os.path.splitext(os.path.basename(image_lst[i]))[0]\n",
    "    test_b = basename\n",
    "    gt_b = os.path.splitext(os.path.basename(gt_lst[i]))[0]\n",
    "    gen_b = os.path.splitext(os.path.basename(gen_lst[i]))[0]\n",
    "    assert test_b == gt_b == gen_b\n",
    "\n",
    "    test_img = imread(image_lst[i])\n",
    "    gt_label = imread(gt_lst[i])\n",
    "    candidate_label = imread(gen_lst[i])\n",
    "    \n",
    "    cur_DICE_IoU_dict = {c:{str(b_i):{\"DICE\":[],\"IoU\":[]} for b_i in range(len(bins))} for c in all_classes}\n",
    "\n",
    "    for c_name, c_ids in (pbar :=tqdm(all_classes.items(), leave=False)): # foreground, penumbra, umbra\n",
    "        pbar.set_description(f\"{c_name}\")\n",
    "\n",
    "        # Get the binary version of the masks \n",
    "        gt_class_fg, candidate_class_fg = get_class_binary_mask(gt_label, candidate_label, c_ids)\n",
    "\n",
    "        # Get the regions (connected components) properties of masks\n",
    "        label_im = label(gt_class_fg)\n",
    "        props_bboxes, props_masks = find_bboxes2(label_im)\n",
    "\n",
    "        label_gen = label(candidate_class_fg)\n",
    "        props_bboxes_gen, props_masks_gen = find_bboxes2(label_gen)\n",
    "\n",
    "        # Create isolated masks for each connected component\n",
    "        whole_m2 = np.array([np.zeros_like(gt_class_fg) for item in props_masks_gen])\n",
    "        for idx, b in enumerate(props_bboxes_gen):\n",
    "            whole_m2[idx][b[0]:b[2], b[1]:b[3]] = props_masks_gen[idx]\n",
    "\n",
    "        # Loop over all regions\n",
    "        for j in tqdm(range(len(props_bboxes[:])),leave=False):\n",
    "            bbox = props_bboxes[j]\n",
    "            m = props_masks[j]\n",
    "\n",
    "            area = np.sum(m)\n",
    "\n",
    "            minX,maxX = max(bbox[0]-padding, 0), min( bbox[2]+padding, test_img.shape[0])\n",
    "            minY,maxY = max(bbox[1]-padding, 0), min( bbox[3]+padding, test_img.shape[1])\n",
    "\n",
    "            bbox2 = [minX,minY,maxX,maxY]\n",
    "\n",
    "            # Identify the bins the region belongs to.\n",
    "            cur_bin = -1\n",
    "            for b in bins:\n",
    "                if (b[0] <= area ) and (b[1]>=area):\n",
    "                    cur_bin = bins.index(b)\n",
    "\n",
    "            whole_m = np.zeros_like(gt_class_fg)\n",
    "            whole_m[bbox[0]:bbox[2], bbox[1]:bbox[3]] = m\n",
    "\n",
    "            if whole_m2.shape[0] != 0: # generated mask is not empty\n",
    "                # Find closest region in prediction/generated mask\n",
    "                m_closest_in_gen, match_intersection, match_idx = find_closest_in_pred2(\n",
    "                                                                        whole_m, whole_m2)\n",
    "                if m_closest_in_gen is not None:\n",
    "                    # Compute metric scores between gt and mask\n",
    "                    union = np.logical_or(m_closest_in_gen, whole_m)\n",
    "                    label_union = label(union)\n",
    "                    union_b = regionprops(label_union)[0].bbox\n",
    "\n",
    "                    reduced_m = whole_m[union_b[0]:union_b[2],\n",
    "                                                      union_b[1]:union_b[3] ]\n",
    "                    reduced_closest = m_closest_in_gen[union_b[0]:union_b[2],\n",
    "                                                      union_b[1]:union_b[3] ]\n",
    "\n",
    "                    score_dict_fg_bg = get_scores_dict(reduced_m, reduced_closest, [1]) \n",
    "#                         print(score_dict_fg_bg)\n",
    "\n",
    "#                        cur_score_dict = {k:v for k,v in score_dict.items()}\n",
    "\n",
    "                    cur_score_dict = {c_name:score_dict_fg_bg[1]}\n",
    "\n",
    "                else: # No corresponding detection\n",
    "                    cur_score_dict = {c_name: {\"DICE\": 0.0, \"IoU\": 0.0}}\n",
    "\n",
    "                details = {\"image\": basename, 'bbox': bbox2, 'misdetected': match_intersection == 0.0 }\n",
    "\n",
    "            else:\n",
    "                cur_score_dict = {c_name: {\"DICE\": 0.0, \"IoU\": 0.0}}\n",
    "                details = {\"image\": basename, 'bbox': bbox2, 'misdetected': True }\n",
    "\n",
    "\n",
    "#             bin2bbox_dict[c_name][str(cur_bin)]['DICE'].append({'value': cur_score_dict[c_name]['DICE'],\n",
    "#                                                            'details': details})\n",
    "#             bin2bbox_dict[c_name][str(cur_bin)]['IoU'].append({'value': cur_score_dict[c_name]['IoU'],\n",
    "#                                                        'details': details})\n",
    "            cur_DICE_IoU_dict[c_name][str(cur_bin)]['DICE'].append({'value': cur_score_dict[c_name]['DICE'],\n",
    "                                                       'details': details})\n",
    "            cur_DICE_IoU_dict[c_name][str(cur_bin)]['IoU'].append({'value': cur_score_dict[c_name]['IoU'],\n",
    "                                                       'details': details})\n",
    "    return cur_DICE_IoU_dict  \n",
    "\n",
    "def binning_sunspot_img(i, image_lst, gt_lst, gen_lst, all_classes, bins, conf_thresh=.5):\n",
    "    basename = os.path.splitext(os.path.basename(image_lst[i]))[0]\n",
    "    test_b = basename\n",
    "    gt_b = os.path.splitext(os.path.basename(gt_lst[i]))[0]\n",
    "    gen_b = os.path.splitext(os.path.basename(gen_lst[i]))[0]\n",
    "\n",
    "    is_proba_map = gen_b.endswith('_proba_map')\n",
    "    if is_proba_map:\n",
    "        gen_b = gen_b[:-len('_proba_map')]\n",
    "\n",
    "    # print(test_b,'\\n', gt_b,'\\n', gen_b)\n",
    "\n",
    "    assert test_b == gt_b == gen_b\n",
    "\n",
    "    test_img = imread(image_lst[i])\n",
    "    gt_label = imread(gt_lst[i])\n",
    "    if is_proba_map:\n",
    "        # Get the binary version of the candidate label mask\n",
    "        candidate_label = np.load(gen_lst[i])\n",
    "        candidate_label = (candidate_label > conf_thresh).astype(int)\n",
    "    else:\n",
    "        candidate_label = imread(gen_lst[i])\n",
    "\n",
    "    # assert test_b == gt_b == gen_b\n",
    "\n",
    "    # test_img = imread(image_lst[i])\n",
    "    # gt_label = imread(gt_lst[i])\n",
    "    # candidate_label = imread(gen_lst[i])\n",
    "    \n",
    "    cur_DICE_IoU_dict = {c:{str(b_i):{\"DICE\":[],\"IoU\":[]} for b_i in range(len(bins))} for c in all_classes}\n",
    "\n",
    "    for c_name, c_ids in (pbar :=tqdm(all_classes.items(), leave=False)): # foreground, penumbra, umbra\n",
    "        pbar.set_description(f\"{c_name}\")\n",
    "\n",
    "        # Get the binary version of the masks \n",
    "        gt_class_fg, candidate_class_fg = get_class_binary_mask(gt_label, candidate_label, c_ids)\n",
    "\n",
    "        # Get the regions (connected components) properties of masks\n",
    "        label_im = label(gt_class_fg)\n",
    "        props_bboxes, props_masks = find_bboxes2(label_im)\n",
    "\n",
    "        label_gen = label(candidate_class_fg)\n",
    "        props_bboxes_gen, props_masks_gen = find_bboxes2(label_gen)\n",
    "\n",
    "        # Create isolated masks for each connected component\n",
    "        whole_m2 = np.array([np.zeros_like(gt_class_fg) for item in props_masks_gen])\n",
    "        for idx, b in enumerate(props_bboxes_gen):\n",
    "            whole_m2[idx][b[0]:b[2], b[1]:b[3]] = props_masks_gen[idx]\n",
    "\n",
    "        # Loop over all regions\n",
    "        for j in tqdm(range(len(props_bboxes[:])),leave=False):\n",
    "            bbox = props_bboxes[j]\n",
    "            m = props_masks[j]\n",
    "\n",
    "            area = np.sum(m)\n",
    "\n",
    "            minX,maxX = max(bbox[0]-padding, 0), min( bbox[2]+padding, test_img.shape[0])\n",
    "            minY,maxY = max(bbox[1]-padding, 0), min( bbox[3]+padding, test_img.shape[1])\n",
    "\n",
    "            bbox2 = [minX,minY,maxX,maxY]\n",
    "\n",
    "            # Identify the bins the region belongs to.\n",
    "            cur_bin = -1\n",
    "            for b in bins:\n",
    "                if (b[0] <= area ) and (b[1]>=area):\n",
    "                    cur_bin = bins.index(b)\n",
    "\n",
    "            whole_m = np.zeros_like(gt_class_fg)\n",
    "            whole_m[bbox[0]:bbox[2], bbox[1]:bbox[3]] = m\n",
    "\n",
    "            if whole_m2.shape[0] != 0: # generated mask is not empty\n",
    "                # Find closest region in prediction/generated mask\n",
    "                m_closest_in_gen, match_intersection, match_idx = find_closest_in_pred2(\n",
    "                                                                        whole_m, whole_m2)\n",
    "                if m_closest_in_gen is not None:\n",
    "                    # Compute metric scores between gt and mask\n",
    "                    union = np.logical_or(m_closest_in_gen, whole_m)\n",
    "                    label_union = label(union)\n",
    "                    union_b = regionprops(label_union)[0].bbox\n",
    "\n",
    "                    reduced_m = whole_m[union_b[0]:union_b[2],\n",
    "                                                      union_b[1]:union_b[3] ]\n",
    "                    reduced_closest = m_closest_in_gen[union_b[0]:union_b[2],\n",
    "                                                      union_b[1]:union_b[3] ]\n",
    "\n",
    "                    score_dict_fg_bg = get_scores_dict(reduced_m, reduced_closest, [1]) \n",
    "#                         print(score_dict_fg_bg)\n",
    "\n",
    "#                        cur_score_dict = {k:v for k,v in score_dict.items()}\n",
    "\n",
    "                    cur_score_dict = {c_name:score_dict_fg_bg[1]}\n",
    "\n",
    "                else: # No corresponding detection\n",
    "                    cur_score_dict = {c_name: {\"DICE\": 0.0, \"IoU\": 0.0}}\n",
    "\n",
    "                details = {\"image\": basename, 'bbox': bbox2, 'misdetected': match_intersection == 0.0 }\n",
    "\n",
    "            else:\n",
    "                cur_score_dict = {c_name: {\"DICE\": 0.0, \"IoU\": 0.0}}\n",
    "                details = {\"image\": basename, 'bbox': bbox2, 'misdetected': True }\n",
    "\n",
    "            cur_DICE_IoU_dict[c_name][str(cur_bin)]['DICE'].append({'value': cur_score_dict[c_name]['DICE'],\n",
    "                                                       'details': details})\n",
    "            cur_DICE_IoU_dict[c_name][str(cur_bin)]['IoU'].append({'value': cur_score_dict[c_name]['IoU'],\n",
    "                                                       'details': details})\n",
    "    return cur_DICE_IoU_dict  \n",
    "    \n",
    "\n",
    "def binning_sunspots3(image_lst, gen_lst, device='cpu', bins =[], all_classes={'foreground':[1,2], 'penumbra':[1], 'umbra':[2]}, conf_thresh=.5):\n",
    "    bin2bbox_dict =  {c: {str(i): {'DICE':[], 'IoU':[]} for i in range(len(bins))}\n",
    "                         for c,ids in all_classes.items()}\n",
    "    \n",
    "    num_cpu = multiprocessing.cpu_count()\n",
    "    with concurrent.futures.ProcessPoolExecutor(max_workers=int(np.floor(0.9*num_cpu))) as executor:\n",
    "        \n",
    "        for cur_img_bin_dict in tqdm(executor.map(binning_sunspot_img, \n",
    "                                                  range(len(image_lst)),\n",
    "                                                  repeat(image_lst), repeat(gt_lst), repeat(gen_lst), \n",
    "                                                  repeat(all_classes), repeat(bins),\n",
    "                                                  repeat(conf_thresh))):\n",
    "            for c in all_classes:\n",
    "                for b in range(len(bins)):\n",
    "                    bin2bbox_dict[c][str(b)][\"DICE\"].extend(cur_img_bin_dict[c][str(b)][\"DICE\"])\n",
    "                    bin2bbox_dict[c][str(b)][\"IoU\"].extend(cur_img_bin_dict[c][str(b)][\"IoU\"])\n",
    "        \n",
    "        \n",
    "#         for i in tqdm(range(len(image_lst)),leave=False):\n",
    "            \n",
    "    return     bin2bbox_dict\n",
    "\n",
    "def binning_sunspots4(image_lst, gen_lst, device='cpu', bins =[], all_classes={'foreground':[1,2], 'penumbra':[1], 'umbra':[2]}, conf_thresh=.5):\n",
    "    bin2bbox_dict =  {c: {str(i): {'DICE':[], 'IoU':[]} for i in range(len(bins))}\n",
    "                         for c,ids in all_classes.items()}\n",
    "    \n",
    "\n",
    "    for idx in tqdm(range(len(image_lst))):\n",
    "        cur_img_bin_dict = binning_sunspot_img(idx, image_lst, gt_lst, gen_lst, all_classes, bins, conf_thresh)\n",
    "        for c in all_classes:\n",
    "            for b in range(len(bins)):\n",
    "                bin2bbox_dict[c][str(b)][\"DICE\"].extend(cur_img_bin_dict[c][str(b)][\"DICE\"])\n",
    "                bin2bbox_dict[c][str(b)][\"IoU\"].extend(cur_img_bin_dict[c][str(b)][\"IoU\"])\n",
    "        \n",
    "    \n",
    "#     num_cpu = multiprocessing.cpu_count()\n",
    "#     with concurrent.futures.ProcessPoolExecutor(max_workers=int(np.floor(0.9*num_cpu))) as executor:\n",
    "        \n",
    "#         for cur_img_bin_dict in tqdm(executor.map(binning_sunspot_img, \n",
    "#                                                   range(len(image_lst)),\n",
    "#                                                   repeat(image_lst), repeat(gt_lst), repeat(gen_lst), \n",
    "#                                                   repeat(all_classes), repeat(bins),\n",
    "#                                                   repeat(conf_thresh))):\n",
    "#             for c in all_classes:\n",
    "#                 for b in range(len(bins)):\n",
    "#                     bin2bbox_dict[c][str(b)][\"DICE\"].extend(cur_img_bin_dict[c][str(b)][\"DICE\"])\n",
    "#                     bin2bbox_dict[c][str(b)][\"IoU\"].extend(cur_img_bin_dict[c][str(b)][\"IoU\"])\n",
    "        \n",
    "        \n",
    "# #         for i in tqdm(range(len(image_lst)),leave=False):\n",
    "            \n",
    "    return     bin2bbox_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padding = 5\n",
    "fg_classes = [1]\n",
    "bins = [[1,50],[51,100],[101,400],[401,800],[801,1200], [1201,2000], [2000,3000], [3000,np.inf]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binned_gt = get_gt_bin_sizes(image_lst, gt_lst, bins)\n",
    "  \n",
    "{k: len(v) for k,v in binned_gt.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import ndimage \n",
    "\n",
    "def find_fusions(label_gt_m, label_gen_m):\n",
    "    \"\"\"\n",
    "    Find the fusions between the ground truth and the generated mask.\n",
    "    A fusion is defined as the number of connected components in the intersection of the two masks.\n",
    "    label_gt_m: (1, H, W)\n",
    "    label_gen_m: (N, H, W)\n",
    "    \"\"\"\n",
    "#     print('find_fusions3')\n",
    "#     print(label_gt_m.shape)\n",
    "#     print(label_gen_m.shape)\n",
    "\n",
    "    intersection = label_gen_m*label_gt_m\n",
    "#     print('intersection',intersection.shape)\n",
    "\n",
    "    count_fusions = [np.unique(cur_intersection) for cur_intersection in intersection]\n",
    "    count_fusions = [cur_count_fusions[cur_count_fusions != 0] for cur_count_fusions in count_fusions]\n",
    "    count_fusions = [len(cur_count_fusions) for cur_count_fusions in count_fusions]\n",
    "\n",
    "    return count_fusions\n",
    "\n",
    "def find_FN(label_gt_m, label_gen_m):\n",
    "    \"\"\"\n",
    "    Find the False Positives in generated mask when compared with gound truth.\n",
    "    label_gt_m: (N, H, W)\n",
    "    label_gen_m: (1, H, W)\n",
    "    \"\"\"\n",
    "#     print('find_FN')\n",
    "#     print(label_gt_m.shape)\n",
    "#     print(label_gen_m.shape)\n",
    "\n",
    "    \n",
    "    intersection = label_gen_m*label_gt_m\n",
    "#     print('intersection',intersection.shape)\n",
    "\n",
    "    FN = np.apply_over_axes(np.sum, intersection, [1,2]).squeeze()\n",
    "#     print('FN',FN.shape)\n",
    "    FN = (FN==0).astype(int)\n",
    "#     print(FN)\n",
    "\n",
    "    return FN\n",
    "\n",
    "def find_FP(label_gt_m, label_gen_m):\n",
    "    \"\"\"\n",
    "    Find the False Positives in generated mask when compared with gound truth.\n",
    "    label_gt_m: (1, H, W)\n",
    "    label_gen_m: (N, H, W)\n",
    "    \"\"\"\n",
    "#     print('find_FP')\n",
    "#     print(label_gt_m.shape)\n",
    "#     print(label_gen_m.shape)\n",
    "\n",
    "    \n",
    "    intersection = label_gen_m*label_gt_m\n",
    "#     print('intersection',intersection.shape)\n",
    "\n",
    "    FP = np.apply_over_axes(np.sum, intersection, [1,2]).squeeze()\n",
    "#     print('FP',FP.shape)\n",
    "    FP = (FP==0).astype(int)\n",
    "#     print(FP)\n",
    "\n",
    "    return FP\n",
    "\n",
    "def get_IoU_and_DICE_OLD(label_gt_m, label_gen_m):\n",
    "    \"\"\"\n",
    "    Find the IoU and DICE between the ground truth and the generated mask.\n",
    "    \"\"\"\n",
    "    intersection = label_gen_m*label_gt_m\n",
    "    # print('intersection',intersection.shape)\n",
    "\n",
    "    \n",
    "    union_with_intersection = label_gen_m + label_gt_m\n",
    "    # print('union',union.shape)\n",
    "    union = (union_with_intersection) > 0\n",
    "\n",
    "    DICE = np.apply_over_axes(np.sum, intersection, [1,2]).squeeze() / np.apply_over_axes(np.sum, union_with_intersection, [1,2]).squeeze()\n",
    "    # print('DICE',DICE.shape)\n",
    "\n",
    "    # intersection per prediction\n",
    "    IoU = np.apply_over_axes(np.sum, intersection, [1,2]).squeeze()\n",
    "\n",
    "    union = np.apply_over_axes(np.sum, union, [1,2]).squeeze()\n",
    "\n",
    "    IoU = IoU/union\n",
    "\n",
    "\n",
    "    return IoU, DICE\n",
    "\n",
    "def get_IoU_and_DICE(label_gt_m, label_gen_m):\n",
    "    \"\"\"\n",
    "    Find the IoU and DICE between the ground truth and the generated mask.\n",
    "    \"\"\"\n",
    "    intersection = label_gen_m*label_gt_m\n",
    "    \n",
    "    # get the indices with non-zero intersection\n",
    "    non_zero_intersections = np.max(intersection, axis=(1,2))\n",
    "    A = np.argwhere(non_zero_intersections)\n",
    "\n",
    "    if A.size == 0:\n",
    "        return 0, 0\n",
    "    \n",
    "    A = A.squeeze()\n",
    "        \n",
    "    intersection = intersection[A,:,:]\n",
    "    if intersection.ndim == 2:\n",
    "        intersection = intersection[None,:,:]\n",
    "    union_with_intersection = (label_gen_m + label_gt_m)[A,:,:]\n",
    "    if union_with_intersection.ndim == 2:\n",
    "        union_with_intersection = union_with_intersection[None,:,:]\n",
    "    \n",
    "    union = (union_with_intersection) > 0\n",
    "\n",
    "    union_limits = np.max(union, axis=0)\n",
    "    \n",
    "    B = np.argwhere(union_limits)\n",
    "    (ystart, xstart), (ystop, xstop) = B.min(0), B.max(0) + 1 \n",
    "\n",
    "    union_with_intersection = union_with_intersection[:,ystart:ystop, xstart:xstop]\n",
    "    union = union[:,ystart:ystop, xstart:xstop]\n",
    "    intersection = intersection[:,ystart:ystop, xstart:xstop]\n",
    "\n",
    "    DICE = np.apply_over_axes(np.sum, intersection, [1,2]).squeeze() / np.apply_over_axes(np.sum, union_with_intersection, [1,2]).squeeze()\n",
    "    \n",
    "    # intersection per prediction\n",
    "    IoU = np.apply_over_axes(np.sum, intersection, [1,2]).squeeze()\n",
    "\n",
    "    union = np.apply_over_axes(np.sum, union, [1,2]).squeeze()\n",
    "\n",
    "    IoU = IoU/union\n",
    "\n",
    "\n",
    "    return IoU, DICE\n",
    "\n",
    "def counting_fusions_FP_FN_img(i, image_lst, gt_lst, gen_lst, all_classes, bins, conf_thresh=0.5):\n",
    "    basename = os.path.splitext(os.path.basename(image_lst[i]))[0]\n",
    "    test_b = basename\n",
    "    gt_b = os.path.splitext(os.path.basename(gt_lst[i]))[0]\n",
    "    gen_b = os.path.splitext(os.path.basename(gen_lst[i]))[0]\n",
    "\n",
    "    is_proba_map = gen_b.endswith('_proba_map')\n",
    "    if is_proba_map:\n",
    "        gen_b = gen_b[:-len('_proba_map')]\n",
    "\n",
    "    # print(test_b,'\\n', gt_b,'\\n', gen_b)\n",
    "\n",
    "    assert test_b == gt_b == gen_b\n",
    "\n",
    "    test_img = imread(image_lst[i])\n",
    "    gt_label = imread(gt_lst[i])\n",
    "    if is_proba_map:\n",
    "        # Get the binary version of the candidate label mask\n",
    "        candidate_label = np.load(gen_lst[i])\n",
    "        if np.any(candidate_label > 5):\n",
    "            candidate_label = candidate_label / 255\n",
    "        candidate_label = (candidate_label > conf_thresh).astype(int)\n",
    "    else:\n",
    "        candidate_label = imread(gen_lst[i])\n",
    "\n",
    "    \n",
    "    \n",
    "    class_fp_fn_fusions = {c:{\n",
    "                            'centroids':{},\n",
    "                            'area':{},\n",
    "                            'FP': {},\n",
    "                            'FN': {},\n",
    "                            'Fusions': {},\n",
    "                            'IoU': {},\n",
    "                            'DICE': {},      \n",
    "                           } for c in all_classes}\n",
    "    \n",
    "    for c_name, c_ids in (pbar :=tqdm(all_classes.items(), leave=False)): # foreground, penumbra, umbra\n",
    "        pbar.set_description(f\"{c_name}\")\n",
    "\n",
    "        # Get the binary version of the masks \n",
    "        gt_class_fg, candidate_class_fg = get_class_binary_mask(gt_label, candidate_label, c_ids)\n",
    "\n",
    "\n",
    "        gt_label, num_gt_label = ndimage.label(gt_class_fg)\n",
    "        gen_label, num_gen_label = ndimage.label(candidate_class_fg)\n",
    "#         print(f'{test_b}_conf{conf_thresh}: {num_gen_label}')\n",
    "        \n",
    "        \n",
    "        whole_mask_gen = np.zeros((num_gen_label, gen_label.shape[0], gen_label.shape[1]))\n",
    "        for idx, b in enumerate(np.unique(gen_label)[1:]):\n",
    "            whole_mask_gen[idx,:,:] = gen_label == b\n",
    "            \n",
    "        whole_mask_gt = np.zeros((num_gt_label, gt_label.shape[0], gt_label.shape[1]))\n",
    "        for idx, b in enumerate(np.unique(gt_label)[1:]):\n",
    "            whole_mask_gt[idx,:,:] = gt_label == b\n",
    "            \n",
    "        \n",
    "        fns     =      find_FN(whole_mask_gt[:,:], gen_label[None,:,:])\n",
    "        fps     =      find_FP(gt_label[None,:,:], whole_mask_gen[:,:])\n",
    "        fusions = find_fusions(gt_label[None,:,:], whole_mask_gen[:,:])\n",
    "        \n",
    "        if len(np.unique(gt_label)[1:])>1:\n",
    "            bestIoUs, bestDICEs = [],[]\n",
    "            for b in np.unique(gt_label)[1:]:\n",
    "                cur_gt_label = gt_label == b\n",
    "    \n",
    "                cur_IoUs,cur_DICEs = get_IoU_and_DICE(cur_gt_label, whole_mask_gen[:,:])\n",
    "                bestIoU, bestDICE = np.max(cur_IoUs),np.max(cur_DICEs)\n",
    "                bestIoUs.append(bestIoU)\n",
    "                bestDICEs.append(bestDICE)\n",
    "            ious = np.array(bestIoUs)\n",
    "            dices = np.array(bestDICEs)\n",
    "            \n",
    "#             ious  = [np.max(get_IoU((gt_label == b)[None,:,:], whole_mask_gen[:,:])) for b in np.unique(gt_label)[1:]]\n",
    "#             dices = [np.max(get_DICE((gt_label == b)[None,:,:], whole_mask_gen[:,:])) for b in np.unique(gt_label)[1:]]\n",
    "#             ious = np.array(ious)\n",
    "#             dices = np.array(dices)\n",
    "        else:\n",
    "            ious = np.array([])\n",
    "            dices = np.array([])\n",
    "        \n",
    "        \n",
    "        centroids = np.array([ item.centroid for item in regionprops(gt_label)])\n",
    "        areas   =      np.apply_over_axes(np.sum, whole_mask_gt, [1,2]).squeeze()\n",
    "        \n",
    "        class_fp_fn_fusions[c_name]['FP'] = fps\n",
    "        class_fp_fn_fusions[c_name]['FN'] = fns\n",
    "        class_fp_fn_fusions[c_name]['Fusions'] = fusions\n",
    "        \n",
    "        \n",
    "        class_fp_fn_fusions[c_name]['centroids'] = centroids\n",
    "        class_fp_fn_fusions[c_name]['area'] = areas\n",
    "        class_fp_fn_fusions[c_name]['IoU'] = ious\n",
    "        class_fp_fn_fusions[c_name]['DICE'] = dices\n",
    "        \n",
    "            \n",
    "    return basename, class_fp_fn_fusions\n",
    "    \n",
    "\n",
    "def counting_fusions(image_lst, gen_lst, device='cpu', bins =[], all_classes={'foreground':[1,2], 'penumbra':[1], 'umbra':[2]}, conf_thresh=0.5):\n",
    "    fp_fn_fusions_dict =  {c: {\n",
    "#                             'FP': {},\n",
    "#                             'FN': {},\n",
    "#                             'Fusions': {}\n",
    "                           }\n",
    "                         for c,ids in all_classes.items()}\n",
    "    print(conf_thresh)\n",
    "#     num_cpu = 2\n",
    "    num_cpu = 16\n",
    "#     print(num_cpu)\n",
    "#     num_cpu = multiprocessing.cpu_count()\n",
    "    with concurrent.futures.ProcessPoolExecutor(max_workers=int(np.floor(0.9*num_cpu))) as executor:\n",
    "        \n",
    "        for bn, res_dict in tqdm(executor.map(counting_fusions_FP_FN_img, \n",
    "                                                  range(len(image_lst)),\n",
    "                                                  repeat(image_lst), repeat(gt_lst), repeat(gen_lst), \n",
    "                                                  repeat(all_classes), repeat(bins),\n",
    "                                                  repeat(conf_thresh),\n",
    "#                                                   [conf_thresh]*len(image_lst)\n",
    "                                                    ),\n",
    "                                                  ):\n",
    "#                 print(bn,res_dict)\n",
    "                for c in all_classes:\n",
    "                    fp_fn_fusions_dict[c][bn] = {}\n",
    "                    fp_fn_fusions_dict[c][bn]['area'] = res_dict[c]['area']\n",
    "                    fp_fn_fusions_dict[c][bn]['centroids'] = res_dict[c]['centroids']\n",
    "                \n",
    "                    fp_fn_fusions_dict[c][bn]['FP'] = res_dict[c]['FP']\n",
    "                    fp_fn_fusions_dict[c][bn]['FN'] = res_dict[c]['FN']\n",
    "                    fp_fn_fusions_dict[c][bn]['Fusions'] = res_dict[c]['Fusions']\n",
    "                    \n",
    "                    fp_fn_fusions_dict[c][bn]['IoU'] = res_dict[c]['IoU']\n",
    "                    fp_fn_fusions_dict[c][bn]['DICE'] = res_dict[c]['DICE']\n",
    "        \n",
    "        \n",
    "            \n",
    "    return fp_fn_fusions_dict\n",
    "\n",
    "\n",
    "def counting_fusions2(image_lst, gen_lst, device='cpu', bins =[], all_classes={'foreground':[1,2], 'penumbra':[1], 'umbra':[2]}, conf_thresh=0.5):\n",
    "    fp_fn_fusions_dict =  {c: {\n",
    "#                             'FP': {},\n",
    "#                             'FN': {},\n",
    "#                             'Fusions': {}\n",
    "                           }\n",
    "                         for c,ids in all_classes.items()}\n",
    "    \n",
    "    for idx in tqdm(range(len(image_lst))):\n",
    "        bn, res_dict = counting_fusions_FP_FN_img(idx, image_lst, gt_lst, gen_lst, all_classes, bins, conf_thresh)\n",
    "        for c in all_classes:\n",
    "                    fp_fn_fusions_dict[c][bn] = {}\n",
    "                    fp_fn_fusions_dict[c][bn]['area'] = res_dict[c]['area']\n",
    "                    fp_fn_fusions_dict[c][bn]['centroids'] = res_dict[c]['centroids']\n",
    "                \n",
    "                    fp_fn_fusions_dict[c][bn]['FP'] = res_dict[c]['FP']\n",
    "                    fp_fn_fusions_dict[c][bn]['FN'] = res_dict[c]['FN']\n",
    "                    fp_fn_fusions_dict[c][bn]['Fusions'] = res_dict[c]['Fusions']\n",
    "                    \n",
    "                    fp_fn_fusions_dict[c][bn]['IoU'] = res_dict[c]['IoU']\n",
    "                    fp_fn_fusions_dict[c][bn]['DICE'] = res_dict[c]['DICE']\n",
    "        \n",
    "            \n",
    "    return fp_fn_fusions_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_classes = {\n",
    "                'foreground':[1,2],\n",
    "#                 'penumbra':[1],\n",
    "#                 'umbra':[2]\n",
    "              }\n",
    "              \n",
    "print(generated_lsts.keys())\n",
    "tmp = counting_fusions2(image_lst, generated_lsts['2023_T275_fgbg'], bins=bins, all_classes=all_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_classes = {\n",
    "                'foreground':[1,2],\n",
    "#                 'penumbra':[1],\n",
    "#                 'umbra':[2]\n",
    "              }\n",
    "fusions_per_generator = {t : counting_fusions(image_lst, generated_lsts[t], bins=bins, all_classes=all_classes)\n",
    "                        for t in tqdm(gen_seg_types[2:])}\n",
    "#                       for t in tqdm(gen_seg_types[2:])}\n",
    "#                         for t in tqdm(gen_seg_types[-1:])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fusions_per_generator.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_classes = {\n",
    "                'foreground':[1,2],\n",
    "#                 'penumbra':[1],\n",
    "#                 'umbra':[2]\n",
    "              }\n",
    "tmp_fusions_per_generator = {t : counting_fusions(image_lst, generated_lsts[t], bins=bins, all_classes=all_classes)\n",
    "                        for t in tqdm(gen_seg_types[1:2])} \n",
    "fusions_per_generator.update(tmp_fusions_per_generator)\n",
    "                                                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids_dict = {}\n",
    "for k in fusions_per_generator.keys():\n",
    "    for img_k, img_dict in fusions_per_generator[k]['foreground'].items():\n",
    "        print(img_k, img_dict)\n",
    "        centroids_dict[img_k] = {'centroids': img_dict['centroids']}\n",
    "    \n",
    "centroids_dict = deepcopy(centroids_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for k in fusions_per_predictor.keys():\n",
    "    print(k)\n",
    "    for img_k, img_dict in fusions_per_predictor[k]['foreground'].items():\n",
    "        fusions_per_predictor[k]['foreground'][img_k]['centroids'] = centroids_dict[img_k]['centroids']\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fusions_per_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_fusions_per_predictor = {f'{t}_conf{c_t}' : deepcopy(counting_fusions(image_lst, pred_lsts[t], bins=bins, all_classes=all_classes, conf_thresh=c_t))\n",
    "#                          for c_t in tqdm([.8,.2,], leave=False) for t in tqdm(list(pred_lsts.keys()) [2:3])}\n",
    "                         for c_t in tqdm([.4,.6,.8,.2, 0], leave=False) for t in tqdm(list(pred_lsts.keys()) [2:3])}\n",
    "fusions_per_predictor.update(tmp_fusions_per_predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(tmp_fusions_per_predictor.keys())\n",
    "# print(tmp_fusions_per_predictor)\n",
    "print(list(tmp_fusions_per_predictor.keys())[0])\n",
    "print(compute_fps_fusions_matches(tmp_fusions_per_predictor, list(tmp_fusions_per_predictor.keys())[0], 'foreground'))\n",
    "print(list(tmp_fusions_per_predictor.keys())[1])\n",
    "print(compute_fps_fusions_matches(tmp_fusions_per_predictor, list(tmp_fusions_per_predictor.keys())[1], 'foreground'))\n",
    "print(list(tmp_fusions_per_predictor.keys())[2])\n",
    "print(compute_fps_fusions_matches(tmp_fusions_per_predictor, list(tmp_fusions_per_predictor.keys())[2], 'foreground'))\n",
    "print(list(tmp_fusions_per_predictor.keys())[3])\n",
    "print(compute_fps_fusions_matches(tmp_fusions_per_predictor, list(tmp_fusions_per_predictor.keys())[3], 'foreground'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_classes = {\n",
    "                'foreground':[1,2],\n",
    "#                 'penumbra':[1],\n",
    "#                 'umbra':[2]\n",
    "              }\n",
    "fusions_per_predictor = {f'{t}_conf{c_t}' : counting_fusions(image_lst, pred_lsts[t], bins=bins, all_classes=all_classes, conf_thresh=c_t)\n",
    "                         for t in tqdm(list(pred_lsts.keys()) [:]) for c_t in tqdm([.4,.6,.8,.2, 0], leave=False)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(fusions_per_predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "all_classes = {\n",
    "                'foreground':[1,2],\n",
    "#                 'penumbra':[1],\n",
    "#                 'umbra':[2]\n",
    "              }\n",
    "scores_per_generator = {t : binning_sunspots3(image_lst, generated_lsts[t], bins=bins, all_classes=all_classes)\n",
    "                        for t in tqdm(gen_seg_types[:])}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_classes = {\n",
    "                'foreground':[1,2],\n",
    "#                 'penumbra':[1],\n",
    "#                 'umbra':[2]\n",
    "              }\n",
    "FP_per_generator = {t : compute_false_positives(image_lst, generated_lsts[t], bins=bins, all_classes=all_classes) \n",
    "                    for t in tqdm(gen_seg_types[:])}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remplir **scores_per_predictor** et **FP_per_predictor**  partir de rien."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scores_per_predictor = {f'{t}_conf{c_t}' : binning_sunspots4(image_lst, pred_lsts[t], bins=bins, all_classes=all_classes,conf_thresh=c_t)\n",
    "                        for c_t in [.4,.6,.8,.2,0] for t in tqdm(list(pred_lsts.keys()) [:])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "FP_per_predictor = {f'{t}_conf{c_t}' : compute_false_positives2(image_lst, pred_lsts[t], bins=bins, all_classes=all_classes, conf_thresh=c_t) \n",
    "                    for c_t in [.4,.6,.8,.2,0] for t in tqdm(list(pred_lsts.keys()) [:])}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ajouter des entres  **scores_per_predictor** et **FP_per_predictor** dj existants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_scores_per_predictor = {t : binning_sunspots3(image_lst, pred_lsts[t], bins=bins)\n",
    "                        for t in tqdm(list(pred_lsts.keys()) [9:])}\n",
    "scores_per_predictor.update(tmp_scores_per_predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_FP_per_predictor = {t : compute_false_positives(image_lst, pred_lsts[t], bins=bins) \n",
    "                    for t in tqdm(list(pred_lsts.keys()) [9:])}\n",
    "FP_per_predictor.update(tmp_FP_per_predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scores_per_generator\n",
    "# FP_per_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(FP_per_predictor.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NpEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        if isinstance(obj, np.int64):\n",
    "            return int(obj)\n",
    "        if isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        if isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        if isinstance(obj, np.bool_):\n",
    "            return bool(obj)\n",
    "        return super(NpEncoder, self).default(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./2023_Fusions_per_generator_fgbg_solDisk.json', 'w') as f:\n",
    "    json.dump(fusions_per_generator,f, cls=NpEncoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./2023_Fusions_per_predictor_fgbg_solDisk_TTA.json', 'w') as f:\n",
    "    json.dump(fusions_per_predictor,f, cls=NpEncoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./2023_Fusions_per_generator_fgbg_solDisk.json', 'r') as f:\n",
    "    fusions_per_generator = json.load(f)\n",
    "with open('./2023_Fusions_per_predictor_fgbg_solDisk.json', 'r') as f:\n",
    "    fusions_per_predictor =  json.load(f)\n",
    "with open('./2023_Fusions_per_predictor_fgbg_solDisk_TTA.json', 'r') as f:\n",
    "    fusions_per_predictor_TTA =  json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./2023_scores_per_generator_fgbg_solDisk.json', 'w') as f:\n",
    "    json.dump(scores_per_generator,f, cls=NpEncoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./2023_scores_per_predictor_fgbg_solDisk_TTA.json', 'w') as f:\n",
    "    json.dump(scores_per_predictor,f, cls=NpEncoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./2023_scores_per_generator_fgbg_solDisk.json', 'r') as f:\n",
    "    scores_per_generator = json.load(f)\n",
    "with open('./2023_scores_per_predictor_fgbg_solDisk.json', 'r') as f:\n",
    "    scores_per_predictor = json.load(f)\n",
    "with open('./2023_scores_per_predictor_fgbg_solDisk_TTA.json', 'r') as f:\n",
    "    scores_per_predictor_TTA = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./2023_FPs_per_generator_fgbg_solDisk.json', 'w') as f:\n",
    "    json.dump(FP_per_generator,f, cls=NpEncoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./2023_FPs_per_predictor_fgbg_solDisk_TTA.json', 'w') as f:\n",
    "    json.dump(FP_per_predictor,f, cls=NpEncoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./2023_FPs_per_generator_fgbg_solDisk.json', 'r') as f:\n",
    "    FP_per_generator = json.load(f)\n",
    "with open('./2023_FPs_per_predictor_fgbg_solDisk.json', 'r') as f:\n",
    "    FP_per_predictor = json.load(f)\n",
    "with open('./2023_FPs_per_predictor_fgbg_solDisk_TTA.json', 'r') as f:\n",
    "    FP_per_predictor_TTA = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_per_segmenter = {\"generators\":scores_per_generator, \"predictors\": scores_per_predictor, \"predictors_TTA\": scores_per_predictor_TTA}\n",
    "FP_per_segmenter = {\"generators\":FP_per_generator, \"predictors\": FP_per_predictor, \"predictors_TTA\": FP_per_predictor_TTA}\n",
    "\n",
    "print(score_per_segmenter[\"generators\"].keys())\n",
    "print(score_per_segmenter[\"predictors\"].keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(score_per_segmenter[\"generators\"].keys())\n",
    "print(fusions_per_generator.keys())\n",
    "print(FP_per_generator.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_generators_stats(generators_names, scores_per_generator, FP_per_generator, metric='DICE',\n",
    "                            all_classes = {'foreground':[1,2], 'penumbra':[1], 'umbra':[2]}):\n",
    "#     all_classes = {'foreground':[1,2], 'penumbra':[1], 'umbra':[2]}\n",
    "    \n",
    "    out_dict = {c: {i:{} for i in range(len(bins))} for c in all_classes.keys()}\n",
    "    \n",
    "#     print(scores_per_generator)\n",
    "    \n",
    "    for c_name, c_ids in (pbar :=tqdm(all_classes.items(), leave=False)):\n",
    "        \n",
    "        for i, b in enumerate( bins[:]):   \n",
    "            scores = {gen_name: [item['value'] \n",
    "                                           for item in scores_per_generator[gen_name][c_name][str(i)][metric]] \n",
    "                                for gen_name in generators_names}\n",
    "            \n",
    "            for i_t in range(len(generators_names)):\n",
    "                gen_name = generators_names[i_t]\n",
    "\n",
    "                scores_fg_bg = [item['value'] \n",
    "                                    for item in scores_per_generator[gen_name][c_name][str(i)][metric] \n",
    "                                if not item['details']['misdetected']]\n",
    "\n",
    "                misdetections_fg_bg = [item['value'] \n",
    "                                           for item in scores_per_generator[gen_name][c_name][str(i)][metric] \n",
    "                                       if item['details']['misdetected']]\n",
    "\n",
    "                \n",
    "                false_positives = FP_per_generator[gen_name][c_name][str(i)][\"num_false_positives\"]\n",
    "                false_positives_area = FP_per_generator[gen_name][c_name][str(i)][\"total_pixel_area\"]\n",
    "        \n",
    "                missed = len(misdetections_fg_bg)\n",
    "        \n",
    "                tmp_scores =  np.array(scores_fg_bg)\n",
    "                mu, sigma = scipy.stats.norm.fit(tmp_scores)\n",
    "                \n",
    "                FP = false_positives\n",
    "                FN = missed\n",
    "                # TP = GT - FN\n",
    "                TP = len(binned_gt[i]) - FN\n",
    "                \n",
    "#                 print(f'TP:{TP}, FP:{FP}, FN:{TP}')\n",
    "                F1 = 2 * (TP / (2*TP + FP + FN)) if len(binned_gt[i]) > 0 else None\n",
    "\n",
    "                out_dict[c_name][i][gen_name] = {\n",
    "                            \"F1\": F1,\n",
    "                            \"num_FN\": missed,\n",
    "                            \"percent_FN\": 100* missed / len(binned_gt[i]) if len(binned_gt[i]) > 0 else None ,\n",
    "                            \"num_FP\": false_positives,\n",
    "                            \"percent_FP\": 100* false_positives / len(binned_gt[i]) if len(binned_gt[i]) > 0 else None, \n",
    "                            \"FP_area\": false_positives_area\n",
    "                            }\n",
    "                \n",
    "    return out_dict\n",
    "    \n",
    "    ################\n",
    "#     out_dict = {i:{} for i in range(len(bins))}\n",
    "    \n",
    "\n",
    "#     for i, b in enumerate( bins[:]):\n",
    "            \n",
    "#         scores_fg_bg = {gen_name: [item['value'] \n",
    "#                                        for item in scores_per_generator[gen_name][0][str(i)][metric]] \n",
    "#                                                                     for gen_name in generators_names}\n",
    "#         for i_t in range(len(generators_names)):\n",
    "#             gen_name = generators_names[i_t]\n",
    "\n",
    "#             scores_fg_bg = [item['value'] \n",
    "# #                                 for item in scores_per_generator[gen_name][0][i][metric] \n",
    "#                                 for item in scores_per_generator[gen_name][0][str(i)][metric] \n",
    "#                             if not item['details']['misdetected']]\n",
    "\n",
    "#             misdetections_fg_bg = [item['value'] \n",
    "#                                        for item in scores_per_generator[gen_name][0][str(i)][metric] \n",
    "# #                                        for item in scores_per_generator[gen_name][0][i][metric] \n",
    "#                                    if item['details']['misdetected']]\n",
    "        \n",
    "        \n",
    "#             false_positives = FP_per_generator[gen_name][str(i)][\"num_false_positives\"]\n",
    "#             false_positives_area = FP_per_generator[gen_name][str(i)][\"total_pixel_area\"]\n",
    "# #             false_positives = FP_per_generator[gen_name][i][\"num_false_positives\"]\n",
    "# #             false_positives_area = FP_per_generator[gen_name][i][\"total_pixel_area\"]\n",
    "            \n",
    "#             missed = len(misdetections_fg_bg)\n",
    "        \n",
    "#             tmp_scores =  np.array(scores_fg_bg)\n",
    "#             mu, sigma = scipy.stats.norm.fit(tmp_scores)\n",
    "            \n",
    "#             out_dict[i][gen_name] = {\n",
    "#                         \"num_FN\": missed,\n",
    "#                         \"percent_FN\": 100* missed / len(binned_gt[i]) if len(binned_gt[i]) > 0 else None ,\n",
    "#                         \"num_FP\": false_positives,\n",
    "#                         \"percent_FP\": 100* false_positives / len(binned_gt[i]) if len(binned_gt[i]) > 0 else None, \n",
    "#                         \"FP_area\": false_positives_area\n",
    "#                         }\n",
    "    \n",
    "#     return out_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = [False,False,False,False,False,\n",
    "#        True,\n",
    "       False,]\n",
    "\n",
    "True in tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import scipy\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "# seg_to_compare = [\n",
    "#     'CNN-Unet_TTA_T400_T350_Alternating_ConstantLR_30epochs_0TTA',\n",
    "#     'CNN-Unet_TTA_T400_T350_Alternating_ConstantLR_30epochs_4TTA',\n",
    "#     'CNN-Unet_TTA_T400_T350_canny_Alternating_MultistepLR_60epochs_0TTA',\n",
    "#     'CNN-Unet_TTA_T400_T350_canny_Alternating_MultistepLR_60epochs_4TTA',\n",
    "#     'CNN-Unet_TTA_T400_T350_canny_Alternating_StepLR_60epochs_0TTA', \n",
    "#     'CNN-Unet_TTA_T400_T350_canny_Alternating_StepLR_60epochs_4TTA', \n",
    "#     'CNN-TransUnet_TTA_T400_T350_canny_Alternating_StepLR_100epochs_0TTA',\n",
    "#     'CNN-TransUnet_TTA_T400_T350_canny_Alternating_StepLR_100epochs_4TTA'\n",
    "# ]\n",
    "seg_to_compare = gen_seg_types\n",
    "\n",
    "int_format = [\"num_FN\", \"num_FP\", \"FP_area\"]\n",
    "float_format = [\"percent_FN\",\"percent_FP\"]\n",
    "\n",
    "all_classes = {'foreground':[1,2], \n",
    "#                'penumbra':[1], \n",
    "#                'umbra':[2]\n",
    "              }\n",
    "bins_stats = compare_generators_stats(seg_to_compare, score_per_segmenter[\"generators\"], FP_per_segmenter[\"generators\"], metric=\"DICE\", all_classes=all_classes)\n",
    "# bins_stats = compare_generators_stats(seg_to_compare, score_per_segmenter[\"predictors\"], FP_per_segmenter[\"predictors\"], metric=\"DICE\", all_classes=all_classes)\n",
    "\n",
    "# print(bins_stats)\n",
    "\n",
    "all_classes = {\n",
    "    'foreground':[1,2], \n",
    "#     'penumbra':[1],\n",
    "#     'umbra':[2]\n",
    "              }\n",
    "for c_name, c_ids in (pbar :=tqdm(all_classes.items(), leave=False)):\n",
    "    print(c_name,c_ids)\n",
    "    for i in range(len(bins)):\n",
    "    #     print(bins[i])\n",
    "        df = pd.DataFrame.from_dict(bins_stats[c_name][i])#.astype(float)\n",
    "    #     display(df)\n",
    "    \n",
    "        \n",
    "        #F1 \n",
    "        F1 = df.transpose()[\"F1\"].to_numpy()\n",
    "    \n",
    "        # Roc curves\n",
    "        FN_rate = df.transpose()[\"percent_FN\"].to_numpy()\n",
    "        print(FN_rate)\n",
    "        FP_rate = df.transpose()[\"percent_FP\"].to_numpy()\n",
    "        \n",
    "        #plot FP and FN rates along their index in a single plot, add a legend, set ticks vertically\n",
    "        fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(5, 5))\n",
    "        ax.set_title(f\"Bin of sunspots of size  {bins[i]} -> {len(binned_gt[i])} in GroudTruth\")\n",
    "        ax.plot( seg_to_compare, FN_rate*0.01, marker='o')\n",
    "        ax.plot( seg_to_compare, FP_rate*0.01, marker='o')\n",
    "        ax.plot( seg_to_compare, F1, marker='o')\n",
    "        ax.legend(['FN_rate', 'FP_rate', 'F1 score'])\n",
    "        ax.set_ylim([0,1])\n",
    "        # set x_label vertically\n",
    "        for tick in ax.get_xticklabels():\n",
    "            tick.set_rotation(90)\n",
    "        fig.tight_layout()\n",
    "        \n",
    "            \n",
    "#         fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(10,5))\n",
    "#         ax[0].plot( seg_to_compare, FN_rate, marker='.')\n",
    "#         ax[0].set_title(\"FN rate\")\n",
    "#         ax[0].set_ylim([0,100])\n",
    "#         for tick in ax[0].get_xticklabels():\n",
    "#             tick.set_rotation(90)\n",
    "#         ax[1].plot( seg_to_compare, FP_rate, marker='.')\n",
    "#         ax[1].set_title(\"FP rate\")\n",
    "#         ax[1].set_ylim([0,100])\n",
    "#         for tick in ax[1].get_xticklabels():\n",
    "#             tick.set_rotation(90)\n",
    "#         plt.show()\n",
    "\n",
    "\n",
    "        for row in df.index:\n",
    "            if row in float_format:\n",
    "                df.loc[row] = df.loc[row].map('{:,.2f}'.format)\n",
    "            elif row in int_format:\n",
    "                df.loc[row] = df.loc[row].map('{:,.0f}'.format)\n",
    "\n",
    "    #     df.loc[\"percent_missed\"] = pd.Series(100* df.loc[\"num_missed\"] / len(binned_gt[i]), dtype=int)\n",
    "\n",
    "        styler = df.transpose().style\n",
    "    #     pd.set_option('display.float_format',\n",
    "    #       lambda x: '{:,0f}'.format(x) if( (abs(x) % 1) != 0 )else '{:,.0f}'.format(x))\n",
    "        styles = [dict(selector=\"caption\", props=[(\"text-align\", \"center\"),\n",
    "                                              (\"font-size\", \"120%\"),\n",
    "                                              (\"font-weight\", \"bold\")])]\n",
    "        styler.set_caption(f\"Bin of sunspots of size  {bins[i]} -> {len(binned_gt[i])} in GroudTruth\").set_table_styles(styles)\n",
    "        styler.set_properties(**{'text-align': 'center'})\n",
    "        display(styler)\n",
    "    #     display(df.transpose())\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import re\n",
    "# seg_to_compare = [k for k,v in score_per_segmenter['predictors'].items()]\n",
    "# print(seg_to_compare)\n",
    "\n",
    "# # seg_to_compare = ['CNN-UNet_T400_T350_run0_0TTA']\n",
    "# # print(seg_to_compare)\n",
    "# # print(re.search('CNN-UNet_(.*)_run(.)_(.)TTA', seg_to_compare[-1]).group(1).split(\"_\"))\n",
    "# # print(re.search('CNN-UNet_(.*)_run(.)_(.)TTA', seg_to_compare[-1]).group(2).split(\"_\"))\n",
    "# # show_names = [f'CNN-{re.search(\"CNN-2013-15_UNet_(.*)_StepLR*\", seg).group(1).split(\"_\")}'.replace(\" \",\"\") for seg in seg_to_compare]\n",
    "# # print(show_names)\n",
    "\n",
    "# show_names = []\n",
    "# run_numbers = []\n",
    "# num_ttas = []\n",
    "# th_vals = []\n",
    "# for seg in seg_to_compare:\n",
    "#     print(seg)\n",
    "#     reg_finder = re.search('CNN-UNet_(.*)_run(.+)_(.)TTA', seg)\n",
    "#     show_names.append(f'CNN-{reg_finder.group(1).split(\"_\")}')\n",
    "#     run_numbers.append(reg_finder.group(2).split(\"_\"))\n",
    "#     num_ttas.append(reg_finder.group(3).split(\"_\"))\n",
    "    \n",
    "#     reg_finder2 = re.findall('T([0-9]+)', reg_finder.group(1))\n",
    "#     th_vals.append([int(x) for x in reg_finder2])\n",
    "\n",
    "    \n",
    "# print(show_names)\n",
    "# print(run_numbers)\n",
    "# print(num_ttas)\n",
    "# print(th_vals)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import scipy\n",
    "import re \n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "import seaborn as sns\n",
    "\n",
    "gen_to_compare = [k for k,v in score_per_segmenter['generators'].items()]#[2:]\n",
    "# print(gen_to_compare)\n",
    "bins_stats_gen = compare_generators_stats(gen_to_compare, score_per_segmenter[\"generators\"], FP_per_segmenter[\"generators\"], metric=\"DICE\", all_classes=all_classes)\n",
    "names_gen = []\n",
    "th_vals_gen = []\n",
    "num_th_gen = []\n",
    "# for seg in fusions_per_generator.keys():\n",
    "for seg in score_per_segmenter[\"generators\"].keys():\n",
    "#     print(seg)\n",
    "    reg_finder = re.search('2023_(.*)_fgbg', seg)\n",
    "    names_gen.append(f'{reg_finder.group(1).split(\"_\")}')\n",
    "    \n",
    "    reg_finder2 = re.findall('T([0-9]+)', reg_finder.group(1))\n",
    "    th_vals_gen.append([int(x) for x in reg_finder2])\n",
    "    num_th_gen.append(len(reg_finder2))\n",
    "# print(names_gen)\n",
    "\n",
    "####################################\n",
    "    \n",
    "\n",
    "seg_to_compare = [k for k,v in score_per_segmenter['predictors'].items()]\n",
    "show_names = []\n",
    "th_vals = []\n",
    "run_numbers = []\n",
    "num_ttas = []\n",
    "for seg in seg_to_compare:\n",
    "    reg_finder = re.search('CNN-UNet_(.*)_run(.+)_(.)TTA', seg)\n",
    "    show_names.append(f'CNN-{reg_finder.group(1).split(\"_\")}')\n",
    "    run_numbers.append(reg_finder.group(2))\n",
    "    num_ttas.append(reg_finder.group(3))\n",
    "    \n",
    "    reg_finder2 = re.findall('T([0-9]+)', reg_finder.group(1))\n",
    "    th_vals.append([int(x) for x in reg_finder2])\n",
    "\n",
    "num_TH = [len(n.split(',')) for n in show_names]\n",
    "# print(run_numbers)\n",
    "# print(num_TH)\n",
    "# print(show_names)\n",
    "# print('here')\n",
    "# print(num_ttas)\n",
    "####################################\n",
    "\n",
    "seg_to_compare_TTA = [k for k,v in score_per_segmenter['predictors_TTA'].items()]\n",
    "show_names_TTA = []\n",
    "th_vals_TTA = []\n",
    "run_numbers_TTA = []\n",
    "num_ttas_TTA = []\n",
    "confidence_TTA = []\n",
    "for seg in seg_to_compare_TTA:\n",
    "    reg_finder = re.search('CNN-UNet_(.*)_run(.+)_(.)TTA_conf(.*)', seg)\n",
    "    show_names_TTA.append(f'CNN-{reg_finder.group(1).split(\"_\")}_conf{reg_finder.group(4)}')\n",
    "    run_numbers_TTA.append(reg_finder.group(2))\n",
    "    num_ttas_TTA.append(reg_finder.group(3))\n",
    "    confidences_pred_TTA.append(float(reg_finder.group(4)) )\n",
    "    \n",
    "    reg_finder2 = re.findall('T([0-9]+)', reg_finder.group(1))\n",
    "    th_vals_TTA.append([int(x) for x in reg_finder2])\n",
    "\n",
    "num_TH_TTA = [len(n.split(',')) for n in show_names_TTA]\n",
    "# print(run_numbers_TTA)\n",
    "# print(num_TH_TTA)\n",
    "# print(show_names_TTA)\n",
    "\n",
    "\n",
    "####################################@\n",
    "    \n",
    "index_lists = [[index for (index, item) in enumerate(num_TH) if item == i] for i in [1,2,3]]\n",
    "print(index_lists)\n",
    "\n",
    "int_format = [\"num_FN\", \"num_FP\", \"FP_area\"]\n",
    "float_format = [\"percent_FN\",\"percent_FP\"]\n",
    "\n",
    "all_classes = {\n",
    "    'foreground':[1,2], \n",
    "#     'penumbra':[1],\n",
    "#     'umbra':[2]\n",
    "              }\n",
    "\n",
    "bins_stats_pred = compare_generators_stats(seg_to_compare, score_per_segmenter[\"predictors\"], FP_per_segmenter[\"predictors\"], metric=\"DICE\",all_classes=all_classes)\n",
    "bins_stats_pred_TTA = compare_generators_stats(seg_to_compare_TTA, score_per_segmenter[\"predictors_TTA\"], FP_per_segmenter[\"predictors_TTA\"], metric=\"DICE\",all_classes=all_classes)\n",
    "\n",
    "# print(bins_stats_pred)\n",
    "\n",
    "\n",
    "for c_name, c_ids in (pbar :=tqdm(all_classes.items(), leave=False)):\n",
    "    print(c_name,c_ids)\n",
    "    for i in range(len(bins)):\n",
    "        df = pd.DataFrame.from_dict(bins_stats_pred[c_name][i])#.astype(float)\n",
    "        \n",
    "        df2 = df.append(pd.Series(show_names, index=df.columns, name='run_name'))\n",
    "        df2 = df2.append(pd.Series(run_numbers, index=df.columns, name='run_number'))\n",
    "        df2 = df2.append(pd.Series(num_TH, index=df.columns, name='num_th'))\n",
    "        df2 = df2.append(pd.Series(num_ttas, index=df.columns, name='num_tta'))\n",
    "        df2 = df2.append(pd.Series([int(np.median(t)) for t in th_vals], index=df.columns, name='th0_val'))\n",
    "        \n",
    "        df2 = df2.transpose()\n",
    "        for f in int_format:\n",
    "            df2[f]=df2[f].astype(int)\n",
    "        for f in float_format:\n",
    "            df2[f]=df2[f].astype(float)\n",
    "        for f in ['F1','th0_val','num_tta','num_th','run_number']:\n",
    "            df2[f] = pd.to_numeric(df2[f])\n",
    "        df2.index = range(len(th_vals))\n",
    "        df2[\"percent_FN\"] = 0.01*df2[\"percent_FN\"]\n",
    "        df2[\"percent_FP\"] = 0.01*df2[\"percent_FP\"]\n",
    "        \n",
    "#         print(df2)\n",
    "        \n",
    "        \n",
    "        #F1 \n",
    "        F1 = df.transpose()[\"F1\"].to_numpy()\n",
    "    \n",
    "        # Roc curves\n",
    "        FN_rate = df.transpose()[\"percent_FN\"].to_numpy()\n",
    "#         print(FN_rate)\n",
    "        FP_rate = df.transpose()[\"percent_FP\"].to_numpy()\n",
    "    \n",
    "        # show the F1 scores of single-thresholds\n",
    "        df3 = df2.loc[df2['num_th'] == 1]\n",
    "         \n",
    "            \n",
    "        sns.set_style(style='whitegrid')\n",
    "        fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(4, 4))\n",
    "#         fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(8, 4))\n",
    "#         g = sns.lineplot(\n",
    "#                     data=df3,\n",
    "#                     x=\"th0_val\", y=\"F1\",\n",
    "# #                     markers=[\"o\"], \n",
    "# #                     kind=\"line\",\n",
    "#                     label='F1',\n",
    "#                     ax=ax[0]\n",
    "#                 )\n",
    "#         g = sns.lineplot(\n",
    "#                     data=df3,\n",
    "#                     x=\"th0_val\", y=\"percent_FN\",\n",
    "# #                     markers=[\"o\"], \n",
    "# #                     kind=\"line\",\n",
    "#                     label='FN',\n",
    "#                     ax=ax[0]\n",
    "#             )\n",
    "#         g = sns.lineplot(\n",
    "#                     data=df3,\n",
    "#                     x=\"th0_val\", y=\"percent_FP\",\n",
    "# #                     markers=[\"o\"], \n",
    "# #                     kind=\"line\",\n",
    "#                     label='FP',\n",
    "#                     ax=ax[0]\n",
    "#             )\n",
    "#         g.set_ylim([0,1.1])\n",
    "        \n",
    "        ########## Compare Thresholding with CNN-1threshold\n",
    "        \n",
    "        df_gen = pd.DataFrame.from_dict(bins_stats_gen[c_name][i]) \n",
    "        df_gen = df_gen.transpose() \n",
    "        df_gen['name'] = names_gen\n",
    "        df_gen['th0_val'] = [np.median(t) for t in th_vals_gen]\n",
    "        \n",
    "        for f in int_format:\n",
    "            df_gen[f]=df_gen[f].astype(int)\n",
    "        for f in float_format:\n",
    "            df_gen[f]=df_gen[f].astype(float)\n",
    "            \n",
    "#         print(df_gen)\n",
    "    \n",
    "        g = sns.lineplot(\n",
    "            data=df_gen,\n",
    "            x=\"th0_val\", y=\"F1\",\n",
    "#             ax=ax[1],\n",
    "            ax=ax,\n",
    "            color='orange',\n",
    "            label='Thresholding'\n",
    "        )\n",
    "        g.set_ylim([0,1.1])\n",
    "        \n",
    "    \n",
    "        g = sns.lineplot(\n",
    "                    data=df3,\n",
    "                    x=\"th0_val\", y=\"F1\",\n",
    "                    ci=None,\n",
    "#                     markers=[\"o\"], \n",
    "#                     kind=\"line\",\n",
    "#             ax=ax[1],\n",
    "            ax=ax,\n",
    "                    color='blue',\n",
    "                    label='CNN-1 Threshold'\n",
    "                )\n",
    "        ax.set_xlabel('Threshold value')\n",
    "        ax.set_ylabel('F1 score')\n",
    "#         ax[1].set_xlabel('Threshold value')\n",
    "#         ax[1].set_ylabel('F1 score')\n",
    "        \n",
    "#         print('here')\n",
    "#         print(names_gen)\n",
    "        \n",
    "\n",
    "        ######## Add CNN-Multithreshold\n",
    "        \n",
    "        df4 = df2.loc[df2['num_th'] > 1]#.drop(['B', 'C'], axis=1)\n",
    "#         df4['th_med'] = th0_val\n",
    "        df4 = df4.loc[df4['run_number'] > 3]\n",
    "        \n",
    "#         print(df4)\n",
    "        df4 = df4.groupby(['run_name']).mean()  \n",
    "#         print(df4)\n",
    "        \n",
    "        g = sns.lineplot(\n",
    "                    data=df4,\n",
    "                    x=\"th0_val\", y=\"F1\",\n",
    "                    markers=[\"o\"], \n",
    "#             ax=ax[1],\n",
    "            ax=ax,\n",
    "                )\n",
    "        sns.scatterplot(data=df4,\n",
    "                x='th0_val', y='F1',\n",
    "#                 marker='X',\n",
    "#                 s=250,\n",
    "              marker='*',\n",
    "              s=350,\n",
    "#             ax=ax[1],\n",
    "            ax=ax,\n",
    "                color='g',\n",
    "                label='CNN-3 Thresholds'\n",
    "               )\n",
    "        ax.set_ylabel('')\n",
    "        ax.set_title('F1 Score')\n",
    "        \n",
    "        \n",
    "        ######## Add CNN-confidence\n",
    "        \n",
    "#         df_TTA = pd.DataFrame.from_dict(bins_stats_pred_TTA[c_name][i])#.astype(float)\n",
    "# #         df_TTA = pd.DataFrame.from_dict(bins_stats_pred[c_name][i])#.astype(float)\n",
    "# #         print(df_TTA)\n",
    "#         df2_TTA = df_TTA.append(pd.Series(show_names_TTA, index=df_TTA.columns, name='run_name'))\n",
    "#         df2_TTA = df2_TTA.append(pd.Series(run_numbers_TTA, index=df_TTA.columns, name='run_number'))\n",
    "#         df2_TTA = df2_TTA.append(pd.Series(num_TH_TTA, index=df_TTA.columns, name='num_th'))\n",
    "#         df2_TTA = df2_TTA.append(pd.Series(num_ttas_TTA, index=df_TTA.columns, name='num_tta'))\n",
    "#         df2_TTA = df2_TTA.append(pd.Series([int(np.median(t)) for t in th_vals_TTA], index=df_TTA.columns, name='th0_val'))\n",
    "        \n",
    "#         df2_TTA = df2_TTA.transpose()\n",
    "#         for f in int_format:\n",
    "#             df2_TTA[f]=df2_TTA[f].astype(int)\n",
    "#         for f in float_format:\n",
    "#             df2_TTA[f]=df2_TTA[f].astype(float)\n",
    "#         for f in ['F1','th0_val','num_tta','num_th','run_number']:\n",
    "#             df2_TTA[f] = pd.to_numeric(df2_TTA[f])\n",
    "#         df2_TTA.index = range(len(th_vals_TTA))\n",
    "#         df2_TTA[\"percent_FN\"] = 0.01*df2_TTA[\"percent_FN\"]\n",
    "#         df2_TTA[\"percent_FP\"] = 0.01*df2_TTA[\"percent_FP\"]\n",
    "        \n",
    "# #         print(df2_TTA)\n",
    "        \n",
    "#         df4_TTA = df2_TTA.loc[df2_TTA['num_th'] > 1]\n",
    "# #         df4_TTA = df4_TTA.loc[df4_TTA['run_number'] > 3]\n",
    "        \n",
    "# #         print(df4)\n",
    "#         df4_TTA = df4_TTA.groupby(['run_name']).mean()  \n",
    "# #         print(df4)\n",
    "        \n",
    "#         pal2 = sns.color_palette('Blues', df_fus_miss_pred_TTA.count()[0])\n",
    "#         g = sns.lineplot(\n",
    "#                     data=df4_TTA,\n",
    "#                     x=\"th0_val\", y=\"F1\",\n",
    "#                     markers=[\"o\"], \n",
    "#                     ax=ax[0]\n",
    "#                 )\n",
    "#         sns.scatterplot(data=df4_TTA,\n",
    "#                 x='th0_val', y='F1',\n",
    "#                 s=150,\n",
    "#                 marker='X',\n",
    "#                 c=pal2,\n",
    "#                 label='CNN-3_Thresholds_TTA'\n",
    "#                )\n",
    "        \n",
    "        \n",
    "        fig.tight_layout()\n",
    "        \n",
    "\n",
    "        plt.savefig(f\"F1Score_bin{i}.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "        plt.show()\n",
    "        \n",
    "        df5 = df2.groupby(['run_name']).mean(numeric_only=True).transpose()\n",
    "        \n",
    "        for row in df5.index:\n",
    "            if row in float_format:\n",
    "                df5.loc[row] = df5.loc[row].map('{:,.2f}'.format)\n",
    "            elif row in int_format:\n",
    "                df5.loc[row] = df5.loc[row].map('{:,.0f}'.format)\n",
    "\n",
    "\n",
    "        styler = df5.transpose().style\n",
    "        styles = [dict(selector=\"caption\", props=[(\"text-align\", \"center\"),\n",
    "                                              (\"font-size\", \"120%\"),\n",
    "                                              (\"font-weight\", \"bold\")])]\n",
    "        styler.set_caption(f\"Bin of sunspots of size  {bins[i]} -> {len(binned_gt[i])} in GroudTruth\").set_table_styles(styles)\n",
    "        styler.set_properties(**{'text-align': 'center'})\n",
    "        display(styler)\n",
    "    #     display(df.transpose())\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = pd.DataFrame(bins_stats_gen[c_name][0])\n",
    "print(tmp)\n",
    "print\n",
    "print(tmp.loc['F1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for c_name, c_ids in (pbar :=tqdm(all_classes.items(), leave=False)):\n",
    "    print(c_name,c_ids)\n",
    "    for i in range(len(bins)):\n",
    "        #### pred\n",
    "        df = pd.DataFrame.from_dict(bins_stats_pred[c_name][i])#.astype(float)\n",
    "        \n",
    "        df2 = df.append(pd.Series(show_names, index=df.columns, name='run_name'))\n",
    "        df2 = df2.append(pd.Series(run_numbers, index=df.columns, name='run_number'))\n",
    "        df2 = df2.append(pd.Series(num_TH, index=df.columns, name='num_th'))\n",
    "        df2 = df2.append(pd.Series(num_ttas, index=df.columns, name='num_tta'))\n",
    "        df2 = df2.append(pd.Series([int(np.median(t)) for t in th_vals], index=df.columns, name='th0_val'))\n",
    "        \n",
    "        df2 = df2.transpose()\n",
    "        for f in int_format:\n",
    "            df2[f]=df2[f].astype(int)\n",
    "        for f in float_format:\n",
    "            df2[f]=df2[f].astype(float)\n",
    "        for f in ['F1','th0_val','num_tta','num_th','run_number']:\n",
    "            df2[f] = pd.to_numeric(df2[f])\n",
    "        df2.index = range(len(th_vals))\n",
    "        df2[\"percent_FN\"] = 0.01*df2[\"percent_FN\"]\n",
    "        df2[\"percent_FP\"] = 0.01*df2[\"percent_FP\"]\n",
    "        \n",
    "#         print(df2)\n",
    "        \n",
    "        \n",
    "        #F1 \n",
    "        F1 = df.transpose()[\"F1\"].to_numpy()\n",
    "    \n",
    "        # Roc curves\n",
    "        FN_rate = df.transpose()[\"percent_FN\"].to_numpy()\n",
    "#         print(FN_rate)\n",
    "        FP_rate = df.transpose()[\"percent_FP\"].to_numpy()\n",
    "    \n",
    "        # show the F1 scores of single-thresholds\n",
    "        df3 = df2.loc[df2['num_th'] == 1]\n",
    "        \n",
    "                \n",
    "        df4 = df2.loc[df2['num_th'] > 1]#.drop(['B', 'C'], axis=1)\n",
    "#         df4['th_med'] = th0_val\n",
    "        df4 = df4.loc[df4['run_number'] > 3]\n",
    "        \n",
    "#         print(df4)\n",
    "        df4 = df4.groupby(['run_name']).mean()  \n",
    "#         print(df4)\n",
    "        \n",
    "        \n",
    "        \n",
    "        df5 = df2.groupby(['run_name']).mean(numeric_only=True)\n",
    "        df5 = df5.loc[df5['num_th'] == 1]\n",
    "#         print(df5.columns)\n",
    "        df5 = df5['F1']\n",
    "        df5 = df5.transpose()\n",
    "        \n",
    "#         print(df5)\n",
    "        \n",
    "        \n",
    "#         for row in df5.index:\n",
    "#             if row in float_format:\n",
    "#                 df5.loc[row] = df5.loc[row].map('{:,.2f}'.format)\n",
    "#             elif row in int_format:\n",
    "#                 df5.loc[row] = df5.loc[row].map('{:,.0f}'.format)\n",
    "\n",
    "\n",
    "        \n",
    "#         styler = df5.transpose().style\n",
    "#         styles = [dict(selector=\"caption\", props=[(\"text-align\", \"center\"),\n",
    "#                                               (\"font-size\", \"120%\"),\n",
    "#                                               (\"font-weight\", \"bold\")])]\n",
    "#         styler.set_caption(f\"Bin of sunspots of size  {bins[i]} -> {len(binned_gt[i])} in GroudTruth\").set_table_styles(styles)\n",
    "#         styler.set_properties(**{'text-align': 'center'})\n",
    "#         display(styler)\n",
    "        \n",
    "        ###### pseudolabels\n",
    "        \n",
    "        df_gen = pd.DataFrame.from_dict(bins_stats[c_name][i])#.astype(float)\n",
    "       \n",
    "        #F1 \n",
    "        F1 = df_gen.transpose()[\"F1\"]\n",
    "        F1 = F1.loc[gen_to_compare[2:]]\n",
    "        print(F1)\n",
    "        \n",
    "        print(f'Bin {i}')\n",
    "        print(df5)\n",
    "        print(df5 - F1.values)\n",
    "        \n",
    "        df6 = df2.groupby(['run_name']).mean(numeric_only=True)\n",
    "        df6 = df6.loc[df6['num_th'] == 3]\n",
    "#         print(df5.columns)\n",
    "        df6 = df6['F1']\n",
    "        df6 = df6.transpose()\n",
    "        print(df6)\n",
    "        print()\n",
    "#         print()\n",
    "#         print(diff)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# tmp = pd.DataFrame(bins_stats_pred[c_name][0])\n",
    "# print('a',tmp)\n",
    "# tmp2 = tmp.append(pd.Series(names_pred, index=tmp.columns, name='run_name'))\n",
    "# tmp2 = tmp2.append(pd.Series(num_th_pred, index=tmp2.columns, name='num_th'))\n",
    "# tmp2 = tmp2.transpose()\n",
    "# print('b',tmp2)\n",
    "# tmp3 = tmp2.loc[tmp2['num_th'] == 1]\n",
    "# print('c',tmp3)\n",
    "# tmp4 = tmp3.groupby('run_name').mean()\n",
    "# print('d',tmp4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import re \n",
    "import itertools\n",
    "sns.set_style(style='whitegrid')\n",
    "import matplotlib.lines as mlines\n",
    "\n",
    "def compute_fps_fusions_matches(big_dict, gen, c_name):\n",
    "#     print(big_dict[gen][c_name])\n",
    "    FPs = []\n",
    "    FNs = []\n",
    "    Matches = []\n",
    "    Fusions = []\n",
    "    Fusions_V2 = []\n",
    "    \n",
    "    for img_key, img_stats in big_dict[gen][c_name].items():\n",
    "#         print(img_stats)\n",
    "        cur_img_FP = np.sum(img_stats['FP'])\n",
    "        cur_img_FN = np.sum(img_stats['FN'])\n",
    "        cur_img_Fusions = np.sum([v for v in img_stats['Fusions'] if v > 1])\n",
    "        cur_img_Fusions_V2 = np.sum([1 for v in img_stats['Fusions'] if v > 1])\n",
    "        cur_img_Matches = np.sum([1 for v in img_stats['Fusions'] if v == 1])\n",
    "\n",
    "#         cur_img_FP = np.sum([1 for k,v in img_fusions.items() if v == 0])\n",
    "#         cur_img_Matches = np.sum([1 for k,v in img_fusions.items() if v == 1])\n",
    "#         cur_img_Fusions = np.sum([v for k,v in img_fusions.items() if v > 1])\n",
    "        \n",
    "        FPs.append(cur_img_FP)\n",
    "        FNs.append(cur_img_FN)\n",
    "        Matches.append(cur_img_Matches)\n",
    "        Fusions.append(cur_img_Fusions)\n",
    "        Fusions_V2.append(cur_img_Fusions_V2)\n",
    "        \n",
    "        \n",
    "    return {'tot_FPs': np.sum(FPs), 'tot_FNs': np.sum(FNs), 'tot_Matches': np.sum(Matches), \n",
    "            'tot_Fusions': np.sum(Fusions), 'tot_Fusions_V2': np.sum(Fusions_V2),\n",
    "           'FPs': FPs, 'Matches': Matches, 'Fusions': Fusions, 'Fusions_V2': Fusions_V2}\n",
    "        \n",
    "\n",
    "names_gen = []\n",
    "th_vals_gen = []\n",
    "num_th_gen = []\n",
    "for seg in fusions_per_generator.keys():\n",
    "#     print(seg)\n",
    "    reg_finder = re.search('2023_(.*)_fgbg', seg)\n",
    "    names_gen.append(f'{reg_finder.group(1).split(\"_\")}')\n",
    "    \n",
    "    reg_finder2 = re.findall('T([0-9]+)', reg_finder.group(1))\n",
    "    th_vals_gen.append([int(x) for x in reg_finder2])\n",
    "    num_th_gen.append(len(reg_finder2))\n",
    "# print(names_gen, th_vals)\n",
    "\n",
    "####################################\n",
    "\n",
    "names_pred = []\n",
    "th_vals_pred = []\n",
    "run_numbers_pred = []\n",
    "num_ttas_pred = []\n",
    "num_th_pred = []\n",
    "for seg in fusions_per_predictor.keys():\n",
    "#     print(seg)\n",
    "    reg_finder = re.search('CNN-UNet_(.*)_run(.+)_(.)TTA', seg)\n",
    "    names_pred.append(f'CNN-{reg_finder.group(1).split(\"_\")}')\n",
    "    run_numbers_pred.append(reg_finder.group(2))\n",
    "    num_ttas_pred.append(reg_finder.group(3))\n",
    "    \n",
    "    reg_finder2 = re.findall('T([0-9]+)', reg_finder.group(1))\n",
    "    th_vals_pred.append([int(x) for x in reg_finder2])\n",
    "    num_th_pred.append(len(reg_finder2))\n",
    "# print(names_pred)\n",
    "# print(num_th)\n",
    "# print(names_gen, th_vals)\n",
    "# print()\n",
    "####################################\n",
    "\n",
    "names_pred_TTA = []\n",
    "th_vals_pred_TTA = []\n",
    "run_numbers_pred_TTA = []\n",
    "num_ttas_pred_TTA = []\n",
    "num_th_pred_TTA = []\n",
    "confidences_pred_TTA = []\n",
    "for seg in fusions_per_predictor_TTA.keys():\n",
    "#     print(seg)\n",
    "    reg_finder = re.search('CNN-UNet_(.*)_run(.+)_(.)TTA_conf(.*)', seg)\n",
    "    names_pred_TTA.append(f'{reg_finder.group(1).split(\"_\")}_conf{reg_finder.group(4)}')\n",
    "    run_numbers_pred_TTA.append(reg_finder.group(2))\n",
    "    num_ttas_pred_TTA.append(reg_finder.group(3))\n",
    "    confidences_pred_TTA.append(float(reg_finder.group(4)) )\n",
    "    \n",
    "    reg_finder2 = re.findall('T([0-9]+)', reg_finder.group(1))\n",
    "    th_vals_pred_TTA.append([int(x) for x in reg_finder2])\n",
    "    num_th_pred_TTA.append(len(reg_finder2))\n",
    "# print(num_th)\n",
    "# print(names_gen, th_vals)\n",
    "# print(confidences_pred_TTA)\n",
    "\n",
    "new_stats_gen = { k: compute_fps_fusions_matches(fusions_per_generator, k, 'foreground') for k, v  in fusions_per_generator.items()}\n",
    "new_stats_pred = { k: compute_fps_fusions_matches(fusions_per_predictor, k, 'foreground') for k, v  in fusions_per_predictor.items()}\n",
    "new_stats_pred_TTA = { k: deepcopy(compute_fps_fusions_matches(fusions_per_predictor_TTA, k, 'foreground')) \n",
    "                                  for k, v  in fusions_per_predictor_TTA.items()}\n",
    "\n",
    "# print(new_stats_pred_TTA)\n",
    "\n",
    "# print('here',new_stats_pred.keys())\n",
    "\n",
    "df_fus_miss_gen = pd.DataFrame(new_stats_gen).transpose()\n",
    "df_fus_miss_pred = pd.DataFrame(new_stats_pred).transpose()\n",
    "df_fus_miss_pred_TTA = pd.DataFrame(new_stats_pred_TTA).transpose()\n",
    "\n",
    "for f in ['tot_FPs','tot_FNs','tot_Fusions','tot_Fusions_V2','tot_Matches']:\n",
    "    df_fus_miss_gen[f]=df_fus_miss_gen[f].astype(int)\n",
    "    df_fus_miss_pred[f]=df_fus_miss_pred[f].astype(int)\n",
    "    df_fus_miss_pred_TTA[f]=df_fus_miss_pred_TTA[f].astype(int)\n",
    "    \n",
    "df_fus_miss_gen['name'] = names_gen\n",
    "df_fus_miss_pred['name'] = names_pred\n",
    "df_fus_miss_pred['run_number'] = run_numbers_pred\n",
    "df_fus_miss_pred['num_ttas'] = num_ttas_pred\n",
    "df_fus_miss_pred['num_th'] = num_th_pred\n",
    "\n",
    "df_fus_miss_pred_TTA['name'] = names_pred_TTA\n",
    "df_fus_miss_pred_TTA['run_number'] = run_numbers_pred_TTA\n",
    "df_fus_miss_pred_TTA['num_ttas'] = num_ttas_pred_TTA\n",
    "df_fus_miss_pred_TTA['num_th'] = num_th_pred_TTA\n",
    "df_fus_miss_pred_TTA['confidence'] = confidences_pred_TTA\n",
    "\n",
    "df_fus_miss_pred = df_fus_miss_pred.groupby(['name']).mean(numeric_only=True)\n",
    "# df_fus_miss_pred_TTA = df_fus_miss_pred_TTA.groupby(['name']).mean(numeric_only=True)\n",
    "\n",
    "# df2.append(pd.Series([int(t[0]) for t in th_vals], index=df.columns, name='th0_val'))\n",
    "# print(df_fus_miss_gen)\n",
    "# print(df_fus_miss_pred)\n",
    "\n",
    "# print(df_fus_miss.info())\n",
    "# print(new_stats)\n",
    "fig_fus_miss, ax_fus_miss = plt.subplots(nrows=1, ncols=1, figsize=(5, 4))\n",
    "sns.set_style(\"darkgrid\")\n",
    "g_l = sns.lineplot(\n",
    "            data=df_fus_miss_gen,\n",
    "            x=\"tot_FNs\", y=\"tot_Fusions_V2\",\n",
    "            ax=ax_fus_miss,\n",
    "            color='orange',\n",
    "            label='Thresholding'\n",
    "    )\n",
    "# print('HERE')\n",
    "# print(df_fus_miss_gen)\n",
    "tmp = pd.DataFrame(bins_stats_gen[c_name][0])\n",
    "for entry in tmp:\n",
    "    coord = df_fus_miss_gen.loc[entry]\n",
    "    x, y=coord['tot_FNs' ],coord['tot_Fusions_V2']\n",
    "    f1 = tmp[entry]['F1']\n",
    "#     print(entry, f1)\n",
    "    t = 'F1: {:.2f}'.format(f1)\n",
    "#     print(t)\n",
    "    ax_fus_miss.text(x+3,y+1, t, size=8, c='orange')\n",
    "\n",
    "# marker = itertools.cycle(['o', 'x', '*', '8', 's', 'p', 'd', 'v', 'h' ,'x','x'])\n",
    "# marker = itertools.cycle(['x','x','x','x','x','x','x','x','x','x','x'])\n",
    "# marker = itertools.cycle([ 'o', 'v',  '<', 'D', '>','^', '8', 's', 'p', 'X', 'X', 'D', 'd', 'P', 'X'])\n",
    "marker = itertools.cycle([  'D', 'v', '<', 'o', '>','^', '8', 's', 'p', 'X', 'X', 'D', 'd', 'P', 'X'])\n",
    "markers = [next(marker) for i in df_fus_miss_gen[\"name\"].unique()]\n",
    "# print(markers)\n",
    "sns.scatterplot(data=df_fus_miss_gen,\n",
    "                x='tot_FNs', y='tot_Fusions_V2', \n",
    "#                 hue='name', \n",
    "                style='name',\n",
    "                s=110,\n",
    "                color='orange',\n",
    "                markers = markers,\n",
    "               )\n",
    "\n",
    "p_l = sns.lineplot(\n",
    "            data=df_fus_miss_pred.loc[df_fus_miss_pred['num_th'] == 1],\n",
    "            x=\"tot_FNs\", y=\"tot_Fusions_V2\",\n",
    "            ax=ax_fus_miss,\n",
    "            color='blue'\n",
    "    )\n",
    "\n",
    "# print(df_fus_miss_pred.index)\n",
    "marker_CNN = itertools.cycle([  'D', 'v', '<', 'o', '>','^', '8', 's', 'p', 'X', 'X', 'D', 'd', 'P', 'X'])\n",
    "markers_CNN = [next(marker_CNN) for i in df_fus_miss_pred.loc[df_fus_miss_pred['num_th'] == 1].index]\n",
    "# print(markers_CNN)\n",
    "p_s = sns.scatterplot(data=df_fus_miss_pred.loc[df_fus_miss_pred['num_th'] == 1],\n",
    "                x='tot_FNs', y='tot_Fusions_V2', \n",
    "#                 hue='name', \n",
    "                style='name',\n",
    "                s=110,\n",
    "                markers=markers_CNN,\n",
    "                color='blue',\n",
    "                ax=ax_fus_miss,\n",
    "               )\n",
    "\n",
    "tmp = pd.DataFrame(bins_stats_pred[c_name][0])\n",
    "# print('a',tmp)\n",
    "tmp2 = tmp.append(pd.Series(names_pred, index=tmp.columns, name='run_name'))\n",
    "tmp2 = tmp2.append(pd.Series(num_th_pred, index=tmp2.columns, name='num_th'))\n",
    "tmp2 = tmp2.transpose()\n",
    "# print('b',tmp2)\n",
    "tmp3 = tmp2.loc[tmp2['num_th'] == 1]\n",
    "# print('c',tmp3)\n",
    "tmp4 = tmp3.groupby('run_name').mean()\n",
    "# print('d',tmp4)\n",
    "for entry in tmp4.transpose():\n",
    "    coord = df_fus_miss_pred.loc[entry]\n",
    "    x, y=coord['tot_FNs' ],coord['tot_Fusions_V2']\n",
    "    f1 = tmp4.transpose()[entry]['F1']\n",
    "#     print(entry, f1)\n",
    "    t = 'F1: {:.2f}'.format(f1)\n",
    "#     print(t)\n",
    "    ax_fus_miss.text(x+3,y+1, t, size=8, c='blue')\n",
    "    \n",
    "\n",
    "# print('b',tmp2)\n",
    "tmp3 = tmp2.loc[tmp2['num_th'] == 3]\n",
    "# print('c',tmp3)\n",
    "tmp4 = tmp3.groupby('run_name').mean()\n",
    "# print('d',tmp4)\n",
    "for entry in tmp4.transpose():\n",
    "    coord = df_fus_miss_pred.loc[entry]\n",
    "    x, y=coord['tot_FNs' ],coord['tot_Fusions_V2']\n",
    "    f1 = tmp4.transpose()[entry]['F1']\n",
    "#     print(entry, f1)\n",
    "    t = 'F1: {:.2f}'.format(f1)\n",
    "#     print(t)\n",
    "    ax_fus_miss.text(x-3,y-2, t, size=8, c='g', va='top', ha='right')\n",
    "\n",
    "# g = sns.lineplot(\n",
    "#             data=df_fus_miss_pred.loc[df_fus_miss_pred['num_th'] == 3],\n",
    "#             x=\"tot_FNs\", y=\"tot_Fusions_V2\",\n",
    "#             ax=ax_fus_miss,\n",
    "#     )\n",
    "\n",
    "p_s3 = sns.scatterplot(data=df_fus_miss_pred.loc[df_fus_miss_pred['num_th'] == 3],\n",
    "                x='tot_FNs', y='tot_Fusions_V2', \n",
    "#                 hue='name', \n",
    "#                 style='name',\n",
    "                s=300,\n",
    "                marker='*',\n",
    "                color='green'\n",
    "               )\n",
    "\n",
    "pal1 = sns.color_palette('Greens', df_fus_miss_pred_TTA.count()[0])\n",
    "pal2 = sns.color_palette('Blues', df_fus_miss_pred_TTA.count()[0])\n",
    "\n",
    "# print('HERE')\n",
    "# print(df_fus_miss_pred_TTA)\n",
    "# sns.scatterplot(data=df_fus_miss_pred_TTA.loc[df_fus_miss_pred_TTA['num_th'] == 3],\n",
    "#                 x='tot_FNs', y='tot_Fusions_V2', \n",
    "# #                 hue='name', \n",
    "# #                 style='name',\n",
    "#                 s=150,\n",
    "#                 marker='X',\n",
    "#                 c=pal1,\n",
    "# #                 hue='confidence',\n",
    "#                )\n",
    "\n",
    "# df_fus_miss_pred_TTA['tot_FNs'] = df_fus_miss_pred_TTA['tot_FNs']-60\n",
    "# df_fus_miss_pred_TTA['tot_Fusions_V2'] = df_fus_miss_pred_TTA['tot_Fusions_V2']+5\n",
    "# # print(df_fus_miss_pred_TTA)\n",
    "# sns.scatterplot(data=df_fus_miss_pred_TTA.loc[df_fus_miss_pred_TTA['num_th'] == 3],\n",
    "#                 x='tot_FNs', y='tot_Fusions_V2', \n",
    "# #                 hue='name', \n",
    "# #                 style='name',\n",
    "#                 s=150,\n",
    "#                 marker='X',\n",
    "# #                 color='green',\n",
    "# #                 hue='confidence',\n",
    "#                 c=pal2\n",
    "#                )\n",
    "\n",
    "ax_fus_miss.set_xlabel('Number of non-detected sunspots')\n",
    "ax_fus_miss.set_ylabel('Number of Fusions')\n",
    "\n",
    "ax_fus_miss.legend([],[], frameon=False)\n",
    "\n",
    "ax_fus_miss.patch.set_edgecolor('black')  \n",
    "ax_fus_miss.patch.set_linewidth('1') \n",
    "\n",
    "ax_fus_miss.set_xlim([50, 370])\n",
    "ax_fus_miss.set_ylim([10, 80])\n",
    "\n",
    "orange_line = mlines.Line2D([], [], color='orange', marker='None', linestyle='-',\n",
    "                          markersize=10, label='Thresholding')\n",
    "blue_line = mlines.Line2D([], [], color='blue', marker='None', linestyle='-',\n",
    "                          markersize=10, label='CNN-1 Threshold')\n",
    "green_cross = mlines.Line2D([], [], color='green', marker='*', linestyle='None',\n",
    "                          markersize=10, label='CNN-3 Thresholds')\n",
    "\n",
    "# fig_fus_miss.legend([g_l, p_l, p_s3] , labels=['Thresholding','CNN-1 Threshold', 'CNN-3 Threshold'])\n",
    "ax_fus_miss.legend(handles=[orange_line, blue_line, green_cross])\n",
    "ax_fus_miss.set_title('Misdetections versus Fusions trade-off')\n",
    "fig_fus_miss.tight_layout()\n",
    "\n",
    "plt.savefig(\"UndetectedVSFusions.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "a = df_fus_miss_gen['tot_FNs'][:-2]\n",
    "b = df_fus_miss_pred.loc[df_fus_miss_pred['num_th'] == 1]['tot_FNs']\n",
    "c = a-b.values\n",
    "d = c / a.values\n",
    "print(a)\n",
    "print(b)\n",
    "print(c)\n",
    "print(d)\n",
    "\n",
    "print()\n",
    "\n",
    "a = df_fus_miss_gen['tot_Fusions_V2'][:-2]\n",
    "b = df_fus_miss_pred.loc[df_fus_miss_pred['num_th'] == 1]['tot_Fusions_V2']\n",
    "c = b-a.values\n",
    "d = c / a.values\n",
    "print(a)\n",
    "print(b)\n",
    "print(c)\n",
    "print(d)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "##### import scipy\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "import seaborn as sns\n",
    "\n",
    "gen_to_compare = [k for k,v in score_per_segmenter['generators'].items()]#[2:]\n",
    "print(gen_to_compare)\n",
    "bins_stats_gen = compare_generators_stats(gen_to_compare, score_per_segmenter[\"generators\"], FP_per_segmenter[\"generators\"], metric=\"DICE\", all_classes=all_classes)\n",
    "names_gen = []\n",
    "th_vals_gen = []\n",
    "num_th_gen = []\n",
    "# for seg in fusions_per_generator.keys():\n",
    "for seg in score_per_segmenter[\"generators\"].keys():\n",
    "    print(seg)\n",
    "    reg_finder = re.search('2023_(.*)_fgbg', seg)\n",
    "    names_gen.append(f'{reg_finder.group(1).split(\"_\")}')\n",
    "    \n",
    "    reg_finder2 = re.findall('T([0-9]+)', reg_finder.group(1))\n",
    "    th_vals_gen.append([int(x) for x in reg_finder2])\n",
    "    num_th_gen.append(len(reg_finder2))\n",
    "print(names_gen)\n",
    "\n",
    "####################################\n",
    "    \n",
    "\n",
    "seg_to_compare = [k for k,v in score_per_segmenter['predictors'].items()]\n",
    "show_names = []\n",
    "th_vals = []\n",
    "run_numbers = []\n",
    "num_ttas = []\n",
    "for seg in seg_to_compare:\n",
    "    reg_finder = re.search('CNN-UNet_(.*)_run(.+)_(.)TTA', seg)\n",
    "    show_names.append(f'CNN-{reg_finder.group(1).split(\"_\")}')\n",
    "    run_numbers.append(reg_finder.group(2))\n",
    "    num_ttas.append(reg_finder.group(3))\n",
    "    \n",
    "    reg_finder2 = re.findall('T([0-9]+)', reg_finder.group(1))\n",
    "    th_vals.append([int(x) for x in reg_finder2])\n",
    "\n",
    "num_TH = [len(n.split(',')) for n in show_names]\n",
    "print(run_numbers)\n",
    "print(num_TH)\n",
    "print(show_names)\n",
    "print('here')\n",
    "print(num_ttas)\n",
    "####################################\n",
    "\n",
    "seg_to_compare_TTA = [k for k,v in score_per_segmenter['predictors_TTA'].items()]\n",
    "show_names_TTA = []\n",
    "th_vals_TTA = []\n",
    "run_numbers_TTA = []\n",
    "num_ttas_TTA = []\n",
    "confidence_TTA = []\n",
    "for seg in seg_to_compare_TTA:\n",
    "    reg_finder = re.search('CNN-UNet_(.*)_run(.+)_(.)TTA_conf(.*)', seg)\n",
    "    show_names_TTA.append(f'CNN-{reg_finder.group(1).split(\"_\")}_conf{reg_finder.group(4)}')\n",
    "    run_numbers_TTA.append(reg_finder.group(2))\n",
    "    num_ttas_TTA.append(reg_finder.group(3))\n",
    "    confidences_pred_TTA.append(float(reg_finder.group(4)) )\n",
    "    \n",
    "    reg_finder2 = re.findall('T([0-9]+)', reg_finder.group(1))\n",
    "    th_vals_TTA.append([int(x) for x in reg_finder2])\n",
    "\n",
    "num_TH_TTA = [len(n.split(',')) for n in show_names_TTA]\n",
    "print(run_numbers_TTA)\n",
    "print(num_TH_TTA)\n",
    "print(show_names_TTA)\n",
    "\n",
    "\n",
    "####################################@\n",
    "    \n",
    "index_lists = [[index for (index, item) in enumerate(num_TH) if item == i] for i in [1,2,3]]\n",
    "print(index_lists)\n",
    "\n",
    "int_format = [\"num_FN\", \"num_FP\", \"FP_area\"]\n",
    "float_format = [\"percent_FN\",\"percent_FP\"]\n",
    "\n",
    "all_classes = {\n",
    "    'foreground':[1,2], \n",
    "#     'penumbra':[1],\n",
    "#     'umbra':[2]\n",
    "              }\n",
    "\n",
    "bins_stats_pred = compare_generators_stats(seg_to_compare, score_per_segmenter[\"predictors\"], FP_per_segmenter[\"predictors\"], metric=\"DICE\",all_classes=all_classes)\n",
    "bins_stats_pred_TTA = compare_generators_stats(seg_to_compare_TTA, score_per_segmenter[\"predictors_TTA\"], FP_per_segmenter[\"predictors_TTA\"], metric=\"DICE\",all_classes=all_classes)\n",
    "\n",
    "# print(bins_stats_pred)\n",
    "\n",
    "\n",
    "sns.set_style(style='whitegrid')\n",
    "nrows = 3\n",
    "ncols = 3\n",
    "fig, ax = plt.subplots(nrows=nrows, ncols=ncols, figsize=(4*ncols, 3*nrows))\n",
    "\n",
    "for c_name, c_ids in (pbar :=tqdm(all_classes.items(), leave=False)):\n",
    "#     print(c_name,c_ids)\n",
    "    for i in range(len(bins)-1):\n",
    "        a,b = i//ncols, i%ncols\n",
    "        \n",
    "        df = pd.DataFrame.from_dict(bins_stats_pred[c_name][i])#.astype(float)\n",
    "        \n",
    "        df2 = df.append(pd.Series(show_names, index=df.columns, name='run_name'))\n",
    "        df2 = df2.append(pd.Series(run_numbers, index=df.columns, name='run_number'))\n",
    "        df2 = df2.append(pd.Series(num_TH, index=df.columns, name='num_th'))\n",
    "        df2 = df2.append(pd.Series(num_ttas, index=df.columns, name='num_tta'))\n",
    "        df2 = df2.append(pd.Series([int(np.median(t)) for t in th_vals], index=df.columns, name='th0_val'))\n",
    "        \n",
    "        df2 = df2.transpose()\n",
    "        for f in int_format:\n",
    "            df2[f]=df2[f].astype(int)\n",
    "        for f in float_format:\n",
    "            df2[f]=df2[f].astype(float)\n",
    "        for f in ['F1','th0_val','num_tta','num_th','run_number']:\n",
    "            df2[f] = pd.to_numeric(df2[f])\n",
    "        df2.index = range(len(th_vals))\n",
    "        df2[\"percent_FN\"] = 0.01*df2[\"percent_FN\"]\n",
    "        df2[\"percent_FP\"] = 0.01*df2[\"percent_FP\"]\n",
    "        \n",
    "#         print(df2)\n",
    "        \n",
    "        \n",
    "        #F1 \n",
    "        F1 = df.transpose()[\"F1\"].to_numpy()\n",
    "    \n",
    "        # Roc curves\n",
    "        FN_rate = df.transpose()[\"percent_FN\"].to_numpy()\n",
    "#         print(FN_rate)\n",
    "        FP_rate = df.transpose()[\"percent_FP\"].to_numpy()\n",
    "    \n",
    "        # show the F1 scores of single-thresholds\n",
    "        df3 = df2.loc[df2['num_th'] == 1]\n",
    "         \n",
    "            \n",
    "#         fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(8, 4))\n",
    "#         g = sns.lineplot(\n",
    "#                     data=df3,\n",
    "#                     x=\"th0_val\", y=\"F1\",\n",
    "# #                     markers=[\"o\"], \n",
    "# #                     kind=\"line\",\n",
    "#                     label='F1',\n",
    "#                     ax=ax[0]\n",
    "#                 )\n",
    "#         g = sns.lineplot(\n",
    "#                     data=df3,\n",
    "#                     x=\"th0_val\", y=\"percent_FN\",\n",
    "# #                     markers=[\"o\"], \n",
    "# #                     kind=\"line\",\n",
    "#                     label='FN',\n",
    "#                     ax=ax[0]\n",
    "#             )\n",
    "#         g = sns.lineplot(\n",
    "#                     data=df3,\n",
    "#                     x=\"th0_val\", y=\"percent_FP\",\n",
    "# #                     markers=[\"o\"], \n",
    "# #                     kind=\"line\",\n",
    "#                     label='FP',\n",
    "#                     ax=ax[0]\n",
    "#             )\n",
    "#         g.set_ylim([0,1.1])\n",
    "        \n",
    "        ########## Compare Thresholding with CNN-1threshold\n",
    "        \n",
    "        df_gen = pd.DataFrame.from_dict(bins_stats_gen[c_name][i]) \n",
    "        df_gen = df_gen.transpose() \n",
    "        df_gen['name'] = names_gen\n",
    "        df_gen['th0_val'] = [np.median(t) for t in th_vals_gen]\n",
    "        \n",
    "        for f in int_format:\n",
    "            df_gen[f]=df_gen[f].astype(int)\n",
    "        for f in float_format:\n",
    "            df_gen[f]=df_gen[f].astype(float)\n",
    "            \n",
    "#         print(df_gen)\n",
    "    \n",
    "        g = sns.lineplot(\n",
    "            data=df_gen,\n",
    "            x=\"th0_val\", y=\"F1\",\n",
    "#             ax=ax[1],\n",
    "            ax=ax[a,b],\n",
    "            color='orange',\n",
    "            label='Thresholding'\n",
    "        )\n",
    "        g.set_ylim([0,1.1])\n",
    "        \n",
    "    \n",
    "        g = sns.lineplot(\n",
    "                    data=df3,\n",
    "                    x=\"th0_val\", y=\"F1\",\n",
    "                    ci=None,\n",
    "#                     markers=[\"o\"], \n",
    "#                     kind=\"line\",\n",
    "#             ax=ax[1],\n",
    "            ax=ax[a,b],\n",
    "                    color='blue',\n",
    "                    label='CNN-1 Threshold'\n",
    "                )\n",
    "        \n",
    "                \n",
    "\n",
    "        ######## Add CNN-Multithreshold\n",
    "        \n",
    "        df4 = df2.loc[df2['num_th'] > 1]#.drop(['B', 'C'], axis=1)\n",
    "#         df4['th_med'] = th0_val\n",
    "        df4 = df4.loc[df4['run_number'] > 3]\n",
    "        \n",
    "#         print(df4)\n",
    "        df4 = df4.groupby(['run_name']).mean()  \n",
    "#         print(df4)\n",
    "        \n",
    "        g = sns.lineplot(\n",
    "                    data=df4,\n",
    "                    x=\"th0_val\", y=\"F1\",\n",
    "                    markers=[\"o\"], \n",
    "#             ax=ax[1],\n",
    "            ax=ax[a,b],\n",
    "                )\n",
    "        sns.scatterplot(data=df4,\n",
    "                x='th0_val', y='F1',\n",
    "                s=300,\n",
    "                marker='*',\n",
    "#             ax=ax[1],\n",
    "            ax=ax[a,b],\n",
    "                color='g',\n",
    "                label='CNN-3 Thresholds'\n",
    "               )\n",
    "        \n",
    "        \n",
    "        ######## Add CNN-confidence\n",
    "        \n",
    "#         df_TTA = pd.DataFrame.from_dict(bins_stats_pred_TTA[c_name][i])#.astype(float)\n",
    "# #         df_TTA = pd.DataFrame.from_dict(bins_stats_pred[c_name][i])#.astype(float)\n",
    "# #         print(df_TTA)\n",
    "#         df2_TTA = df_TTA.append(pd.Series(show_names_TTA, index=df_TTA.columns, name='run_name'))\n",
    "#         df2_TTA = df2_TTA.append(pd.Series(run_numbers_TTA, index=df_TTA.columns, name='run_number'))\n",
    "#         df2_TTA = df2_TTA.append(pd.Series(num_TH_TTA, index=df_TTA.columns, name='num_th'))\n",
    "#         df2_TTA = df2_TTA.append(pd.Series(num_ttas_TTA, index=df_TTA.columns, name='num_tta'))\n",
    "#         df2_TTA = df2_TTA.append(pd.Series([int(np.median(t)) for t in th_vals_TTA], index=df_TTA.columns, name='th0_val'))\n",
    "        \n",
    "#         df2_TTA = df2_TTA.transpose()\n",
    "#         for f in int_format:\n",
    "#             df2_TTA[f]=df2_TTA[f].astype(int)\n",
    "#         for f in float_format:\n",
    "#             df2_TTA[f]=df2_TTA[f].astype(float)\n",
    "#         for f in ['F1','th0_val','num_tta','num_th','run_number']:\n",
    "#             df2_TTA[f] = pd.to_numeric(df2_TTA[f])\n",
    "#         df2_TTA.index = range(len(th_vals_TTA))\n",
    "#         df2_TTA[\"percent_FN\"] = 0.01*df2_TTA[\"percent_FN\"]\n",
    "#         df2_TTA[\"percent_FP\"] = 0.01*df2_TTA[\"percent_FP\"]\n",
    "        \n",
    "# #         print(df2_TTA)\n",
    "        \n",
    "#         df4_TTA = df2_TTA.loc[df2_TTA['num_th'] > 1]\n",
    "# #         df4_TTA = df4_TTA.loc[df4_TTA['run_number'] > 3]\n",
    "        \n",
    "# #         print(df4)\n",
    "#         df4_TTA = df4_TTA.groupby(['run_name']).mean()  \n",
    "# #         print(df4)\n",
    "        \n",
    "#         pal2 = sns.color_palette('Blues', df_fus_miss_pred_TTA.count()[0])\n",
    "#         g = sns.lineplot(\n",
    "#                     data=df4_TTA,\n",
    "#                     x=\"th0_val\", y=\"F1\",\n",
    "#                     markers=[\"o\"], \n",
    "#                     ax=ax[0]\n",
    "#                 )\n",
    "#         sns.scatterplot(data=df4_TTA,\n",
    "#                 x='th0_val', y='F1',\n",
    "#                 s=150,\n",
    "#                 marker='X',\n",
    "#                 c=pal2,\n",
    "#                 label='CNN-3_Thresholds_TTA'\n",
    "#                )\n",
    "        \n",
    "        ax[a,b].get_legend().remove()\n",
    "        ax[a,b].set_title(f'Bin {i}: F1 score')\n",
    "        ax[a,b].set_xlabel('Threshold value')\n",
    "        ax[a,b].set_ylabel('')\n",
    "#         ax[a,b].set_ylabel('F1 score')\n",
    "#         ax[1].set_xlabel('Threshold value')\n",
    "#         ax[1].set_ylabel('F1 score')\n",
    "        \n",
    "#         print('here')\n",
    "#         print(names_gen)\n",
    "\n",
    "        fig.tight_layout()\n",
    "        \n",
    "\n",
    "#         plt.show()\n",
    "        \n",
    "#         df5 = df2.groupby(['run_name']).mean(numeric_only=True).transpose()\n",
    "        \n",
    "#         for row in df5.index:\n",
    "#             if row in float_format:\n",
    "#                 df5.loc[row] = df5.loc[row].map('{:,.2f}'.format)\n",
    "#             elif row in int_format:\n",
    "#                 df5.loc[row] = df5.loc[row].map('{:,.0f}'.format)\n",
    "\n",
    "\n",
    "#         styler = df5.transpose().style\n",
    "#         styles = [dict(selector=\"caption\", props=[(\"text-align\", \"center\"),\n",
    "#                                               (\"font-size\", \"120%\"),\n",
    "#                                               (\"font-weight\", \"bold\")])]\n",
    "#         styler.set_caption(f\"Bin of sunspots of size  {bins[i]} -> {len(binned_gt[i])} in GroudTruth\").set_table_styles(styles)\n",
    "#         styler.set_properties(**{'text-align': 'center'})\n",
    "#         display(styler)\n",
    "#     #     display(df.transpose())\n",
    "\n",
    "ax[-1,-1].remove()\n",
    "ax[-1,-2].remove()\n",
    "\n",
    "orange_line = mlines.Line2D([], [], color='orange', marker='None', linestyle='-',\n",
    "                          markersize=10, label='Thresholding')\n",
    "blue_line = mlines.Line2D([], [], color='blue', marker='None', linestyle='-',\n",
    "                          markersize=10, label='CNN-1 Threshold')\n",
    "green_cross = mlines.Line2D([], [], color='green', marker='*', linestyle='None',\n",
    "                          markersize=10, label='CNN-3 Thresholds')\n",
    "\n",
    "fig.legend(handles=[orange_line, blue_line, green_cross],loc='lower right', bbox_to_anchor=(.85, .1) )\n",
    "    \n",
    "plt.savefig(f\"F1Score_All_bins.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for c_name, c_ids in (pbar :=tqdm(all_classes.items(), leave=False)):\n",
    "#     print(c_name,c_ids)\n",
    "#     for i in range(len(bins)):\n",
    "#     #     print(bins[i])\n",
    "#         df = pd.DataFrame.from_dict(bins_stats[c_name][i])#.astype(float)\n",
    "        \n",
    "#         df2 = df.append(pd.Series(show_names, index=df.columns, name='run_name'))\n",
    "#         df2 = df.append(pd.Series(show_names, index=df.columns, name='run_name'))\n",
    "#         df2 = df2.append(pd.Series(run_numbers, index=df.columns, name='run_number'))\n",
    "#         df2 = df2.append(pd.Series(num_TH, index=df.columns, name='num_th'))\n",
    "#         df2 = df2.append(pd.Series(num_ttas, index=df.columns, name='num_tta'))\n",
    "#         df2 = df2.append(pd.Series([int(t[0]) for t in th_vals], index=df.columns, name='th0_val'))\n",
    "        \n",
    "#         df2 = df2.transpose()\n",
    "#         for f in int_format:\n",
    "#             df2[f]=df2[f].astype(int)\n",
    "#         for f in float_format:\n",
    "#             df2[f]=df2[f].astype(float)\n",
    "#         for f in ['F1','th0_val','num_tta','num_th','run_number']:\n",
    "#             df2[f] = pd.to_numeric(df2[f])\n",
    "#         df2.index = range(len(th_vals))\n",
    "#         df2[\"percent_FN\"] = 0.01*df2[\"percent_FN\"]\n",
    "#         df2[\"percent_FP\"] = 0.01*df2[\"percent_FP\"]\n",
    "        \n",
    "        \n",
    "# #         print(df2.dtypes)\n",
    "\n",
    "# #         df2.index = [int(t[0]) for t in th_vals]\n",
    "        \n",
    "# #         numbers = [show_names[i] for i in index_lists[0]]\n",
    "\n",
    "\n",
    "# #         tmp_df = pd.DataFrame(show_names)\n",
    "# #         print(tmp_df)\n",
    "# #         df2 = df.append(tmp_df.transpose())\n",
    "# #         display(df2)\n",
    "# #         display(df)\n",
    "    \n",
    "# #         df.columns = show_names\n",
    "    \n",
    "#         #F1 \n",
    "#         F1 = df.transpose()[\"F1\"].to_numpy()\n",
    "    \n",
    "#         # Roc curves\n",
    "#         FN_rate = df.transpose()[\"percent_FN\"].to_numpy()\n",
    "# #         print(FN_rate)\n",
    "#         FP_rate = df.transpose()[\"percent_FP\"].to_numpy()\n",
    "\n",
    "        \n",
    "#         fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(5, 5))\n",
    "#         g = sns.lineplot(\n",
    "#                     data=df2,\n",
    "#                     x=\"th0_val\", y=\"F1\",\n",
    "# #                     markers=[\"o\"], \n",
    "# #                     kind=\"line\",\n",
    "#                     ax=ax\n",
    "#                 )\n",
    "#         g = sns.lineplot(\n",
    "#                     data=df2,\n",
    "#                     x=\"th0_val\", y=\"percent_FN\",\n",
    "# #                     markers=[\"o\"], \n",
    "# #                     kind=\"line\",\n",
    "#                     ax=ax\n",
    "#             )\n",
    "#         g = sns.lineplot(\n",
    "#                     data=df2,\n",
    "#                     x=\"th0_val\", y=\"percent_FP\",\n",
    "# #                     markers=[\"o\"], \n",
    "# #                     kind=\"line\",\n",
    "#                     ax=ax\n",
    "#             )\n",
    "#         g.set_ylim([0,1])\n",
    "#         fig.tight_layout()\n",
    "# #         g = sns.catplot(\n",
    "# #             data=df2.transpose(),\n",
    "# #             x=\"run_name\", y=\"F1\",\n",
    "# #             markers=[\"o\"], \n",
    "# #             linestyles=[\"-\", \"--\"],\n",
    "# #             kind=\"point\",\n",
    "# #         )\n",
    "# #         [plt.setp(ax.get_xticklabels(), rotation=90) for ax in g.axes.flat]\n",
    "# #         [a.invert_xaxis() for a in g.axes.flat]\n",
    "# #         [a.set_ylim([0,1]) for a in g.axes.flat]\n",
    "# #         g.tight_layout()\n",
    "\n",
    "# #         fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(5, 5))\n",
    "# # #         sns.scatterplot(\n",
    "# #         sns.boxplot(\n",
    "# #                     data=df2.transpose(),\n",
    "# # #                     kind=\"line\",\n",
    "# #                     x=\"run_name\", y=\"F1\",ax = ax\n",
    "# #                 )\n",
    "# #         for tick in ax.get_xticklabels():\n",
    "# #             tick.set_rotation(90)\n",
    "# #         fig.tight_layout()\n",
    "        \n",
    "# #         fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(5, 5))\n",
    "# #         #figure for CNN with single TH.\n",
    "# #         FN_rate = np.array([FN_rate[i] for i in index_lists[0]])\n",
    "# #         show_names_single_thresh = [show_names[i] for i in index_lists[0]]\n",
    "# #         ax[0].set_title(f\"Bin of sunspots of size  {bins[i]} -> {len(binned_gt[i])} in GroudTruth\")\n",
    "# #         ax[0].plot( show_names_single_thresh, FN_rate*0.01, marker='o')\n",
    "# #         ax[0].plot( show_names_single_thresh, FP_rate*0.01, marker='o')\n",
    "# #         ax[0].plot( show_names_single_thresh, F1, marker='o')\n",
    "# #         ax[0].invert_xaxis()\n",
    "# #         ax[0].legend(['FN_rate', 'FP_rate', 'F1 score'])\n",
    "# #         ax[0].set_ylim([0,1])\n",
    "# #         # set x_label vertically\n",
    "# #         for tick in ax[0].get_xticklabels():\n",
    "# #             tick.set_rotation(90)\n",
    "# #         fig.tight_layout()\n",
    "        \n",
    "        \n",
    "# #         #plot FP and FN rates along their index in a single plot, add a legend, set ticks vertically\n",
    "# #         fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(5, 5))\n",
    "# #         ax.set_title(f\"Bin of sunspots of size  {bins[i]} -> {len(binned_gt[i])} in GroudTruth\")\n",
    "# #         ax.plot( show_names, FN_rate*0.01, marker='o')\n",
    "# #         ax.plot( show_names, FP_rate*0.01, marker='o')\n",
    "# #         ax.plot( show_names, F1, marker='o')\n",
    "# #         ax.invert_xaxis()\n",
    "# #         ax.legend(['FN_rate', 'FP_rate', 'F1 score'])\n",
    "# #         ax.set_ylim([0,1])\n",
    "# #         # set x_label vertically\n",
    "# #         for tick in ax.get_xticklabels():\n",
    "# #             tick.set_rotation(90)\n",
    "# #         fig.tight_layout()\n",
    "\n",
    "#         for row in df.index:\n",
    "#             if row in float_format:\n",
    "#                 df.loc[row] = df.loc[row].map('{:,.2f}'.format)\n",
    "#             elif row in int_format:\n",
    "#                 df.loc[row] = df.loc[row].map('{:,.0f}'.format)\n",
    "\n",
    "#     #     df.loc[\"percent_missed\"] = pd.Series(100* df.loc[\"num_missed\"] / len(binned_gt[i]), dtype=int)\n",
    "\n",
    "#         styler = df.transpose().style\n",
    "#     #     pd.set_option('display.float_format',\n",
    "#     #       lambda x: '{:,0f}'.format(x) if( (abs(x) % 1) != 0 )else '{:,.0f}'.format(x))\n",
    "#         styles = [dict(selector=\"caption\", props=[(\"text-align\", \"center\"),\n",
    "#                                               (\"font-size\", \"120%\"),\n",
    "#                                               (\"font-weight\", \"bold\")])]\n",
    "#         styler.set_caption(f\"Bin of sunspots of size  {bins[i]} -> {len(binned_gt[i])} in GroudTruth\").set_table_styles(styles)\n",
    "#         styler.set_properties(**{'text-align': 'center'})\n",
    "#         display(styler)\n",
    "#     #     display(df.transpose())\n",
    "    \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Distribution of FPs along images\n",
    "def get_FP_distribution(seg_to_compare, FP_per_segmenter):\n",
    "    \n",
    "    distributions = {}\n",
    "    \n",
    "    for seg in tqdm(seg_to_compare):\n",
    "        cur_FPs = FP_per_segmenter[seg]\n",
    "        \n",
    "#         print(cur_FPs.keys())\n",
    "        \n",
    "#         for c_name, c_ids in (pbar :=tqdm(all_classes.items(), leave=False)):\n",
    "        cur_class_FPs = cur_FPs['foreground']\n",
    "        cur_images = {}\n",
    "\n",
    "#         print(cur_class_FPs.keys())\n",
    "        for b, bin_FPs  in cur_class_FPs.items():\n",
    "#             print(bin_FPs)\n",
    "            for FP in bin_FPs['false_positives']:\n",
    "\n",
    "#                 print(FP)\n",
    "                if FP['image'] not in cur_images:\n",
    "                    cur_images[FP['image']] = 0\n",
    "\n",
    "                cur_images[FP['image']] += 1\n",
    "\n",
    "#         print(cur_images)\n",
    "\n",
    "        distributions[seg] = cur_images\n",
    "\n",
    "    return distributions\n",
    "\n",
    "distros = get_FP_distribution(seg_to_compare, FP_per_segmenter['predictors'])\n",
    "\n",
    "for k,v in distros.items():\n",
    "    all_im_distro = {os.path.basename(test_im).split('.')[0] :0 for test_im in gt_lst }\n",
    "    for  im , num_fp in v.items():\n",
    "        all_im_distro[im] += num_fp\n",
    "        \n",
    "    plt.figure()\n",
    "    plt.title(k)\n",
    "    plt.xticks(range(len(all_im_distro.keys())),rotation=90)\n",
    "    plt.bar(range(len(all_im_distro.keys())), all_im_distro.values(), .5, color='g')\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style = 'whitegrid')\n",
    "fmri = sns.load_dataset(\"fmri\")\n",
    "fmri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# TOKEN + CONFUSION MATRIX\n",
    "\n",
    "def compare_generators_with_tokens(generators_names, scores_per_generator, metric='DICE', tolerance=0.05, normalized=False):\n",
    "    out_dict = {i:{} for i in range(len(bins))}\n",
    "       \n",
    "    for i, b in enumerate( bins[:]):\n",
    "        \n",
    "        bin_conf_matrix = np.zeros((len(generators_names),len(generators_names)))\n",
    "        \n",
    "        bin_tokens = {t:0 for t in generators_names}\n",
    "            \n",
    "        scores_fg_bg = {gen_name: np.array([item['value']\n",
    "                                            for item in scores_per_generator[gen_name]['foreground'][str(i)][metric]\n",
    "#                                             for item in scores_per_generator[gen_name][0][str(i)][metric]\n",
    "#                                             for item in scores_per_generator[gen_name][0][i][metric]\n",
    "                                           ]) \n",
    "                                                                    \n",
    "                        for gen_name in generators_names}\n",
    "        \n",
    "        stacked = np.expand_dims(scores_fg_bg[list(scores_fg_bg.keys())[0]].copy(), axis=0).T\n",
    "        for k in list(scores_fg_bg.keys())[1:]:\n",
    "            stacked = np.hstack((stacked, np.expand_dims(scores_fg_bg[k],axis=0).T))\n",
    "\n",
    "        \n",
    "        ### Find the winners for all sunspots (winners = those with metrics are in the tolerance range from\n",
    "        ### maximal value of metric)\n",
    "        max_stacked = np.max(stacked, axis=1)\n",
    "        print(max_stacked.shape)\n",
    "        for j, m in enumerate(max_stacked):\n",
    "#             winners = np.argwhere(stacked[j]== m).flatten().tolist()\n",
    "            winners = np.argwhere(abs(stacked[j] - m) < tolerance).flatten().tolist()\n",
    "            losers = [item for item in range(len(generators_names)) if not item in winners]\n",
    "                        \n",
    "            winners_names = [generators_names[w] for w in winners]\n",
    "\n",
    "            for w in winners_names: \n",
    "                bin_tokens[w] += 1\n",
    "            \n",
    "            for n in range(len(generators_names)):\n",
    "                if n in winners: ## all winners get 1 point \n",
    "                    for w in winners:\n",
    "                        bin_conf_matrix[n,w] += 1\n",
    "        if normalized:\n",
    "            bin_conf_matrix /= len(scores_fg_bg[list(scores_fg_bg.keys())[0]])\n",
    "                    \n",
    "        for t in generators_names:         \n",
    "            out_dict[i][t] = bin_tokens[t]\n",
    "        out_dict[i][\"confusion_matrix\"] = bin_conf_matrix\n",
    "#         print(out_dict)\n",
    "    \n",
    "    return out_dict\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "# scores_per_segmenter = deepcopy(score_per_segmenter['generators'])\n",
    "# scores_per_segmenter.update(score_per_segmenter['predictors'])\n",
    "\n",
    "# seg_to_compare = [k for k in scores_per_segmenter.keys()]\n",
    "# gen_to_compare =  [k for k in score_per_segmenter['generators'].keys()]\n",
    "gen_to_compare =  [\n",
    "    '2023_T300',\n",
    "#     '2023_T325',\n",
    "    '2023_T350',\n",
    "#     '2023_T375',\n",
    "    '2023_T400',\n",
    "#     '2023_T425',\n",
    "    '2023_T450',\n",
    "#     '2023_T475',\n",
    "    '2023_T500',                 \n",
    "    ]\n",
    "gen_to_compare_show_names = [ f'T{n.split(\"T\")[1:]}' for n in gen_to_compare]\n",
    "print(gen_to_compare_show_names)\n",
    "\n",
    "# pred_to_compare = [k for k in score_per_segmenter['predictors'].keys()]\n",
    "pred_to_compare = [\n",
    "    'CNN-2013-15_UNet_T500_StepLR_epoch_1_0TTA',\n",
    "#    'CNN-2013-15_UNet_T475_StepLR_epoch_1_0TTA', \n",
    "   'CNN-2013-15_UNet_T450_StepLR_epoch_1_0TTA',\n",
    "#    'CNN-2013-15_UNet_T425_StepLR_epoch_1_0TTA',\n",
    "   'CNN-2013-15_UNet_T400_StepLR_epoch_1_0TTA', \n",
    "#    'CNN-2013-15_UNet_T375_StepLR_epoch_1_0TTA', \n",
    "   'CNN-2013-15_UNet_T350_StepLR_epoch_1_0TTA', \n",
    "#    'CNN-2013-15_UNet_T325_StepLR_epoch_1_0TTA', \n",
    "   'CNN-2013-15_UNet_T300_StepLR_epoch_1_0TTA',\n",
    "    \n",
    "    \n",
    "]\n",
    "pred_to_compare.reverse()\n",
    "pred_to_compare_show_names = [ f\"CNN-{n.split('_')[2:3]}\" for n in pred_to_compare]\n",
    "\n",
    "pred_to_compare = [k for k,v in score_per_segmenter['predictors'].items()]\n",
    "# show_names = [f'CNN-{seg.split(\"_\")[2:3]}' for seg in seg_to_compare]\n",
    "pred_to_compare_show_names = [f'CNN-{re.search(\"CNN-2013-15_UNet_(.*)_StepLR*\", seg).group(1).split(\"_\")}'.replace(\" \",\"\") for seg in seg_to_compare]\n",
    "\n",
    "print(pred_to_compare_show_names)\n",
    "\n",
    "seg_to_compare = gen_to_compare + pred_to_compare\n",
    "\n",
    "scores_per_segmenter = {k: deepcopy(score_per_segmenter['generators'][k]) for k in gen_to_compare}\n",
    "scores_per_segmenter.update({k: deepcopy(score_per_segmenter['predictors'][k]) for k in pred_to_compare})\n",
    "\n",
    "\n",
    "\n",
    "bins_tokens = compare_generators_with_tokens(seg_to_compare,scores_per_segmenter, metric=\"IoU\",\n",
    "                                             tolerance=0.05, normalized=True)\n",
    "\n",
    "# print(bins_tokens[0])\n",
    "\n",
    "def style_diag(data):\n",
    "    diag_mask = pd.DataFrame(\"\", index=data.index, columns=data.columns)\n",
    "    min_axis = min(diag_mask.shape)\n",
    "    diag_mask.iloc[range(min_axis), range(min_axis)] = \"font-weight:bold\"\n",
    "    return diag_mask\n",
    "\n",
    "for i in range(len(bins)):\n",
    "#     print(bins[i])\n",
    "#     col_names = [t for t in seg_to_compare]\n",
    "#     col_names = [ f'{name}' for name in col_names]\n",
    "    col_names = gen_to_compare_show_names + pred_to_compare_show_names\n",
    "    \n",
    "    df = pd.DataFrame(bins_tokens[i][\"confusion_matrix\"], columns=col_names, index=col_names)\n",
    "    styles = [dict(selector=\"caption\", props=[(\"text-align\", \"center\"),\n",
    "                                              (\"font-size\", \"120%\"),\n",
    "                                              (\"font-weight\", \"bold\")])]\n",
    "    styler = df.style\n",
    "    styler.set_caption(f\"Token matrix for bin {bins[i]}\").set_table_styles(styles)\n",
    "    styler.format(na_rep='MISSING',formatter=\"{:.2f}\")\n",
    "    styler.apply(style_diag, axis=None)\n",
    "    styler.set_properties(**{'text-align': 'center'})\n",
    "    display(styler)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Poubelle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = np.array([0,1])\n",
    "b = np.random.randint(5, size=(2, 4))\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "c=np.zeros_like(b)\n",
    "for i in a:\n",
    "    print()\n",
    "    c[b == i] = 1\n",
    "    print(c)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 1\n",
    "str(str(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "ce394780a46863705f4bd951221470256d662bb5725b1c2ce0c5daa445587e9d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
