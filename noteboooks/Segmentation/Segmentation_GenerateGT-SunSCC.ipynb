{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "import os, sys\n",
    "\n",
    "sys.path.append(os.path.abspath(\"../..\"))\n",
    "\n",
    "from utilities.create_detection_masks import *\n",
    "from utilities import Otsu_Grid_utilities as otsu_grid\n",
    "from utilities import Otsu_Local_utilities as otsu_local\n",
    "\n",
    "import glob\n",
    "import cv2\n",
    "from skimage import data, filters, segmentation\n",
    "\n",
    "from skimage import data\n",
    "from skimage.filters.rank import entropy\n",
    "from skimage.morphology import disk, ball, black_tophat, white_tophat\n",
    "from skimage.measure import shannon_entropy, label, regionprops\n",
    "from skimage.morphology import square, disk\n",
    "from skimage.io import imread\n",
    "\n",
    "import scipy\n",
    "from scipy.signal import find_peaks\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib ipympl\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# from mpl_interactions import ipyplot as iplt\n",
    "import concurrent.futures\n",
    "import time\n",
    "from datetime import timedelta\n",
    "\n",
    "from itertools import repeat\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump_mask(dest_filename, mask):\n",
    "    m2 = mask.astype(np.uint8)\n",
    "    out_im = Image.fromarray(m2, mode='L')\n",
    "    out_im.save(dest_filename)\n",
    "    \n",
    "def load_file(filename):\n",
    "    basename = os.path.basename(filename)\n",
    "    base_no_ext = basename.split('.')[0]\n",
    "    # print(basename)\n",
    "\n",
    "    header = get_FITS_header(filename)\n",
    "    wl_resized  = imread(filename)\n",
    "\n",
    "    center = [wl_resized.shape[0]//2,wl_resized.shape[1]//2]\n",
    "    radius = header['SOLAR_R']\n",
    "\n",
    "    pixel_nb = header['NAXIS1']\n",
    "    pix_bit = pow(2, 12) - 1 \n",
    "    poly_order = 2\n",
    "\n",
    "    # centered images\n",
    "    xce = int(pixel_nb/2)\n",
    "    yce = int(pixel_nb/2)\n",
    "\n",
    "    pixMat_flat = clv_correction(wl_resized,\n",
    "                                header['SOLAR_R'],\n",
    "                                xce,\n",
    "                                yce,\n",
    "                                poly_order,\n",
    "                                pix_bit)\n",
    "\n",
    "    return pixMat_flat, center, radius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def local_hist_bbox_new(image, tophat_mask):\n",
    "    \n",
    "    values = image[tophat_mask == 1]\n",
    "    hist_val, hist_edges = np.histogram(values, bins=255) \n",
    "    \n",
    "    peaks, properties = find_peaks(hist_val,prominence=0.8*max(hist_val[50:255]))\n",
    "    print(peaks,properties)\n",
    "    \n",
    "    fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(10, 2))\n",
    "    ax.hist(values.ravel(), bins=255)\n",
    "    ax.vlines(hist_edges[peaks[-1]],0, max(hist_val) ,color='r')\n",
    "    ax.vlines(hist_edges[properties['left_bases'][-1]],0, max(hist_val) ,color='g')\n",
    "    ax.vlines(hist_edges[properties['right_bases'][-1]],0, max(hist_val) ,color='g')\n",
    "    fig.show()\n",
    "    \n",
    "    return \n",
    "\n",
    "def local_hist_bbox(image, bbox, padding):\n",
    "    # Bounding box (min_row, min_col, max_row, max_col)\n",
    "    init_region = image[bbox[0] : bbox[2] , bbox[1] : bbox[3]]\n",
    "    img_cpy= image.copy()\n",
    "    \n",
    "    #original image\n",
    "    region = img_cpy[bbox[0] : bbox[2] , bbox[1] : bbox[3]]\n",
    "    \n",
    "    center = [image.shape[0]//2, image.shape[1]//2]\n",
    "    outside = create_circular_mask( image.shape[1], image.shape[0] ,center, radius)\n",
    "    tmp = outside[bbox[0] : bbox[2] , bbox[1] : bbox[3]]\n",
    "    \n",
    "    tmp2 = np.stack((tmp[:,:,None], region[:,:, None]), axis =-1)\n",
    "    tmp2 = np.max(tmp2, axis=-1).squeeze()\n",
    "    \n",
    "    thresholds = search_max_threshold2(region, tmp2)\n",
    "    levels = np.digitize(tmp2, bins=thresholds)\n",
    "    \n",
    "    return levels, thresholds\n",
    "\n",
    "\n",
    "def find_bboxes(labels):\n",
    "    props_labels = regionprops(labels)\n",
    "    \n",
    "    bboxes = []\n",
    "    for prop in props_labels:\n",
    "        if prop.area > 15:\n",
    "            bbox = np.array(prop.bbox)\n",
    "            bboxes.append(bbox)\n",
    "\n",
    "    return bboxes  \n",
    "\n",
    "def get_tophat(tophat_radius, image):\n",
    "    \n",
    "    tophat_disk = disk(tophat_radius)\n",
    "\n",
    "    spots = speed_black_tophat(image, tophat_disk)\n",
    "    \n",
    "    return spots\n",
    "\n",
    "def speed_black_tophat(image,footprint=None,out=None):\n",
    "    kernel = footprint\n",
    "    blackhat = cv2.morphologyEx(image, cv2.MORPH_BLACKHAT, kernel)\n",
    "    \n",
    "    return blackhat\n",
    "\n",
    "def local_hist_bbox_2(image, sun_radius, mask, bbox, method='MAX'):\n",
    "    # Bounding box (min_row, min_col, max_row, max_col)\n",
    "    init_region = image[bbox[0] : bbox[2] , bbox[1] : bbox[3]]\n",
    "    img_cpy= image.copy()\n",
    "    \n",
    "    #original image\n",
    "    region = img_cpy[bbox[0] : bbox[2] , bbox[1] : bbox[3]]\n",
    "    \n",
    "    center = [image.shape[0]//2, image.shape[1]//2]\n",
    "    \n",
    "    outside = create_circular_mask( image.shape[1], image.shape[0] ,center, sun_radius)\n",
    "    tmp = outside[bbox[0] : bbox[2] , bbox[1] : bbox[3]]\n",
    "\n",
    "    \n",
    "    tmp2 = np.stack((tmp[:,:,None], region[:,:, None]), axis =-1)\n",
    "    tmp2 = np.max(tmp2, axis=-1).squeeze()\n",
    "    \n",
    "    thresholds = search_max_threshold3(region, mask, tmp2, method=method)\n",
    "    levels = np.digitize(tmp2, bins=thresholds)*mask\n",
    "    \n",
    "    return levels, thresholds\n",
    "\n",
    "def find_bboxes2(labels):\n",
    "    props_labels = regionprops(labels)\n",
    "    \n",
    "    bboxes = []\n",
    "    masks = []\n",
    "    for prop in props_labels:\n",
    "        if prop.area > 15:\n",
    "            bbox = np.array(prop.bbox)\n",
    "            bboxes.append(bbox)\n",
    "            masks.append(prop.image)\n",
    "\n",
    "    return bboxes , masks\n",
    "\n",
    "def bbox_f_tophat2(image, sun_radius, tophat_thresholds, tophat_radius, padding, method='MAX'):\n",
    "    padding = int(padding)\n",
    "    spots = get_tophat(tophat_radius, image)\n",
    "        \n",
    "    all_outs = []\n",
    "    for tophat_threshold in tophat_thresholds:\n",
    "    \n",
    "        spots_cpy = spots.copy()\n",
    "        spots_cpy[spots_cpy<=tophat_threshold] = 0\n",
    "        spots_cpy[spots_cpy>tophat_threshold] = 1\n",
    "\n",
    "\n",
    "        label_im = label(spots_cpy)\n",
    "        props_bboxes, props_masks = find_bboxes2(label_im)\n",
    "\n",
    "        out_mask = np.zeros_like(image)\n",
    "\n",
    "        #for i, bbox in tqdm(enumerate(props_bboxes[:])):\n",
    "        for i, bbox in enumerate(props_bboxes[:]):\n",
    "            prop_mask = props_masks[i]\n",
    "\n",
    "            minX,maxX = max(bbox[0]-padding, 0), min( bbox[2]+padding, out_mask.shape[0])\n",
    "            minY,maxY = max(bbox[1]-padding, 0), min( bbox[3]+padding, out_mask.shape[1])\n",
    "\n",
    "            bbox2 = [minX,minY,maxX,maxY]\n",
    "\n",
    "            deltasX = [bbox[0]-minX, (maxX-bbox[2])]\n",
    "            deltasY = [bbox[1]-minY, (maxY-bbox[3])]\n",
    "\n",
    "\n",
    "            new_prop_mask = np.zeros((maxX-minX,maxY-minY))\n",
    "            new_prop_mask[deltasX[0]:deltasX[0]+prop_mask.shape[0],\n",
    "                          deltasY[0]: deltasY[0]+prop_mask.shape[1]] = prop_mask\n",
    "\n",
    "            levels = local_mask(image, sun_radius, new_prop_mask, bbox2, method)       \n",
    "            #levels, _ = local_hist_bbox_2(image, sun_radius, new_prop_mask, bbox2, method)\n",
    "\n",
    "            #if _ != [0,0]:\n",
    "                #out_mask[bbox2[0] : bbox2[2] , bbox2[1] : bbox2[3]] += levels\n",
    "            out_mask[bbox2[0] : bbox2[2] , bbox2[1] : bbox2[3]] += levels\n",
    "\n",
    "        global final_mask\n",
    "        final_mask = out_mask.astype(np.uint8)\n",
    "        \n",
    "        all_outs.append((out_mask, spots))\n",
    "        \n",
    "    return all_outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_local_tophat(image, sun_radius, tophat_threshold=300, tophat_radius=35, padding=35, method='MAX'):  \n",
    "    local_masks = bbox_f_tophat2(image, sun_radius, tophat_threshold, tophat_radius, padding, method)\n",
    "\n",
    "    return local_masks\n",
    "\n",
    "def apply(img, root_dir, method, tophat_thresh=400,context_width = 5,tophat_radius = 40):\n",
    "    basename = os.path.splitext(os.path.basename(img))[0]\n",
    "    pmf, center, radius = load_file(img)\n",
    "    m = create_circular_mask(pmf.shape[0], pmf.shape[1], radius=radius)\n",
    "    pmf = pmf*m\n",
    "    try:\n",
    "        out_masks = apply_local_tophat(pmf, radius, tophat_thresh, tophat_radius, context_width, method=method)\n",
    "\n",
    "        assert len(out_masks) == len(tophat_thresh)\n",
    "\n",
    "        for i, t in enumerate(tophat_thresh):\n",
    "            dest_dir = os.path.join(root_dir, f'2023_T{t}')\n",
    "\n",
    "            if not os.path.exists(dest_dir):\n",
    "                os.makedirs(dest_dir)\n",
    "\n",
    "            outfile = os.path.join(dest_dir, basename+'.png')\n",
    "            cur_out_mask, _ = out_masks[i]\n",
    "            dump_mask(outfile, cur_out_mask)\n",
    "    except ValueError:\n",
    "        print(f'error on : {basename}')\n",
    "    \n",
    "    except AssertionError:\n",
    "        print(f'error on : {basename}')\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Cells to separate penumbra and umbra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def local_mask(image, sun_radius, mask, bbox, method='MAX'):\n",
    "    init_region = image[bbox[0] : bbox[2] , bbox[1] : bbox[3]]\n",
    "    img_cpy= image.copy()\n",
    "    \n",
    "    #original image\n",
    "    region = img_cpy[bbox[0] : bbox[2] , bbox[1] : bbox[3]]\n",
    "    \n",
    "    center = [image.shape[0]//2, image.shape[1]//2]\n",
    "    \n",
    "    outside = create_circular_mask( image.shape[1], image.shape[0] ,center, sun_radius)\n",
    "    tmp = outside[bbox[0] : bbox[2] , bbox[1] : bbox[3]]\n",
    "\n",
    "    tmp2 = np.stack((tmp[:,:,None], region[:,:, None]), axis =-1)\n",
    "    tmp2 = np.max(tmp2, axis=-1).squeeze()\n",
    "    \n",
    "    #levels =  discriminate_penumbra(region, mask, tmp2, method=method)\n",
    "    levels =  discriminate_penumbra2(region, mask, tmp2, method=method)\n",
    "    \n",
    "    return levels\n",
    "\n",
    "def discriminate_penumbra2(region, mask, tmp2, show=False, method='MAX'):\n",
    "    import cv2 as cv\n",
    "    \n",
    "    edges = auto_canny(tmp2, mask,sigma=0.33,show=show)\n",
    "    edges_V1 = edges.copy()\n",
    "    if np.sum(edges) == 0:\n",
    "        #print('failed')\n",
    "        for sig in [0.1 , 0.2, 0.3 ]:\n",
    "            edges = canny_with_pooling(tmp2,mask,sig, show=show)\n",
    "            if np.sum(edges) > 0:\n",
    "                break\n",
    "    try:\n",
    "        assert np.sum(edges) > 0\n",
    "    except AssertionError:\n",
    "        print('lol')\n",
    "        auto_canny(tmp2, mask,sigma=0.33,show=True)\n",
    "        print('canny_pooling')\n",
    "        canny_with_pooling(tmp2,mask,0.05, show=True)\n",
    "        canny_with_pooling(tmp2,mask,0.1, show=True)\n",
    "        canny_with_pooling(tmp2,mask,0.2, show=True)\n",
    "        canny_with_pooling(tmp2,mask,0.33, show=True)\n",
    "        canny_with_pooling(tmp2,mask,0.4, show=True)\n",
    "        canny_with_pooling(tmp2,mask,0.5, show=True)\n",
    "        print('lol2')\n",
    "        fig, ax = plt.subplots(nrows=1, ncols=5, figsize=(9, 3))\n",
    "        ax[0].imshow(tmp2,cmap='gray')\n",
    "        ax[1].imshow(region)\n",
    "        ax[2].imshow(edges_V1)\n",
    "        ax[3].imshow(edges)\n",
    "        ax[4].imshow(edges*mask)\n",
    "        plt.show()\n",
    "        \n",
    "        assert np.sum(edges) > 0\n",
    "        \n",
    "    \n",
    "    pen_um_mask = splitter(tmp2,mask,edges)\n",
    "                \n",
    "    return pen_um_mask\n",
    "\n",
    "def discriminate_penumbra(region, mask, tmp2, show=False, method='MAX'):\n",
    "    #Apply Canny filter inside\n",
    "    import cv2 as cv\n",
    "    \n",
    "    edges = auto_canny(tmp2, mask)\n",
    "    \n",
    "    splitter(tmp2,mask,edges)\n",
    "    if show:\n",
    "        fig, ax = plt.subplots(nrows=1, ncols=6, figsize=(9, 3))\n",
    "        ax[0].imshow(tmp2,cmap='gray')\n",
    "        ax[1].imshow(mask)\n",
    "        ax[2].imshow(edges)\n",
    "        ax[2].imshow(edges*mask)\n",
    "        ax[3].imshow(auto_canny(tmp2,mask,sigma=1))\n",
    "        ax[4].imshow(auto_canny(tmp2,mask,sigma=2))\n",
    "        ax[5].imshow(auto_canny(tmp2,mask,sigma=.1))\n",
    "        plt.show()\n",
    "            \n",
    "    return edges\n",
    "\n",
    "from skimage.transform import rescale, resize, downscale_local_mean\n",
    "def canny_with_pooling(img, mask, sigma, show=False):\n",
    "    \n",
    "    masked_img = img*mask\n",
    "    \n",
    "    minpool = skimage.measure.block_reduce(img, (2,2), np.min)\n",
    "    maxpool = skimage.measure.block_reduce(img, (2,2), np.max)\n",
    "    meanpool = skimage.measure.block_reduce(img, (2,2), np.mean)\n",
    "    \n",
    "    msk2 = skimage.measure.block_reduce(mask, (2,2), np.max)\n",
    "    \n",
    "    min_canny = auto_canny(minpool,msk2,sigma=sigma,show=show)\n",
    "    \n",
    "    thresh = 0.4\n",
    "    min_canny = resize(min_canny, img.shape,anti_aliasing=False)\n",
    "    min_canny = np.digitize(min_canny, bins=[thresh])\n",
    "    \n",
    "    #max_canny = auto_canny(maxpool,msk2,sigma=sigma)\n",
    "    #mean_canny = auto_canny(meanpool,msk2,sigma=sigma)\n",
    "    #max_canny = resize(max_canny, img.shape,anti_aliasing=True)\n",
    "    #max_canny = np.digitize(max_canny, bins=[thresh])\n",
    "    #mean_canny = resize(mean_canny, img.shape,anti_aliasing=True)\n",
    "    #mean_canny = np.digitize(mean_canny, bins=[thresh])\n",
    "    \n",
    "    \n",
    "    \n",
    "    return min_canny\n",
    "\n",
    "def auto_canny(image, mask, sigma=0.2, show=False):\n",
    "    # compute the median of the single channel pixel intensities\n",
    "    img = image.copy()\n",
    "    img = cv2.normalize(img,img, 0, 255, cv2.NORM_MINMAX)\n",
    "    img =img.astype(np.uint8)\n",
    "    #clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(4,4))\n",
    "    #img = clahe.apply(img)\n",
    "    \n",
    "    v = np.median(img[mask.astype(bool)])\n",
    "    \n",
    "    # apply automatic Canny edge detection using the computed median\n",
    "    lower = int(max(0, (1.0 - sigma) * v))\n",
    "    upper = int(min(255, (1.0 + sigma) * v))\n",
    "    #edged = cv2.Canny(img, lower, upper, L2gradient=True)\n",
    "    edged = cv2.Canny(img, lower, upper)\n",
    "    # return the edged image\n",
    "    \n",
    "    if show:\n",
    "        fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(9, 3))\n",
    "        ax[0].imshow(image,cmap='gray')\n",
    "        ax[1].imshow(edged,cmap='gray')\n",
    "        ax[2].hist(img.ravel(), bins=255) \n",
    "        hist_val, hist_edges = np.histogram(img, bins=255)\n",
    "        #hist_val, hist_edges = np.histogram(img[mask.astype(np.bool)], bins=255)\n",
    "        ax[2].vlines(v, 0, max(hist_val ), color='r')\n",
    "        ax[2].vlines((1.0 + sigma) * v, 0, max(hist_val) , color='b') \n",
    "        ax[2].vlines((1.0 - sigma) * v, 0, max(hist_val) , color='g') \n",
    "        fig.suptitle(f'SIGMA: {sigma}, median: {v}, lower {lower}, upper: {upper}')\n",
    "        plt.show()\n",
    "    \n",
    "    return edged\n",
    "\n",
    "def splitter(image,mask, canny_edges, show=False):\n",
    "    max_intensity = np.max(image[mask>0])\n",
    "    min_intensity = np.min(image[mask>0])\n",
    "    \n",
    "    num_linspace = 20\n",
    "    th = np.linspace(max_intensity, min_intensity, num=num_linspace, endpoint=False)\n",
    "    \n",
    "    umbra_tree = UmbraSplitTree(th, mask.copy(), canny_edges)\n",
    "    \n",
    "    if show:\n",
    "        fig, ax = plt.subplots(nrows=1, ncols=4+num_linspace, figsize=(num_linspace, 4))\n",
    "        #fig, ax = plt.subplots(nrows=1, ncols=4+num_linspace)\n",
    "        ax[0].imshow(image,cmap='gray')\n",
    "        ax[1].imshow(mask,cmap='gray')\n",
    "        ax[2].imshow(canny_edges,cmap='gray')\n",
    "        for a in ax:\n",
    "            a.axis('Off')\n",
    "    \n",
    "    for i,t in enumerate(th):\n",
    "        idx = image[:,:] < t\n",
    "        m2 = mask.copy()\n",
    "        m3 = np.zeros_like(m2)\n",
    "        \n",
    "        m2[idx] = 2\n",
    "        m3[idx] = 1\n",
    "        \n",
    "        umbra_tree.update(t, m3)\n",
    "        \n",
    "        if show:\n",
    "            ax[4+i].imshow(m2)\n",
    "    \n",
    "    best_mask = umbra_tree.get_best_mask()\n",
    "    best_mask += mask\n",
    "    \n",
    "    if show:\n",
    "        ax[3].imshow(image,cmap='gray')\n",
    "        ax[3].imshow(best_mask,alpha=.5)\n",
    "        plt.show()\n",
    "        \n",
    "    return best_mask\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "def compute_dist(edges_mask, edges_canny, showErrors=True):\n",
    "    st_mask = np.stack(edges_mask, axis=-1)\n",
    "    st_canny = np.stack(edges_canny, axis=-1)\n",
    "    \n",
    "    matrix = scipy.spatial.distance.cdist(st_mask, st_canny, metric='euclidean')\n",
    "    \n",
    "    if 0 in matrix.shape:\n",
    "        print(matrix.shape)\n",
    "        print(edges_mask)\n",
    "        print(edges_canny)\n",
    "    \n",
    "    min_distances = np.min(matrix,axis=-1)  \n",
    "    dist = st_mask.shape[0] + np.sum(min_distances)/((np.sum(min_distances==0)/st_mask.shape[0])+1e-10)\n",
    "    \n",
    "    return dist\n",
    "\n",
    "def canny_distance(mask, canny, show=False):\n",
    "    #find contours\n",
    "    contours, hierarchy = cv2.findContours(255*mask.astype(np.uint8), cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    #create an empty image for contours\n",
    "    img_contours = np.zeros(mask.shape)\n",
    "    # draw the contours on the empty image\n",
    "    img_contours = cv2.drawContours(img_contours, contours, -1, (1), 1)\n",
    "    \n",
    "    ones_mask = np.nonzero(img_contours)\n",
    "    ones_canny = np.nonzero(canny)\n",
    "    \n",
    "    try:\n",
    "        dist = compute_dist(ones_mask, ones_canny)\n",
    "    except ValueError:\n",
    "        fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(6, 3))\n",
    "        ax[0].imshow(mask ,cmap='gray')\n",
    "        ax[0].imshow(img_contours ,alpha=.5)\n",
    "        ax[1].imshow(canny,cmap='gray')\n",
    "        fig.show()\n",
    "        return np.Inf\n",
    "        \n",
    "    \n",
    "    if show:\n",
    "        fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(6, 3))\n",
    "        ax[0].imshow(mask ,cmap='gray')\n",
    "        ax[0].imshow(img_contours ,alpha=.5)\n",
    "        ax[1].imshow(canny,cmap='gray')\n",
    "        fig.suptitle(f'dist = {dist}')\n",
    "    \n",
    "    return dist\n",
    "\n",
    "class UmbraSplitTreeNode():\n",
    "    def __init__(self,th, mask, canny, parent):\n",
    "        self.parent = parent\n",
    "        \n",
    "        self.threshold_history = [th]\n",
    "        self.mask = mask.copy()\n",
    "        self.canny_edges = canny\n",
    "        \n",
    "        score = canny_distance(self.mask, self.canny_edges)\n",
    "        if self.parent is not None:\n",
    "            b = self.parent.best_canny\n",
    "            score = np.minimum(score, b)           \n",
    "        self.canny_scores = [score]\n",
    "        \n",
    "        self.best_canny = self.canny_scores[0]\n",
    "        self.best_mask = self.mask.copy()\n",
    "        \n",
    "        self.children = None\n",
    "        \n",
    "    def get_best(self):\n",
    "        return self.best_mask, self.best_canny\n",
    "        \n",
    "    def update(self, new_th, new_mask, show=False):\n",
    "        self.threshold_history.append(new_th)\n",
    "        \n",
    "        tmp_mask = self.mask * new_mask\n",
    "        \n",
    "        label_im = label(tmp_mask)\n",
    "        regions = regionprops(label_im)\n",
    "        \n",
    "        if show:\n",
    "            fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(6, 3))\n",
    "            ax[0].imshow(self.mask ,cmap='gray')\n",
    "            ax[0].imshow(new_mask , alpha=.5)\n",
    "            ax[1].imshow(tmp_mask,cmap='gray')\n",
    "\n",
    "        if len(regions) == 0: # region disappeared -> terminal status return mask with best canny fit\n",
    "            pass\n",
    "        elif len(regions)== 1: # region shrinked -> check for canny distance, update if better\n",
    "            self.mask = tmp_mask\n",
    "            score = canny_distance(self.mask, self.canny_edges)\n",
    "            if self.parent is not None:\n",
    "                b = self.parent.best_canny\n",
    "                score = np.minimum(score, b)\n",
    "                \n",
    "            self.canny_scores.append(score)\n",
    "            if score <= self.best_canny: # We fit better the canny edges\n",
    "                self.best_canny = score\n",
    "                self.best_mask = tmp_mask\n",
    "                if show:\n",
    "                    ax[1].set_title(f'best = {score}')\n",
    "                    \n",
    "        else: # region was split -> create nodes , 1 for each region\n",
    "            if show:\n",
    "                ax[1].set_title(f'region splitted')            \n",
    "            self.children = []\n",
    "            for i, region in enumerate(regions):\n",
    "                child_mask = np.zeros_like(tmp_mask)\n",
    "                child_mask[label_im == region.label] = 1\n",
    "                self.children.append(UmbraSplitTreeNode(new_th, child_mask, self.canny_edges, self))\n",
    "        if show:\n",
    "            plt.show()\n",
    "        \n",
    "        \n",
    "class UmbraSplitTree():\n",
    "    def __init__(self, th_init, mask_init, canny):\n",
    "        self.root = UmbraSplitTreeNode(th_init, mask_init, canny,None)\n",
    "        self.leaves = [self.root]\n",
    "        \n",
    "    def print(self):\n",
    "        indent_lvl = 0\n",
    "        to_print: List = [self.root]\n",
    "        added = [1]\n",
    "        while len(to_print)>0:\n",
    "            cur: UmbraSplitTreeNode = to_print.pop(0)\n",
    "            print(indent_lvl*\"\\t\",'TreeNode',': ', cur.best_canny, f'({cur.canny_scores[0]})')\n",
    "            added[-1] -= 1\n",
    "\n",
    "            if cur.children is not None:\n",
    "                to_print = cur.children + to_print\n",
    "                added.append(len(cur.children))\n",
    "                indent_lvl += 1\n",
    "\n",
    "            while (len(added)>0) and (added[-1] == 0):\n",
    "                indent_lvl -=1\n",
    "                added.pop(-1)\n",
    "        return\n",
    "        \n",
    "    def get_best_mask(self):\n",
    "        bests_lst = []\n",
    "        \n",
    "        for leaf in self.leaves:\n",
    "            cur  = leaf\n",
    "            best_on_path = cur\n",
    "            best_mask, best_score = cur.get_best()\n",
    "            \n",
    "            while cur.parent is not None:\n",
    "                cur_best_m, cur_best_score  = cur.get_best()\n",
    "                \n",
    "                if cur_best_score <= best_score:\n",
    "                    best_score = cur_best_score\n",
    "                    best_mask = cur_best_m\n",
    "                \n",
    "                cur = cur.parent\n",
    "                    \n",
    "            # print(f'leaf: {leaf.best_canny} -> best_on_branch: {best_score}')\n",
    "            bests_lst.append(best_mask)\n",
    "            \n",
    "        best_masks = np.stack(bests_lst, axis=-1)\n",
    "        or_mask = np.logical_or.reduce(best_masks,axis=-1)\n",
    "        \n",
    "        return or_mask.astype(np.float64)\n",
    "        \n",
    "    def update(self, new_th, new_mask,show=False):\n",
    "        n = len(self.leaves)\n",
    "        new_leaves = []\n",
    "        \n",
    "        #for leaf in tqdm(self.leaves):\n",
    "        for leaf in self.leaves:\n",
    "            leaf.update(new_th, new_mask)\n",
    "            if leaf.children is not None:\n",
    "                self.leaves.remove(leaf)\n",
    "                new_leaves.extend(leaf.children)\n",
    "                \n",
    "        self.leaves.extend(new_leaves)               \n",
    "        \n",
    "        if show:\n",
    "            tmp_len = len(self.leaves)\n",
    "            fig, ax = plt.subplots(nrows=1, ncols=tmp_len, figsize=(6, 3))\n",
    "            if tmp_len == 1:\n",
    "                ax.imshow(self.leaves[0].mask)\n",
    "            else:\n",
    "                for i, leaf in enumerate(self.leaves):\n",
    "                    ax[i].imshow(leaf.mask)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n"
     ]
    }
   ],
   "source": [
    "# images_dir = \"../../datasets/segmentation/ManualAnnotation\"\n",
    "root_dir = \"../../datasets/segmentation/ManualAnnotation\"\n",
    "# root_dir = \"../../datasets/segmentation/All/train\"\n",
    "# root_dir = \"../../datasets/segmentation/All/test\"\n",
    "# root_dir = \"../../datasets/segmentation/test_GT\"\n",
    "images_dir = os.path.join(root_dir,\"image\")\n",
    "\n",
    "images = sorted(glob.glob(os.path.join(images_dir, '**/*.FTS'),recursive=True))\n",
    "\n",
    "print(len(images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "663438bd38a142bb9698aefd8217517c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "msks = sorted(glob.glob(os.path.join(images_dir, '**/*.FTS'),recursive=True))\n",
    "for m in tqdm(msks[:]):\n",
    "    cur = imread(m)\n",
    "#     print(cur.shape)\n",
    "    assert cur.shape == (2048,2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73072dcb14c84b3db92687d6a4335949",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total elapsed time 0:00:42.832532\n"
     ]
    }
   ],
   "source": [
    "st = 25\n",
    "# tophat_thresh= range(250, 500 + st, st)\n",
    "tophat_thresh = [\n",
    "    525,\n",
    "#     500,\n",
    "#     475,\n",
    "#     450,\n",
    "#     425,\n",
    "#     400,\n",
    "#     375,\n",
    "#     350,\n",
    "#     325,\n",
    "#     300,\n",
    "#     275,\n",
    "#     250,\n",
    "    \n",
    "]\n",
    "\n",
    "context_width = 5\n",
    "tophat_radius = 40\n",
    "padding=5\n",
    "\n",
    "methods = ['MIN']\n",
    "\n",
    "num_cpu = multiprocessing.cpu_count()\n",
    "num_cpu = 16\n",
    "\n",
    "for method in methods:\n",
    "\n",
    "#         executor = concurrent.futures.ProcessPoolExecutor(max_workers=int(np.floor(0.9*num_cpu)))\n",
    "        with concurrent.futures.ProcessPoolExecutor(max_workers=int(np.floor(0.9*num_cpu))) as executor:\n",
    "            to_process = images[:]\n",
    "#             to_process = images[964:]\n",
    "            # to_process = images[225:230]\n",
    "#             to_process = images[230:240]\n",
    "#             to_process = images[:1]\n",
    "            #to_process = images[3882:]\n",
    "        #     to_process = images[3883:]\n",
    "\n",
    "            start = time.time()\n",
    "            for i in tqdm(executor.map(apply, to_process, repeat(root_dir), repeat(method),\n",
    "                                       repeat(tophat_thresh), repeat(padding), repeat(tophat_radius))):\n",
    "                    pass\n",
    "            end = time.time()\n",
    "            print(f\"Total elapsed time {str(timedelta(seconds=end - start))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# tophat_thresh=400\n",
    "# context_width = 5\n",
    "# tophat_radius = 40\n",
    "# padding=5\n",
    "# dest_dir = os.path.join(root_dir,f'T{tophat_thresh}_canny')\n",
    "# # def apply(img, dest_dir, method,tophat_thresh=400,context_width = 5,tophat_radius = 40):\n",
    "\n",
    "# #for im in tqdm(images[10:11]):\n",
    "# #for im in tqdm(images[11:12]):\n",
    "# #for im in tqdm(images[16:17]):\n",
    "# #for im in tqdm(images[26:27]):\n",
    "# #or im in tqdm(images[29:30]):\n",
    "# #for im in tqdm(images[30:31]):\n",
    "# for im in tqdm(images[:]):\n",
    "#     apply(im, dest_dir, method=\"MAX\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert Backgound/Umbra/Penubra masks to Background/foreground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../datasets/segmentation/ManualAnnotation/2023_T525\n",
      "../../datasets/segmentation/ManualAnnotation/2023_T525_fgbg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbab9a6971d1480981828d2f3bae7441",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../datasets/segmentation/ManualAnnotation/2023_T500\n",
      "../../datasets/segmentation/ManualAnnotation/2023_T500_fgbg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08922f785c9d4472b4b7a47e4c39b83b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../datasets/segmentation/ManualAnnotation/2023_T475\n",
      "../../datasets/segmentation/ManualAnnotation/2023_T475_fgbg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "606954cfc6c540209d7a6d2f7cf707f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../datasets/segmentation/ManualAnnotation/2023_T450\n",
      "../../datasets/segmentation/ManualAnnotation/2023_T450_fgbg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3ac6ce421a04c25949ed69ee0fd5a2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../datasets/segmentation/ManualAnnotation/2023_T425\n",
      "../../datasets/segmentation/ManualAnnotation/2023_T425_fgbg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ef7cf75d9fb4392a045c159ed178de3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../datasets/segmentation/ManualAnnotation/2023_T400\n",
      "../../datasets/segmentation/ManualAnnotation/2023_T400_fgbg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51de19ea82a049be999d7db83da275d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../datasets/segmentation/ManualAnnotation/2023_T375\n",
      "../../datasets/segmentation/ManualAnnotation/2023_T375_fgbg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13c3127976cf47dc986cb10c15b55033",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../datasets/segmentation/ManualAnnotation/2023_T350\n",
      "../../datasets/segmentation/ManualAnnotation/2023_T350_fgbg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "533eedc5629148c79d43de51619ec509",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../datasets/segmentation/ManualAnnotation/2023_T325\n",
      "../../datasets/segmentation/ManualAnnotation/2023_T325_fgbg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1968bb9ca8f34ceea06a4f3afe8780a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../datasets/segmentation/ManualAnnotation/2023_T300\n",
      "../../datasets/segmentation/ManualAnnotation/2023_T300_fgbg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7624a3994884ff8ae0e7cb7870bf7ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../datasets/segmentation/ManualAnnotation/2023_T275\n",
      "../../datasets/segmentation/ManualAnnotation/2023_T275_fgbg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8acffe39ae943d28ceea933fc3f4cf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../datasets/segmentation/ManualAnnotation/2023_T250\n",
      "../../datasets/segmentation/ManualAnnotation/2023_T250_fgbg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89a567dd86de45b087aa682353f9f0ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import skimage.io as io\n",
    "\n",
    "# ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import concurrent.futures\n",
    "import numpy as np\n",
    "\n",
    "from itertools import repeat\n",
    "import multiprocessing\n",
    "\n",
    "tophat_thresh = [\n",
    "    525,\n",
    "    500,\n",
    "    475,\n",
    "    450,\n",
    "    425,\n",
    "    400,\n",
    "    375,\n",
    "    350,\n",
    "    325,\n",
    "    300,\n",
    "    275,\n",
    "    250,\n",
    "]\n",
    "\n",
    "\n",
    "num_cpu = multiprocessing.cpu_count()\n",
    "\n",
    "root_output_dir = \"../../datasets/segmentation/ManualAnnotation\"\n",
    "# root_output_dir = \"../../datasets/segmentation/All/test\"\n",
    "# root_output_dir = \"../../datasets/segmentation/All/train\"\n",
    "\n",
    "def to_fg_bg(m, cur_out_dir_fgbg):\n",
    "    cur_bn = os.path.basename(m).split('.')[0]\n",
    "    cur_fgbg = os.path.join(cur_out_dir_fgbg, cur_bn + '.png')\n",
    "    if not os.path.exists(cur_fgbg):\n",
    "        im = io.imread(m)\n",
    "        im[im>0] = 1\n",
    "        io.imsave(cur_fgbg, im)\n",
    "\n",
    "for t in tophat_thresh:\n",
    "    cur_out_dir = os.path.join(root_output_dir, f'2023_T{t}')\n",
    "    print(cur_out_dir)\n",
    "    cur_out_dir_fgbg = cur_out_dir + '_fgbg'\n",
    "    print(cur_out_dir_fgbg)\n",
    "    \n",
    "    if not os.path.exists(cur_out_dir_fgbg):\n",
    "        os.mkdir(cur_out_dir_fgbg)\n",
    "\n",
    "    msks = sorted(glob.glob(os.path.join(cur_out_dir, '**/*.png'),recursive=True))\n",
    "    \n",
    "    with concurrent.futures.ProcessPoolExecutor(max_workers=int(np.floor(0.9*num_cpu))) as executor:\n",
    "        for i in tqdm(executor.map(to_fg_bg, msks, repeat(cur_out_dir_fgbg))):\n",
    "            pass\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize the generated Masks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2023_T300_fgbg', '2023_T325_fgbg', '2023_T350_fgbg', '2023_T375_fgbg', '2023_T400_fgbg', '2023_T425_fgbg', '2023_T450_fgbg', '2023_T475_fgbg', '2023_T500_fgbg', '2023_T525_fgbg']\n",
      "36\n",
      "dict_keys(['2023_T300_fgbg', '2023_T325_fgbg', '2023_T350_fgbg', '2023_T375_fgbg', '2023_T400_fgbg', '2023_T425_fgbg', '2023_T450_fgbg', '2023_T475_fgbg', '2023_T500_fgbg', '2023_T525_fgbg'])\n",
      "[Checkbox(value=False, description='2023_T300_fgbg'), Checkbox(value=False, description='2023_T325_fgbg'), Checkbox(value=False, description='2023_T350_fgbg'), Checkbox(value=False, description='2023_T375_fgbg'), Checkbox(value=False, description='2023_T400_fgbg'), Checkbox(value=False, description='2023_T425_fgbg'), Checkbox(value=False, description='2023_T450_fgbg'), Checkbox(value=False, description='2023_T475_fgbg'), Checkbox(value=False, description='2023_T500_fgbg'), Checkbox(value=False, description='2023_T525_fgbg')]\n",
      "uint16\n",
      "uint8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fefd51b6703436bb727faf9277b8dbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntSlider(value=0, description='Image Index', max=35), Checkbox(value=False, desâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "\n",
    "from matplotlib import cm\n",
    "from matplotlib.colors import ListedColormap, LinearSegmentedColormap, Normalize\n",
    "\n",
    "#dataset_root = \"../../datasets/segmentation/All/test_GT\"\n",
    "# dataset_root = \"../../datasets/segmentation/All/train\"\n",
    "# dataset_root = \"../../datasets/segmentation/All/test\"\n",
    "dataset_root = \"../../datasets/segmentation/ManualAnnotation\"\n",
    "\n",
    "st = 25\n",
    "\n",
    "# tophat_thresh= range(250, 500 + st, st)\n",
    "tophat_thresh = range(300, 525 + st, st)\n",
    "seg_types = [ f'2023_T{t}_fgbg' for t in tophat_thresh ]\n",
    "\n",
    "print(seg_types)\n",
    "\n",
    "\n",
    "image_folder     = os.path.join(dataset_root, 'image')\n",
    "gt_folder        = os.path.join(dataset_root, 'GroundTruth')\n",
    "generated_folders = {t : os.path.join(dataset_root, t) for t in seg_types}\n",
    "\n",
    "image_lst = sorted(glob.glob(os.path.join(image_folder, '**/*.FTS'), recursive=True))\n",
    "print(len(image_lst))\n",
    "\n",
    "generated_lsts = {t : sorted(glob.glob(os.path.join(generated_folders[t], '*.png'))) for t in seg_types}\n",
    "gt_lst = sorted(glob.glob(os.path.join(gt_folder, '*.png')))\n",
    "\n",
    "print(generated_lsts.keys())\n",
    "\n",
    "max_idx = len(image_lst)\n",
    "\n",
    "cmap_gen = cm.turbo\n",
    "cmap_gen = cmap_gen(range(255))\n",
    "cmap_gen = ListedColormap([(0, 0, 0, 0), *cmap_gen])\n",
    "\n",
    "cmap_gt = cm.autumn\n",
    "cmap_gt = cmap_gt(range(255))\n",
    "cmap_gt = ListedColormap([(0, 0, 0, 0), *cmap_gt])\n",
    "\n",
    "\n",
    "def refresh(slider): \n",
    "    xlims0 = axes0[0].get_xlim()\n",
    "    ylims0 = axes0[0].get_ylim()\n",
    "    \n",
    "    axes0[0].clear()\n",
    "    \n",
    "    test_img = imread(image_lst[idx_slider.value])\n",
    "    \n",
    "    axes0[0].set_title(os.path.basename(image_lst[idx_slider.value]))\n",
    "    \n",
    "    if img_cb.value:\n",
    "        axes0[0].imshow(test_img, cmap=\"gray\", interpolation=\"None\")\n",
    "#         axes0[0].invert_yaxis()\n",
    "        axes0[0].get_xlim()[::-1]\n",
    "    if gt_cb.value:\n",
    "        gt_label = imread(gt_lst[idx_slider.value])\n",
    "        axes0[0].imshow(gt_label, cmap=cmap_gt, interpolation=\"None\", alpha=.5)\n",
    "    for i, cb in enumerate(gen_cbs):\n",
    "        if cb.value:\n",
    "            tmp = imread(generated_lsts[seg_types[i]][idx_slider.value])\n",
    "            cs = axes0[0].imshow(tmp, cmap=cmap_gen, interpolation=\"None\", alpha=.7)\n",
    "        \n",
    "    if xlims0 != (0.0, 1.0):\n",
    "        axes0[0].set_xlim(xlims0)\n",
    "        axes0[0].set_ylim(ylims0)\n",
    "        \n",
    "    \n",
    "    return\n",
    "\n",
    "max_rows = 1\n",
    "max_cols = 1\n",
    "\n",
    "plt.ioff()\n",
    "plt.style.use('default')\n",
    "fig_widget0, axes0 = plt.subplots(nrows=max_rows, ncols=max_cols, figsize=(8,5))\n",
    "try:\n",
    "    len(axes0)\n",
    "except TypeError:\n",
    "    axes0 = [axes0]\n",
    "\n",
    "plt.ion()\n",
    "img_cb = widgets.Checkbox(value=True, description='img')\n",
    "gt_cb = widgets.Checkbox(value=False, description='gt')\n",
    "gen_cbs = [widgets.Checkbox(value=False, description=f'{t.replace(\"COMPARE_LTH_\",\"\")}') for t in seg_types]\n",
    "idx_slider = widgets.IntSlider(value=0, min=0, max=max_idx-1, step=1, description=\"Image Index\")\n",
    "\n",
    "\n",
    "# Input image to predict\n",
    "test_img = imread(image_lst[0])\n",
    "#prediction\n",
    "gen_label = imread(generated_lsts[seg_types[0]][0])\n",
    "\n",
    "print(gen_cbs)\n",
    "print(test_img.dtype)\n",
    "print(gen_label.dtype)\n",
    "\n",
    "axes0[0].set_title(os.path.basename(image_lst[0]))\n",
    "axes0[0].imshow(test_img, cmap=\"gray\", interpolation=\"None\")\n",
    "\n",
    "\n",
    "img_cb.observe(refresh, names='value')\n",
    "gt_cb.observe(refresh, names='value')\n",
    "for cb in gen_cbs:\n",
    "    cb.observe(refresh, names='value')\n",
    "idx_slider.observe(refresh, names='value')\n",
    "\n",
    "h_len = 4\n",
    "\n",
    "vbox = widgets.VBox([ widgets.HBox(gen_cbs[i*h_len:(i+1)*h_len]) for i in range(ceil(len(seg_types)/h_len)) ])\n",
    "\n",
    "widgets.VBox([widgets.HBox([idx_slider,gt_cb]), vbox, fig_widget0.canvas])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verify that masks are correctly created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/globalscratch/users/n/s/nsayez/deepsun_bioblue/All/test/2023_T500_fgbg\n",
      "1199\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13ce05ec5c32451eba7a0287ca68fe2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tophat_thresh = [\n",
    "    525,\n",
    "#     500,\n",
    "#     475,\n",
    "#     450,\n",
    "#     425,\n",
    "#     400,\n",
    "#     375,\n",
    "#     350,\n",
    "#     325,\n",
    "#     300,\n",
    "]\n",
    "\n",
    "\n",
    "# root_dir = \"../../datasets/segmentation/All/train\"\n",
    "root_output_dir = \"../../datasets/segmentation/All/test\"\n",
    "# root_output_dir = \"../../datasets/segmentation/All/train\"\n",
    "\n",
    "erroneous_masks = {t:[] for t in tophat_thresh}\n",
    "for t in tophat_thresh:\n",
    "    cur_out_dir = os.path.join(root_output_dir, f'2023_T{t}_fgbg')\n",
    "    print(cur_out_dir)\n",
    "    msks = sorted(glob.glob(os.path.join(cur_out_dir, '**/*.png'),recursive=True))\n",
    "    \n",
    "    print(len(msks))\n",
    "    for i,m in tqdm(enumerate(msks[200:220])):\n",
    "#     for i,m in tqdm(enumerate(msks[:])):\n",
    "        try:\n",
    "            cur = imread(m)\n",
    "        #     print(cur.shape)\n",
    "            assert cur.shape == (2048,2048)\n",
    "        except:\n",
    "            erroneous_masks[t].append(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Format of next cell: threshold, num_errors -> errors indices\n",
    "\n",
    "Ok if 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500   0 -> []\n",
      "475   0 -> []\n",
      "450   0 -> []\n",
      "425   0 -> []\n",
      "400   0 -> []\n",
      "375   0 -> []\n",
      "350   0 -> []\n",
      "325   0 -> []\n",
      "300   0 -> []\n"
     ]
    }
   ],
   "source": [
    "for k,v in erroneous_masks.items():\n",
    "    print(k, ' ', len(v),'->',v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UPH20150716131500\n",
      "UPH20150716141018\n",
      "UPH20150716141019\n",
      "UPH20150716141021\n",
      "UPH20150716141022\n",
      "UPH20150716141023\n",
      "UPH20150716141024\n",
      "UPH20150716141025\n",
      "UPH20150716141500\n",
      "UPH20150716141525\n",
      "UPH20150716143000\n",
      "UPH20150716144500\n",
      "UPH20150716144904\n",
      "['UPH20150716131500', 'UPH20150716141018', 'UPH20150716141019', 'UPH20150716141021', 'UPH20150716141022', 'UPH20150716141023', 'UPH20150716141024', 'UPH20150716141025', 'UPH20150716141500', 'UPH20150716141525', 'UPH20150716143000', 'UPH20150716144500', 'UPH20150716144904']\n"
     ]
    }
   ],
   "source": [
    "to_remove = []\n",
    "for name in images[3877:3890]:\n",
    "    to_remove.append(os.path.basename(name).split('.')[0])\n",
    "    print(to_remove[-1])\n",
    "    \n",
    "print(to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4777\n",
      "T300_canny\n",
      "4777\n",
      "T350_canny\n",
      "4777\n",
      "T400_canny\n",
      "4777\n",
      "T450_canny\n",
      "4777\n"
     ]
    }
   ],
   "source": [
    "fits = sorted(glob.glob(os.path.join(dataset_root, 'image','*')))\n",
    "print(len(fits))\n",
    "\n",
    "masks_dirs = [  'T300_canny',\n",
    "                'T350_canny',\n",
    "                'T400_canny',\n",
    "                'T450_canny',\n",
    "             ]\n",
    "              \n",
    "for cur_name in fits:\n",
    "    # print(cur_name)\n",
    "    cur_bn = os.path.basename(cur_name).split('.')[0]\n",
    "    # print(cur_bn)\n",
    "    if cur_bn in to_remove:\n",
    "        print('delete')\n",
    "        os.remove(cur_name)\n",
    "        \n",
    "    \n",
    "for th in masks_dirs:\n",
    "    print(th)\n",
    "    cur_dir = os.path.join(dataset_root, th)\n",
    "    \n",
    "    files = sorted(glob.glob(os.path.join(dataset_root, th,'*')))\n",
    "    \n",
    "    for cur_name in files:\n",
    "        # print(cur_name)\n",
    "        cur_bn = os.path.basename(cur_name).split('.')[0]\n",
    "        # print(cur_bn)\n",
    "        if cur_bn in to_remove:\n",
    "            print('delete')\n",
    "            os.remove(cur_name)\n",
    "            \n",
    "    print(len(files))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "vscode": {
   "interpreter": {
    "hash": "1f3e85867ab3feeb73691fcc67a502ec8f0fc265745d17c9ab3a5329e7f22e4e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
