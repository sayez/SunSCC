{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "import concurrent.futures\n",
    "from itertools import repeat\n",
    "\n",
    "import json\n",
    "from copy import deepcopy\n",
    "import collections\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from functools import partial\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "from astropy.io import fits\n",
    "import skimage.io as io\n",
    "\n",
    "from ...sunscc.dataset.utils import *\n",
    "from ...sunscc.dataset.transform.pipelines import Compose \n",
    "from albumentations.core.transforms_interface import DualTransform\n",
    "\n",
    "from hydra.utils import call, instantiate\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib widget\n",
    "# %matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_elapsed_time(st, msg):\n",
    "        end_time = time.time()\n",
    "        print(f'Elapsed time {msg}: {end_time - st}')\n",
    "\n",
    "class ClassificationDatasetSuperclasses(Dataset):\n",
    "\n",
    "    def __init__(\n",
    "        self, root_dir, partition, dtypes, classes,\n",
    "        first_classes, second_classes, third_classes, \n",
    "        json_file, classification='SuperClass' , transforms=None) -> None:\n",
    "        super().__init__()\n",
    "        if isinstance(transforms, collections.Mapping):\n",
    "            transforms = partial(call, config=transforms)\n",
    "        elif isinstance(transforms, collections.Sequence):\n",
    "            transforms_init = []\n",
    "            for transform in transforms:\n",
    "                transforms_init.append(instantiate(transform))\n",
    "            transforms = Compose(transforms_init)\n",
    "\n",
    "        self.transforms = transforms\n",
    "        self.root_dir = Path(root_dir)\n",
    "\n",
    "        self.main_dtype = dtypes[0]\n",
    "        self.mask_dtype = dtypes[1]\n",
    "\n",
    "        self.json_file = self.root_dir / json_file \n",
    "        # print(self.json_file)\n",
    "        self.partition_dict = None\n",
    "\n",
    "        self.c1_mapper = {c: i for i,c in enumerate(first_classes)}\n",
    "        self.c2_mapper = {c: i for i,c in enumerate(second_classes)}\n",
    "        self.c3_mapper = {c: i for i,c in enumerate(third_classes)}\n",
    "\n",
    "\n",
    "        with open(self.json_file, 'r') as f:\n",
    "            self.partition_dict = json.load(f)[partition]\n",
    "\n",
    "        \n",
    "        assert (classification == 'Zurich') or (classification == 'McIntosh') or (classification == 'SuperClass')\n",
    "\n",
    "        self.classification = classification\n",
    "\n",
    "        self.FirstClass_mapper = {c: i for i,c in enumerate(first_classes)}\n",
    "        self.SecondClass_mapper = {c: i for i,c in enumerate(second_classes)}\n",
    "        self.ThirdClass_mapper = {c: i for i,c in enumerate(third_classes)}\n",
    "\n",
    "        # print(classes)\n",
    "        self.files = {}\n",
    "        for i, bn in enumerate(sorted(list(self.partition_dict.keys()))):\n",
    "            bn = bn.split('_')[0]\n",
    "            # print(bn)\n",
    "            cur = {}\n",
    "            image_basename = bn + '.FTS'\n",
    "            image_filename = self.root_dir / self.main_dtype / image_basename\n",
    "\n",
    "            sun_mask_filename = self.root_dir / 'sun_mask' / (bn + '.png')\n",
    "\n",
    "\n",
    "            mask_basename = bn + '.png'\n",
    "            mask_filename = self.root_dir / self.mask_dtype / mask_basename\n",
    "\n",
    "            conf_map_basename = bn + '_proba_map.npy'\n",
    "            conf_map_filename = self.root_dir / self.mask_dtype / conf_map_basename\n",
    "\n",
    "            cur[\"name\"] = bn\n",
    "            cur[self.main_dtype] = image_filename\n",
    "            cur[self.mask_dtype] = mask_filename\n",
    "            cur[self.mask_dtype+\"_conf_map\"] = conf_map_filename\n",
    "            cur[\"sun_mask\"] = sun_mask_filename\n",
    "\n",
    "            self.files[bn] = cur\n",
    "\n",
    "        self.partition_dict\n",
    "\n",
    "        self.groups = {}\n",
    "        for k,v in self.partition_dict.items():\n",
    "\n",
    "            if v[self.classification][\"1\"] in classes:\n",
    "                    self.groups[k] = v\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "        self.dataset_length = len(list(self.groups.keys()))\n",
    "\n",
    "\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.dataset_length\n",
    "    \n",
    "    def __getitem__(self, index: int, do_transform=True):\n",
    "\n",
    "        sample = {} # dictionnary with 'image', 'class', 'angular_excentricity', 'centroid_lat'\n",
    "\n",
    "        k = sorted(list(self.groups.keys()))[index]\n",
    "        basename = k.split('_')[0]\n",
    "\n",
    "        group_dict = self.groups[k]\n",
    "\n",
    "        img_name = self.files[basename][self.main_dtype] # path of FITS file\n",
    "        mask_name = self.files[basename][self.mask_dtype]\n",
    "        conf_map_name = self.files[basename][self.mask_dtype+\"_conf_map\"]\n",
    "\n",
    "        hdulst:fits.HDUList = fits.open(img_name)\n",
    "        image = hdulst[0]\n",
    "        header = image.header\n",
    "        center = np.array(image.shape)//2\n",
    "        radius = header['SOLAR_R']\n",
    "        \n",
    "        sample['solar_disk'] = io.imread(self.files[basename][\"sun_mask\"])\n",
    "        sample['excentricity_map'] = create_excentricity_map(sample['solar_disk'], radius, value_outside=-1)\n",
    "        sample['mask'] = io.imread(mask_name)#.astype(float)\n",
    "        sample['confidence_map'] = np.load(conf_map_name)\n",
    "\n",
    "        sample['image'] = (image.data).astype(float)\n",
    "\n",
    "        sample['members'] = np.array(group_dict['members']) if 'members' in group_dict else np.array([0])\n",
    "        sample['members_mean_px'] = np.array(group_dict['members_mean_px']) if 'members_mean_px' in group_dict else np.array([0])\n",
    "\n",
    "        sample['name'] = basename\n",
    "        sample['group_name'] = k\n",
    "\n",
    "        sample['solar_angle'] = group_dict['angle']\n",
    "        sample['deltashapeX'] = group_dict['deltashapeX']\n",
    "        sample['deltashapeY'] = group_dict['deltashapeY']\n",
    "        \n",
    "        sample['angular_excentricity'] = np.array([group_dict[\"angular_excentricity_deg\"]])\n",
    "        sample['centroid_px'] = np.array(group_dict[\"centroid_px\"])\n",
    "        sample['centroid_Lat'] = np.array([group_dict[\"centroid_Lat\"]])\n",
    "\n",
    "        sample['class1'] = group_dict[self.classification]['1']\n",
    "        sample['class2'] = group_dict[self.classification]['2']\n",
    "        sample['class3'] = group_dict[self.classification]['3']\n",
    "        \n",
    "        if sample[\"image\"].shape == (1024,1024):\n",
    "            fig,ax = plt.subplots(2, 5, figsize=(10, 4) )\n",
    "            ax[0,0].imshow(sample[\"image\"], cmap='gray', interpolation='none')\n",
    "            ax[0,1].imshow(sample[\"mask\"], cmap='gray', interpolation='none')\n",
    "            ax[0,2].imshow(sample[\"confidence_map\"], cmap='gray', interpolation='none')\n",
    "            ax[0,3].imshow(sample[\"solar_disk\"], cmap='gray', interpolation='none')\n",
    "            ax[0,4].imshow(sample[\"excentricity_map\"], cmap='gray', interpolation='none')\n",
    "        \n",
    "            ax[0,0].scatter(sample['centroid_px'][0], sample['centroid_px'][1], c='r', s=10)\n",
    "\n",
    "            sample[\"image\"] = np.repeat(np.repeat(sample[\"image\"], 2, axis=0), 2, axis=1)\n",
    "            sample[\"mask\"] = np.repeat(np.repeat(sample[\"mask\"], 2, axis=0), 2, axis=1)\n",
    "            sample[\"confidence_map\"] = np.repeat(np.repeat(sample[\"confidence_map\"], 2, axis=0), 2, axis=1)\n",
    "            sample[\"solar_disk\"] = np.repeat(np.repeat(sample[\"solar_disk\"], 2, axis=0), 2, axis=1)\n",
    "            sample[\"excentricity_map\"] = np.repeat(np.repeat(sample[\"excentricity_map\"], 2, axis=0), 2, axis=1)\n",
    "        \n",
    "            sample['deltashapeX'] = sample['deltashapeX']*2\n",
    "            sample['deltashapeY'] = sample['deltashapeY']*2\n",
    "\n",
    "            # also double the centroid values\n",
    "            sample['centroid_px'] = sample['centroid_px']*2\n",
    "\n",
    "\n",
    "            ax[1,0].imshow(sample[\"image\"], cmap='gray', interpolation='none')\n",
    "            ax[1,1].imshow(sample[\"mask\"], cmap='gray', interpolation='none')\n",
    "            ax[1,2].imshow(sample[\"confidence_map\"], cmap='gray', interpolation='none')\n",
    "            ax[1,3].imshow(sample[\"solar_disk\"], cmap='gray', interpolation='none')\n",
    "            ax[1,4].imshow(sample[\"excentricity_map\"], cmap='gray', interpolation='none')\n",
    "            # scatter the centroid\n",
    "            ax[1,0].scatter(sample['centroid_px'][0], sample['centroid_px'][1], c='r', s=10)\n",
    "\n",
    "            fig.tight_layout()\n",
    "            \n",
    "        flip_time = \"2003-03-08T00:00:00\"\n",
    "        date = whitelight_to_datetime(basename)\n",
    "        datetime_str = datetime_to_db_string(date).replace(' ', 'T')\n",
    "        \n",
    "        should_flip = (datetime.fromisoformat(datetime_str) - datetime.fromisoformat(flip_time)) < timedelta(0)\n",
    "        sample['should_flip'] = should_flip\n",
    "\n",
    "        if should_flip:\n",
    "            sample['image'] = np.flip(sample['image'],axis=0)\n",
    "            sample['solar_disk'] = np.flip(sample['solar_disk'],axis=0)\n",
    "            sample['mask'] = np.flip(sample['mask'],axis=0)\n",
    "            sample['confidence_map'] = np.flip(sample['confidence_map'],axis=0)\n",
    "            sample['excentricity_map'] = np.flip(sample['excentricity_map'],axis=0)\n",
    "\n",
    "        if self.transforms is not None and do_transform:\n",
    "            sample = self.transforms(**sample)\n",
    "        \n",
    "        return sample\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = \"../datasets/Classification_dataset/2002-2019\"\n",
    "partition = 'train'\n",
    "dtypes = ['image', 'T425-T375-T325_fgbg']\n",
    "\n",
    "classes = ['A','B','C','SuperGroup','H']\n",
    "first_classes = [ 'A','B','C','SuperGroup','H']\n",
    "second_classes= [ 'x','r','sym','asym']\n",
    "third_classes= [ \"x\",\"o\",\"frag\"]\n",
    "json_file = 'test/dataset_final.json'\n",
    "classification = 'SuperClass'\n",
    "transforms = OmegaConf.load('../sunscc/conf/exp/Classification_Superclasses4.yaml').dataset.train_dataset.transforms\n",
    "\n",
    "transforms[3].standard_height = 350\n",
    "transforms[3].standard_width = 350\n",
    "\n",
    "dataset =  ClassificationDatasetSuperclasses(root_dir, partition, dtypes, classes, first_classes, second_classes, third_classes, json_file, classification, transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition_samples = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample(idx, dataset):\n",
    "    tmp = dataset[idx]\n",
    "    center  = tmp['image'].shape[0]//2 , tmp['image'].shape[1]//2\n",
    "    slice_x = center[0]-( 256 //2 ), center[0]+ (256 //2 )\n",
    "    slice_y = center[1]-( 256 //2 ), center[1]+ (256 //2 )\n",
    "    center_region = tmp['confidence_map'][slice_x[0]:slice_x[1], slice_y[0]:slice_y[1]]\n",
    "    if np.sum(center_region) == 0:\n",
    "        print('skipping sample', idx)\n",
    "        return None\n",
    "    return dataset[idx]\n",
    "\n",
    "num_cpu = 15\n",
    "\n",
    "with concurrent.futures.ProcessPoolExecutor(max_workers=int(num_cpu)) as executor:\n",
    "    for sample in tqdm(executor.map(get_sample, range(len(dataset)) , repeat(deepcopy(dataset)))):\n",
    "    # for sample in tqdm(executor.map(get_sample, range(10) , repeat(deepcopy(dataset)))):\n",
    "        if sample is not None:\n",
    "            if sample['group_name'] not in partition_samples:\n",
    "                partition_samples[sample['group_name']] = sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npy_file = os.path.join(root_dir, 'test', f'all_samples_{partition}.npy')\n",
    "\n",
    "np.save(npy_file, partition_samples)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Join the train, val, test .npy files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_samples ={'train':{},'val':{},'test':{}}\n",
    "for p in all_samples.keys():\n",
    "    print('loading', p)\n",
    "    st = time.time()    \n",
    "    filename = os.path.join(root_dir,'test',f'all_samples_{p}.npy' )\n",
    "    tmp = np.load(filename, allow_pickle=True).item()\n",
    "    print('Elapsed time', time.time()-st)\n",
    "    \n",
    "    all_samples[p] = tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dumping\")\n",
    "st = time.time()    \n",
    "tot_npy_file = os.path.join(root_dir, 'test', f'all_samples.npy')\n",
    "np.save(tot_npy_file, all_samples)\n",
    "print('Elapsed time', time.time()-st)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_elapsed_time(st, msg):\n",
    "        end_time = time.time()\n",
    "        print(f'Elapsed time {msg}: {end_time - st}')\n",
    "        \n",
    "class Deepsun_Focus_Move(DualTransform):\n",
    "    def __init__(self, standard_height=256, standard_width=256,\n",
    "                        focus_on_group=True,\n",
    "                        random_move=False, random_move_percent=0.1,  \n",
    "                        always_apply=False, p=1.0) -> None:\n",
    "\n",
    "        super().__init__(always_apply, p)\n",
    "\n",
    "        self.standard_height = standard_height\n",
    "        self.standard_width = standard_width\n",
    "        \n",
    "        self.focus_on_group = focus_on_group\n",
    "        self.random_move = random_move\n",
    "        self.random_move_percent = random_move_percent\n",
    "\n",
    "    def get_bounding_box_around_group_with_padding(self, mask, offset):\n",
    "        # Get the bounding box around non-zero pixels in mask\n",
    "        x, y = np.nonzero(mask)\n",
    "        # print(x, y)\n",
    "        x1, x2 = (np.min(x), np.max(x)) if len(x) > 0 else (None, None)\n",
    "        y1, y2 = (np.min(y), np.max(y)) if len(y) > 0 else (None, None)\n",
    "\n",
    "        if (x1 is None) or (y1 is None):\n",
    "            return 0, mask.shape[0]-1, 0, mask.shape[1]-1\n",
    "\n",
    "        # Add padding\n",
    "        x1 -= offset\n",
    "        x2 += offset\n",
    "        y1 -= offset\n",
    "        y2 += offset\n",
    "\n",
    "\n",
    "        # Make sure the bounding box is not outside the image\n",
    "        x1 = max(x1, 0)\n",
    "        x2 = min(x2, mask.shape[0])\n",
    "        y1 = max(y1, 0)\n",
    "        y2 = min(y2, mask.shape[1])\n",
    "\n",
    "        return x1, x2, y1, y2\n",
    "\n",
    "    def adapt_bbox_to_image_size(self, bbox, image_size):\n",
    "        bbox_center = ((bbox[0] + bbox[1]) // 2, (bbox[2] + bbox[3]) // 2)\n",
    "        bbox_size = (bbox[1] - bbox[0], bbox[3] - bbox[2])\n",
    "\n",
    "        # if bbox is too small, expand it\n",
    "        minimal_percentage = .4\n",
    "\n",
    "        bbox_size = (max(bbox_size[0], image_size[0] * minimal_percentage),\n",
    "                     max(bbox_size[1], image_size[1] * minimal_percentage))\n",
    "        \n",
    "        return (int(bbox_center[0] - bbox_size[0] // 2), int(bbox_center[0] + bbox_size[0] // 2),\n",
    "                int(bbox_center[1] - bbox_size[1] // 2), int(bbox_center[1] + bbox_size[1] // 2))\n",
    "\n",
    "    def crop_img(self, img, bbox):\n",
    "        # Crop image\n",
    "        x1, x2, y1, y2 = bbox\n",
    "        img = img[x1:x2, y1:y2]\n",
    "        return img\n",
    "        \n",
    "    def padding(self, array, xx, yy):\n",
    "        \"\"\"\n",
    "        :param array: numpy array\n",
    "        :param xx: desired height\n",
    "        :param yy: desirex width\n",
    "        :return: padded array\n",
    "        \"\"\"\n",
    "\n",
    "        h = array.shape[0]\n",
    "        w = array.shape[1]\n",
    "\n",
    "        a = (xx - h) // 2\n",
    "        aa = xx - a - h\n",
    "\n",
    "        b = (yy - w) // 2\n",
    "        bb = yy - b - w\n",
    "\n",
    "        a = max(a,0)\n",
    "        b = max(b,0)\n",
    "        aa = max(aa,0)\n",
    "        bb = max(bb,0)\n",
    "\n",
    "        return np.pad(array, pad_width=((a, aa), (b, bb)), mode='constant')\n",
    "\n",
    "    def crop_and_pad(self, img, bbox, image_size):\n",
    "        \n",
    "        # Crop image\n",
    "        img = self.crop_img(img, bbox)\n",
    "        # Pad image\n",
    "        img = self.padding(img, image_size[0], image_size[1])\n",
    "        return img\n",
    "\n",
    "    def data_aug_random_move(self, bbox, max_offset):\n",
    "        '''\n",
    "        Randomly move the bounding box\n",
    "        param bbox: bounding box\n",
    "        param max_offset: maximum offset in portion of the bbox size\n",
    "        '''\n",
    "        # Randomly move the bounding box\n",
    "        x1, x2, y1, y2 = bbox\n",
    "        horizontal_offset = (np.random.random(1) * 2*max_offset) - max_offset\n",
    "        vertical_offset = (np.random.random(1) * 2*max_offset) - max_offset\n",
    "        \n",
    "        x1 += int(horizontal_offset * (bbox[1] - bbox[0]))\n",
    "        x2 += int(horizontal_offset * (bbox[1] - bbox[0]))\n",
    "        y1 += int(vertical_offset * (bbox[3] - bbox[2]))\n",
    "        y2 += int(vertical_offset * (bbox[3] - bbox[2]))\n",
    "        \n",
    "        return x1, x2, y1, y2\n",
    "        \n",
    "    def __call__(self, *args, force_apply=False, **kwargs):\n",
    "\n",
    "        img_group_crop = kwargs['image'].copy()\n",
    "        msk_group_crop = kwargs['mask'].copy()\n",
    "        grp_msk_group_crop = kwargs['group_mask'].copy()\n",
    "        disk_group_crop = kwargs['solar_disk'].copy()\n",
    "        excentricity_group_crop = kwargs['excentricity_map'].copy()\n",
    "        confidence_group_crop = kwargs['confidence_map'].copy()\n",
    "        grp_confidence_group_crop = kwargs['group_confidence_map'].copy()\n",
    "        \n",
    "        shape  = img_group_crop.shape\n",
    "        \n",
    "        bbox = self.get_bounding_box_around_group_with_padding((grp_confidence_group_crop>0), 10)\n",
    "\n",
    "        \n",
    "        minX, maxX, minY, maxY =  (\n",
    "                                    ((bbox[1]+bbox[0])//2)-(self.standard_height//2), \n",
    "                                    ((bbox[1]+bbox[0])//2)+(self.standard_height//2), \n",
    "                                    ((bbox[3]+bbox[2])//2)-(self.standard_width//2), \n",
    "                                    ((bbox[3]+bbox[2])//2)+(self.standard_width//2)\n",
    "                                    \n",
    "                                    )\n",
    "        \n",
    "            \n",
    "        if self.focus_on_group:\n",
    "            # focus on the group\n",
    "            # Modify the bounding box if data augmentation is enabled\n",
    "            if self.random_move:\n",
    "                # print('random_move')\n",
    "                bbox = self.data_aug_random_move(bbox, max_offset=self.random_move_percent)\n",
    "                                    \n",
    "                # Make sure the bounding box is not outside the image\n",
    "                x1, x2, y1, y2 = bbox\n",
    "                x1 = max(x1, 0)\n",
    "                x2 = min(x2, self.standard_width)\n",
    "                y1 = max(y1, 0)\n",
    "                y2 = min(y2, self.standard_height)\n",
    "                bbox = x1, x2, y1, y2\n",
    "            else:\n",
    "               \n",
    "                pass\n",
    "                \n",
    "            bbox = self.adapt_bbox_to_image_size( bbox, (self.standard_height, self.standard_width))\n",
    "            img_group_crop = self.crop_and_pad(img_group_crop, bbox, (self.standard_height, self.standard_width))\n",
    "            msk_group_crop = self.crop_and_pad(msk_group_crop, bbox, (self.standard_height, self.standard_width))\n",
    "            grp_msk_group_crop = self.crop_and_pad(grp_msk_group_crop, bbox, (self.standard_height, self.standard_width))\n",
    "            disk_group_crop = self.crop_and_pad(disk_group_crop, bbox, (self.standard_height, self.standard_width))\n",
    "            excentricity_group_crop = self.crop_and_pad(excentricity_group_crop, bbox, (self.standard_height, self.standard_width))\n",
    "            confidence_group_crop = self.crop_and_pad(confidence_group_crop, bbox, (self.standard_height, self.standard_width))\n",
    "            grp_confidence_group_crop = self.crop_and_pad(grp_confidence_group_crop, bbox, (self.standard_height, self.standard_width))\n",
    "        else:\n",
    "            if self.random_move:\n",
    "                \n",
    "                frac = np.max([(bbox[1]-bbox[0]) /self.standard_height, (bbox[3]-bbox[2]) /self.standard_width])\n",
    "                frac = np.sqrt(frac)\n",
    "                \n",
    "                bbox = self.data_aug_random_move([minX,maxX,minY,maxY], max_offset=self.random_move_percent*frac)\n",
    "                \n",
    "                if not ((bbox[1] > shape[0]) or (bbox[3] > shape[1]) or (bbox[0] < 0) or (bbox[2] < 0)):\n",
    "                    bbox = [bbox[0], bbox[0]+self.standard_height, bbox[2], bbox[3]]\n",
    "                    minX,maxX,minY,maxY = bbox\n",
    "                \n",
    "            else:\n",
    "                \n",
    "                pass\n",
    "                \n",
    "            img_group_crop = img_group_crop[minX:maxX,minY:maxY]\n",
    "            msk_group_crop = msk_group_crop[minX:maxX,minY:maxY]\n",
    "            grp_msk_group_crop = grp_msk_group_crop[minX:maxX,minY:maxY]\n",
    "            disk_group_crop = disk_group_crop[minX:maxX,minY:maxY]\n",
    "            excentricity_group_crop = excentricity_group_crop[minX:maxX,minY:maxY]\n",
    "            confidence_group_crop = confidence_group_crop[minX:maxX,minY:maxY]\n",
    "            grp_confidence_group_crop = grp_confidence_group_crop[minX:maxX,minY:maxY]\n",
    "\n",
    "        if img_group_crop.shape != (self.standard_height, self.standard_width):\n",
    "            img_group_crop = self.padding(img_group_crop, self.standard_height, self.standard_width)\n",
    "            msk_group_crop = self.padding(msk_group_crop, self.standard_height, self.standard_width)\n",
    "            grp_msk_group_crop = self.padding(grp_msk_group_crop, self.standard_height, self.standard_width)\n",
    "            disk_group_crop = self.padding(disk_group_crop, self.standard_height, self.standard_width)\n",
    "            excentricity_group_crop = self.padding(excentricity_group_crop, self.standard_height, self.standard_width)\n",
    "            confidence_group_crop = self.padding(confidence_group_crop, self.standard_height, self.standard_width)\n",
    "            grp_confidence_group_crop = self.padding(grp_confidence_group_crop, self.standard_height, self.standard_width)\n",
    "\n",
    "\n",
    "        assert img_group_crop.shape == (self.standard_height, self.standard_width)\n",
    "        assert msk_group_crop.shape == (self.standard_height, self.standard_width)\n",
    "        assert grp_msk_group_crop.shape == (self.standard_height, self.standard_width)\n",
    "        assert disk_group_crop.shape == (self.standard_height, self.standard_width)\n",
    "        assert excentricity_group_crop.shape == (self.standard_height, self.standard_width)\n",
    "        assert confidence_group_crop.shape == (self.standard_height, self.standard_width)\n",
    "        assert grp_confidence_group_crop.shape == (self.standard_height, self.standard_width)\n",
    "\n",
    "        kwargs['image'] = img_group_crop.copy()\n",
    "        kwargs['mask'] = msk_group_crop.copy()\n",
    "        kwargs['group_mask'] = grp_msk_group_crop.copy()\n",
    "        kwargs['solar_disk'] = disk_group_crop.copy()\n",
    "        kwargs['excentricity_map'] = excentricity_group_crop.copy()\n",
    "        kwargs['confidence_map'] = confidence_group_crop.copy()\n",
    "        kwargs['group_confidence_map'] = grp_confidence_group_crop.copy()\n",
    "        \n",
    "        return kwargs\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "class ClassificationDatasetSuperclasses_fast(Dataset):\n",
    "\n",
    "    def __init__(\n",
    "        self, root_dir, partition, dtypes, classes,\n",
    "        first_classes, second_classes, third_classes, \n",
    "        dataset_file, classification='SuperClass' , transforms=None) -> None:\n",
    "        super().__init__()\n",
    "        if isinstance(transforms, collections.Mapping):\n",
    "            transforms = partial(call, config=transforms)\n",
    "        elif isinstance(transforms, collections.Sequence):\n",
    "            transforms_init = []\n",
    "            for transform in transforms:\n",
    "                transforms_init.append(instantiate(transform))\n",
    "            transforms = Compose(transforms_init)\n",
    "\n",
    "        self.transforms = transforms\n",
    "        self.root_dir = Path(root_dir)\n",
    "\n",
    "        self.main_dtype = dtypes[0]\n",
    "        self.mask_dtype = dtypes[1]\n",
    "\n",
    "\n",
    "        self.disk_file = self.root_dir / dataset_file.replace('.', '_'+partition+'.')\n",
    "\n",
    "        self.partition_dict = None\n",
    "\n",
    "        self.c1_mapper = {c: i for i,c in enumerate(first_classes)}\n",
    "        self.c2_mapper = {c: i for i,c in enumerate(second_classes)}\n",
    "        self.c3_mapper = {c: i for i,c in enumerate(third_classes)}\n",
    "        \n",
    "        st = time.time()\n",
    "        \n",
    "        dataset = np.load(self.disk_file, allow_pickle=True).item()\n",
    "        \n",
    "        self.partition_dict = dataset\n",
    "\n",
    "        self.classification = classification\n",
    "\n",
    "        self.FirstClass_mapper = {c: i for i,c in enumerate(first_classes)}\n",
    "        self.SecondClass_mapper = {c: i for i,c in enumerate(second_classes)}\n",
    "        self.ThirdClass_mapper = {c: i for i,c in enumerate(third_classes)}\n",
    "\n",
    "        self.groups = {}\n",
    "        for k,v in self.partition_dict.items():\n",
    "\n",
    "            if v[\"class1\"] in classes:\n",
    "                    self.groups[k] = v\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "        self.dataset_length = len(list(self.groups.keys()))\n",
    "\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.dataset_length\n",
    "    \n",
    "    def __getitem__(self, index: int, do_transform=True):\n",
    "\n",
    "        idx = list(self.groups.keys())[index]\n",
    "        \n",
    "        sample = deepcopy(self.groups[idx])\n",
    "\n",
    "\n",
    "        sample['class1'] = np.array([self.FirstClass_mapper[sample['class1']]])\n",
    "        sample['class2'] = np.array([self.SecondClass_mapper[sample['class2']]])\n",
    "        sample['class3'] = np.array([self.ThirdClass_mapper[sample['class3']]])\n",
    "\n",
    "       \n",
    "        fig,ax = plt.subplots(2, 5, figsize=(10, 4) )\n",
    "        ax[0,0].set_title(sample['group_name'])\n",
    "        ax[0,0].imshow(sample[\"image\"], cmap='gray', interpolation='none')\n",
    "        ax[0,1].imshow(sample[\"mask\"], cmap='gray', interpolation='none')\n",
    "        ax[0,2].imshow(sample[\"confidence_map\"], cmap='gray', interpolation='none')\n",
    "        ax[0,3].imshow(sample[\"solar_disk\"], cmap='gray', interpolation='none')\n",
    "        ax[0,4].imshow(sample[\"excentricity_map\"], cmap='gray', interpolation='none')\n",
    "\n",
    "\n",
    "        tmp = np.argwhere(sample['confidence_map'] > 0)\n",
    "        ax[0,2].scatter(tmp[:,1], tmp[:,0], s=1, c='r', alpha=0.5)\n",
    "\n",
    "        sample['image'][sample['excentricity_map'] < 0] = 0\n",
    "        sample['mask'][sample['excentricity_map'] < 0] = 0\n",
    "        sample['group_mask'][sample['excentricity_map'] < 0] = 0\n",
    "        sample['solar_disk'][sample['excentricity_map'] < 0] = 0\n",
    "        sample['confidence_map'][sample['excentricity_map'] < 0] = 0\n",
    "        sample['group_confidence_map'][sample['excentricity_map'] < 0] = 0\n",
    "        sample['group_confidence_map'][sample['excentricity_map'] > 0.95] = 0\n",
    "        \n",
    "        sample['excentricity_map'][sample['excentricity_map'] < 0] = 0\n",
    "        \n",
    "        # st = time.time()\n",
    "        if self.transforms is not None and do_transform:\n",
    "            sample = self.transforms(**sample)\n",
    "        # print_elapsed_time(st, 'transform')\n",
    "\n",
    "        \n",
    "\n",
    "        ax[1,0].imshow(sample[\"image\"], cmap='gray', interpolation='none')\n",
    "        ax[1,1].imshow(sample[\"mask\"], cmap='gray', interpolation='none')\n",
    "        ax[1,2].imshow(sample[\"confidence_map\"], cmap='gray', interpolation='none')\n",
    "        ax[1,3].imshow(sample[\"solar_disk\"], cmap='gray', interpolation='none')\n",
    "        ax[1,4].imshow(sample[\"excentricity_map\"], cmap='gray', interpolation='none')\n",
    "        \n",
    "        fig.tight_layout()\n",
    "\n",
    "        tmp = np.argwhere(sample['confidence_map'] > 0)\n",
    "        ax[1,2].scatter(tmp[:,1], tmp[:,0], s=1, c='r', alpha=0.5)\n",
    "\n",
    "        return sample\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "root_dir = \"../dataset/Classification_dataset/2002-2019\"\n",
    "partition='train'\n",
    "dtypes = ['image', 'T425-T375-T325_fgbg']\n",
    "\n",
    "classes = ['A','B','C','SuperGroup','H']\n",
    "first_classes = [ 'A','B','C','SuperGroup','H']\n",
    "second_classes= [ 'x','r','sym','asym']\n",
    "third_classes= [ \"x\",\"o\",\"frag\"]\n",
    "classification='SuperClasses'\n",
    "\n",
    "transforms2 = [{'_target_': Deepsun_Focus_Move, 'standard_height': 256, 'standard_width': 256,\n",
    "                 'focus_on_group': False, 'random_move': False, 'random_move_percent': .2}]\n",
    "dataset_file = os.path.join(root_dir, 'test', f'all_samples.npy')\n",
    "d2 = ClassificationDatasetSuperclasses_fast(root_dir, partition, dtypes, classes, first_classes, second_classes, third_classes, \n",
    "                                                dataset_file, classification, transforms2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(len(d2))):\n",
    "    try:\n",
    "        tmp = d2[i]\n",
    "    except:\n",
    "        print(tmp['image'].shape)\n",
    "        print(i)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    d2[i]\n",
    "pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "1f3e85867ab3feeb73691fcc67a502ec8f0fc265745d17c9ab3a5329e7f22e4e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
